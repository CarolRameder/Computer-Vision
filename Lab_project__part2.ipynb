{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7PRKfXw99hy"
   },
   "source": [
    "# **Lab Project Part 2 - CNNs for Image Classification**\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY6wdmc299h1"
   },
   "source": [
    "### General Guideline\n",
    "1. **Aim**:\n",
    "    - *Understand  the  basic  Image  Classification/Recognition  pipeline  and  the  data-driven  approach (train/predict stages).*\n",
    "    - *Get used to one of deep learning framework(PyTorch).*\n",
    "2. **Prerequisite**:\n",
    "    - *Familiar with python and relevant packages.*\n",
    "    - *Known the basic knowledge of Convolutional Neural Networks*\n",
    "3. **Guidelines**:\n",
    "    Students should work on the assignments in a group of *three person* for two weeks. Some minor additions and changes might be done during these three weeks. Students will be informed for these changes via Canvas. Any questions regarding the assignment content can be discussed on Piazza. Students are expected to do this assignment in Python and Pytorch, however students are free to choose other tools (like Tensorflow). Your source code and report must be handed in together in a zip file (*ID1_ID2_ID3.zip*) before the deadline. Make sure your report follows these guidelines:\n",
    "    - *The maximum number of pages is 10 (single-column, including tables and figures). Please express your thoughts concisely.*\n",
    "    - *Follow the given script and answer all given questions (in green boxes). Briefly describe what you implemented. Blue boxes are there to give you hints to answer questions.*\n",
    "    - *Analyze your results and discuss them, e.g.*, why algorithm A works better than algorithm B in a certain problem.\n",
    "    - *Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.*\n",
    "4. **Late submissions** are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs' system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "5. **Plagiarism note**: \n",
    "Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations.\n",
    "\n",
    "### PyTorch versions\n",
    "we assume that you are using the latest PyTorch version(>=1.4)\n",
    "\n",
    "### PyTorch Tutorial & Docs\n",
    "This tutorial aims to make you familiar with the programming environment that will be used throughout the course. If you have experience with PyTorch or other frameworks (TensorFlow, MXNet *etc.*), you can skip the tutorial exercises; otherwise, we suggest that you complete them all, as they are helpful for getting hands-on experience.\n",
    "\n",
    "**Anaconda Environment** If you want to run the notebook locally, we recommend installing *anaconda* for configuring *python* package dependencies, whereas it's also fine to use other environment managers as you like. The installation of anaconda can be found in [anaconda link](https://docs.anaconda.com/anaconda/install/).\n",
    "\n",
    "**Installation** The installation of PyTorch is available at [install link](https://pytorch.org/get-started/locally/) depending on your device and system.\n",
    "\n",
    "**Getting start** The 60-minute blitz can be found at [blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), and and examples are at [examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "**Documents** There might be potential unknown functions or classes, you shall look through the official documents website ([Docs](https://pytorch.org/docs/stable/index.html)) and figure them out by yourself. (***Think***: What's the difference between *torch.nn.Conv2d* and *torch.nn.functional.conv2d*?)\n",
    "<!-- You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). In this assignments, we wish you to form the basic capability of using one of the well-known   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2EILNe3JR1b"
   },
   "source": [
    "##  **Introduction**\n",
    "\n",
    "This part of the assignment makes use of Convolutional Neural Networks (CNN). The previous part makes use of hand-crafted features like SIFT to represent images, then trains a classifier on top of them. In this way, learning is a two-step procedure with image representation and learning. The method used here instead *learns* the features jointly with the classification. Training CNNs roughly consists of three parts:  (i) Creating the network architecture, (ii) Reprocessing the data, (iii) Feeding the data to the network, and updating the parameters. Please follow the instruction and finish the below tasks. (**Note:**  **Feel free to change the provided codes**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXQBnhT499h2"
   },
   "source": [
    "## **Session 1: Image Classifiation on CIFAR-100**\n",
    "### 1.1 Install pytorch and run the given codes\n",
    "\n",
    "First of all, you need to install PyTorch and relevant packages. In this session, we will use [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) as the training and testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "47K1Kzph99h3"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# referenced codes: https://pytorch.org/tutorials/\n",
    "# referenced codes: http://cs231n.stanford.edu/\n",
    "# referenced codes: https://cs.stanford.edu/~acoates/stl10/\n",
    "######################################################\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#check the device available on your system\n",
    "#For this project, a single GPU system with CUDA enabled was used \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CUDA_LAUNCH_BLOCKING environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1781,
     "status": "ok",
     "timestamp": 1665063807401,
     "user": {
      "displayName": "Zehao Xiao",
      "userId": "03533745454963237857"
     },
     "user_tz": -120
    },
    "id": "8AU03smp99h4",
    "outputId": "d6d35ff0-2d0d-482f-f975-a13e0d6204e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5,shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('apple', 'aquarium_fish', 'baby','bear', 'beaver','bed','bee','beetle','bicycle','bottle', 'bowl','boy','bridge', 'bus','butterfly', 'camel','can','castle','caterpillar','cattle',\n",
    " 'chair','chimpanzee','clock','cloud', 'cockroach','couch', 'cra','crocodile', 'cup','dinosaur','dolphin', 'elephant','flatfish', 'forest', 'fox','girl', 'hamster', 'house','kangaroo','keyboard',\n",
    "'lamp', 'lawn_mower', 'leopard', 'lion','lizard','lobster', 'man','maple_tree','motorcycle', 'mountain', 'mouse','mushroom','oak_tree', 'orange','orchid', 'otter', 'palm_tree','pear', 'pickup_truck','pine_tree',\n",
    "'plain', 'plate', 'poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea', 'seal', 'shark','shrew', 'skunk','skyscraper', 'snail','snake','spider',\n",
    "'squirrel', 'streetcar', 'sunflower','sweet_pepper', 'table','tank','telephone', 'television', 'tiger','tractor','train','trout', 'tulip', 'turtle','wardrobe', 'whale', 'willow_tree','wolf', 'woman','worm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvNbVoMh99h5",
    "tags": []
   },
   "source": [
    "####  **` Q1.1: test dataloader and show the images of each class  of CIFAR-100 (3-pts)`**  \n",
    "You need to run and modify the given code and **show** the example images of CIFAR-100, **describe** the classes and images of CIFAR-100. (Please visualize at least one picture for the classes of labels from 0 to 4.) (3-*pts*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7hhMU6z999h6"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1665063813631,
     "user": {
      "displayName": "Zehao Xiao",
      "userId": "03533745454963237857"
     },
     "user_tz": -120
    },
    "id": "7iPwTYDB99h7",
    "outputId": "bb01296b-3ec6-4b9a-a8d1-a46d4163a2bb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " worm   can   bus streetcar skunk\n"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aquarium_fish', 'baby', 'beaver', 'bear', 'apple']\n"
     ]
    }
   ],
   "source": [
    "#first 5 classes\n",
    "sample_images = []\n",
    "outstanding_labels = [c for c in classes[:5]]\n",
    "labels_ordered = []\n",
    "\n",
    "for step, (images, labels) in enumerate(trainloader):\n",
    "    if len(outstanding_labels) == 0 or step > 100:\n",
    "        break\n",
    "    for image, label in zip(images, labels):\n",
    "        _label = classes[label.item()]\n",
    "        if _label in outstanding_labels:\n",
    "            sample_images.append(image)\n",
    "            outstanding_labels.remove(_label)\n",
    "            labels_ordered.append(_label)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(sample_images, nrow=10))\n",
    "print(labels_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ6OkCV9LIM9"
   },
   "source": [
    "### 1.2 Architecture understanding\n",
    "\n",
    "In this section, we provide two wrapped classes of architectures defined by *nn.Module*. One is an ordinary two-layer network (*TwolayerNet*) with fully connected layers and ReLu, and the other is a Convolutional Network (*ConvNet*) utilizing the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6a1z2vK99h7",
    "tags": []
   },
   "source": [
    "####  **`Q1.2: Architecture understanding. Implement architecture of TwolayerNet and ConvNet (4-pts).`**\n",
    "1. Complement the architecture of *TwolayerNet* class, and complement the architecture of *ConvNet* class using the structure of LeNet-5. (2-*pts*)\n",
    "2. Since you need to feed color images into these two networks, what's the kernel size of the first convolutional layer in *ConvNet*? and how many trainable parameters are there in \"F6\" layer (given the calculation process)? (2-*pts*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ILrfnDDv99h8"
   },
   "outputs": [],
   "source": [
    "class TwolayerNet(nn.Module):\n",
    "    # assign layer objects to class attributes\n",
    "    # nn.init package contains convenient initialization methods\n",
    "    # http://pytorch.org/docs/master/nn.html#torch-nn-init\n",
    "    def __init__(self,input_size ,hidden_size ,num_classes ):\n",
    "        '''\n",
    "        :param input_size: 3*32*32\n",
    "        :param hidden_size: \n",
    "        :param num_classes: \n",
    "        '''\n",
    "        ################################\n",
    "        super(TwolayerNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.F1 = nn.Linear(input_size, hidden_size)\n",
    "        self.F2 = nn.Linear(hidden_size, num_classes)\n",
    "        ################################\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # the input is flatten\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         x.to(device)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = nn.functional.relu(self.F1(x))\n",
    "        scores = self.F2(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5E2a9-lz99h8"
   },
   "outputs": [],
   "source": [
    "### NOTE: For simplicity, you can use nn.tanh as the activation function and output the 1x100 !! length logits directly (omit the RBF units output).\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 6, 5)\n",
    "        self.S2 = nn.AvgPool2d(2,2)\n",
    "        self.C3 = nn.Conv2d(6, 16, 5)\n",
    "        self.S4 = nn.AvgPool2d(2,2)\n",
    "        self.F5 = nn.Linear(400, 120)\n",
    "        self.F6 = nn.Linear(120, 84)\n",
    "        self.out = nn.Linear(84, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.C1(x))\n",
    "        x = self.S2(x)\n",
    "        x = torch.tanh(self.C3(x))\n",
    "        x = self.S4(x)\n",
    "        #flatten before linear layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.F5(x))\n",
    "        x = torch.tanh(self.F6(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Linear: 1-1                            [-1, 128]                 393,344\n",
      "├─Linear: 1-2                            [-1, 100]                 12,900\n",
      "==========================================================================================\n",
      "Total params: 406,244\n",
      "Trainable params: 406,244\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.41\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 1.55\n",
      "Estimated Total Size (MB): 1.56\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Linear: 1-1                            [-1, 128]                 393,344\n",
       "├─Linear: 1-2                            [-1, 100]                 12,900\n",
       "==========================================================================================\n",
       "Total params: 406,244\n",
       "Trainable params: 406,244\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.41\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 1.55\n",
       "Estimated Total Size (MB): 1.56\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tln_model = TwolayerNet(3072, 128, 100)\n",
    "summary(tln_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 6, 28, 28]           456\n",
      "├─AvgPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
      "├─Conv2d: 1-3                            [-1, 16, 10, 10]          2,416\n",
      "├─AvgPool2d: 1-4                         [-1, 16, 5, 5]            --\n",
      "├─Linear: 1-5                            [-1, 120]                 48,120\n",
      "├─Linear: 1-6                            [-1, 84]                  10,164\n",
      "├─Linear: 1-7                            [-1, 100]                 8,500\n",
      "==========================================================================================\n",
      "Total params: 69,656\n",
      "Trainable params: 69,656\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.66\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.27\n",
      "Estimated Total Size (MB): 0.33\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 6, 28, 28]           456\n",
       "├─AvgPool2d: 1-2                         [-1, 6, 14, 14]           --\n",
       "├─Conv2d: 1-3                            [-1, 16, 10, 10]          2,416\n",
       "├─AvgPool2d: 1-4                         [-1, 16, 5, 5]            --\n",
       "├─Linear: 1-5                            [-1, 120]                 48,120\n",
       "├─Linear: 1-6                            [-1, 84]                  10,164\n",
       "├─Linear: 1-7                            [-1, 100]                 8,500\n",
       "==========================================================================================\n",
       "Total params: 69,656\n",
       "Trainable params: 69,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.66\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.27\n",
       "Estimated Total Size (MB): 0.33\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = ConvNet()\n",
    "summary(cnn_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCP1tS7lL8hF"
   },
   "source": [
    "### 1.3 Preparation of training\n",
    "\n",
    "In above section, we use the *CIFAR-100* dataset class from *torchvision.utils* provided by PyTorch. Whereas in most cases, you need to prepare the dataset yourself. One of the ways is to create a *dataset* class yourself and then use the *DataLoader* to make it iterable. After preparing the training and testing data, you also need to define the transform function for data augmentation and optimizer for parameter updating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oyc8Ksd99h-"
   },
   "source": [
    "####  **` Q1.3: Preparation of training. Create Dataloader yourself and define Transform, optimizer.(8-pts)`**  \n",
    "1. Complement the *CIFAR100\\_loader* (2-pts)\n",
    "2. Complement *Transform* function and *Optimizer* (2-pts)\n",
    "3. Train the *TwolayerNet* and *ConvNet* with *CIFAR100\\_loader*, *Transform* and *Optimizer* you implemented and compare the results (4-pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkImoAsU99h-",
    "tags": []
   },
   "source": [
    "##### *` Complement  CIFAR100_loader()(2-pts)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G9Za4VSP99h_"
   },
   "outputs": [],
   "source": [
    "###  suggested reference: https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html?highlight=dataloader\n",
    "# functions to show an image\n",
    "\n",
    "class CIFAR100_loader(Dataset):\n",
    "    def __init__(self,root,train=True,transform = None):\n",
    "        dict_data = self.unpickle(root)\n",
    "        self.labels = dict_data['fine_labels']\n",
    "        imgs = dict_data['data']\n",
    "        self.data = self.img_format(imgs)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def img_format(self, data):\n",
    "        #dim should be 32 in this case\n",
    "        dim=np.sqrt(data.shape[1]/3).astype(int)\n",
    "        \n",
    "        r = data[:, 0:1024].reshape(data.shape[0], dim, dim, 1)\n",
    "        g = data[:, 1024:2048].reshape(data.shape[0], dim, dim, 1)\n",
    "        b = data[:, 2048:3072].reshape(data.shape[0], dim, dim, 1)\n",
    "        return np.concatenate([r,g,b], -1)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        img = self.data[item]\n",
    "        label = self.labels[item]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label\n",
    "    \n",
    "    def unpickle(self, file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='latin')\n",
    "        return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYKtNWW_99h_",
    "tags": []
   },
   "source": [
    "##### *` Complement Transform function and Optimizer (2-pts)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E2yNqZpZ99h_"
   },
   "outputs": [],
   "source": [
    "#initially transform train & test identical\n",
    "transform_train = transform_train = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomVerticalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
    "\n",
    "transform_test= transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"/mnt/2c91ad6d-c5b0-418d-b8c0-ec45155a1fd9/Master/Year_2/Period 1/Computer Vision 1/Lab_proj/Final_Lab_Part_2CNN/data/cifar-100-python/train\"\n",
    "path2 = \"/mnt/2c91ad6d-c5b0-418d-b8c0-ec45155a1fd9/Master/Year_2/Period 1/Computer Vision 1/Lab_proj/Final_Lab_Part_2CNN/data/cifar-100-python/test\"\n",
    "path3 = \"/mnt/2c91ad6d-c5b0-418d-b8c0-ec45155a1fd9/Master/Year_2/Period 1/Computer Vision 1/Lab_proj/Final_Lab_Part_2CNN/data/cifar-100-python/meta\"\n",
    "\n",
    "transformed_train = CIFAR100_loader(path1, transform = transform_train)\n",
    "trainloader = DataLoader(transformed_train, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "transformed_test= CIFAR100_loader(path2, transform = transform_test)\n",
    "testloader = DataLoader(transformed_test, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking trainloader content\n",
    "# for x,l in trainloader:\n",
    "#     print(np.shape(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYpJN5Cb99h_"
   },
   "source": [
    "##### *` Train the TwolayerNet and ConvNet with CIFAR100_loader, transform and optimizer you implemented and compare the results (4-pts)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hNwBZvfX99h_"
   },
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "def train(net, trainloader,epochs=1):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    net.to(device)\n",
    "    t_ls=[]\n",
    "    \n",
    "    #one epoch -> processing all data once\n",
    "    for epoch in range(epochs):  \n",
    "        loss_ep = 0\n",
    "        net.train()\n",
    "        \n",
    "        #iterating through batches\n",
    "        for x, l in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x=x.to(device)\n",
    "            l=l.to(device)\n",
    "            net.zero_grad()\n",
    "\n",
    "            ## Make predictions for this batch\n",
    "            tag_scores = net(x)\n",
    "            \n",
    "            # Compute the loss and its gradients, save the current value\n",
    "            loss = loss_function(tag_scores, l)\n",
    "            loss_ep+=loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        print(\"Epoch: \" + str(epoch) + \". Training data loss: \", loss_ep.item())\n",
    "        t_ls.append(loss_ep)\n",
    "\n",
    "    #prints & figures\n",
    "    print('Finished Training')\n",
    "    plt.title('Cross Entropy Loss {}'.format(type(net).__name__))\n",
    "    plt.xlabel('Epoch')\n",
    "    if torch.cuda.is_available():\n",
    "        t_ls = torch.tensor(t_ls, device = 'cpu') \n",
    "        \n",
    "    if isinstance(t_ls, torch.Tensor):\n",
    "        # If 't_ls' is a tensor, detach and convert to NumPy array\n",
    "        t_ls = t_ls.detach().numpy()\n",
    "        \n",
    "    elif isinstance(t_ls, list):\n",
    "        # If 't_ls' is a list, convert to NumPy array\n",
    "        t_ls = np.array(t_ls)\n",
    "        \n",
    "    plt.plot(t_ls,label=\"train\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxfxa_Rj8X5z"
   },
   "source": [
    "*train TwolayerNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rd3ql4B27yEl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  6167.2275390625\n",
      "Epoch: 1. Training data loss:  5702.57421875\n",
      "Epoch: 2. Training data loss:  5543.75244140625\n",
      "Epoch: 3. Training data loss:  5460.93359375\n",
      "Epoch: 4. Training data loss:  5407.7060546875\n",
      "Epoch: 5. Training data loss:  5366.9091796875\n",
      "Epoch: 6. Training data loss:  5344.38134765625\n",
      "Epoch: 7. Training data loss:  5320.0390625\n",
      "Epoch: 8. Training data loss:  5301.6328125\n",
      "Epoch: 9. Training data loss:  5285.35986328125\n",
      "Epoch: 10. Training data loss:  5273.46630859375\n",
      "Epoch: 11. Training data loss:  5268.72314453125\n",
      "Epoch: 12. Training data loss:  5261.22216796875\n",
      "Epoch: 13. Training data loss:  5248.88671875\n",
      "Epoch: 14. Training data loss:  5243.6474609375\n",
      "Epoch: 15. Training data loss:  5236.67724609375\n",
      "Epoch: 16. Training data loss:  5230.26220703125\n",
      "Epoch: 17. Training data loss:  5238.46044921875\n",
      "Epoch: 18. Training data loss:  5216.16552734375\n",
      "Epoch: 19. Training data loss:  5218.99462890625\n",
      "Epoch: 20. Training data loss:  5221.51416015625\n",
      "Epoch: 21. Training data loss:  5214.12890625\n",
      "Epoch: 22. Training data loss:  5220.50146484375\n",
      "Epoch: 23. Training data loss:  5215.01416015625\n",
      "Epoch: 24. Training data loss:  5207.51513671875\n",
      "Epoch: 25. Training data loss:  5206.03857421875\n",
      "Epoch: 26. Training data loss:  5208.78466796875\n",
      "Epoch: 27. Training data loss:  5208.6123046875\n",
      "Epoch: 28. Training data loss:  5198.9296875\n",
      "Epoch: 29. Training data loss:  5200.12255859375\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#size of image representation\n",
    "tln_model = TwolayerNet(3072, 128, 100)\n",
    "train(tln_model, trainloader, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GySb8UWX8emz"
   },
   "source": [
    "*train ConvNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  6290.88720703125\n",
      "Epoch: 1. Training data loss:  5738.25634765625\n",
      "Epoch: 2. Training data loss:  5515.04736328125\n",
      "Epoch: 3. Training data loss:  5367.26513671875\n",
      "Epoch: 4. Training data loss:  5258.28857421875\n",
      "Epoch: 5. Training data loss:  5175.8447265625\n",
      "Epoch: 6. Training data loss:  5103.52294921875\n",
      "Epoch: 7. Training data loss:  5038.1005859375\n",
      "Epoch: 8. Training data loss:  4987.4169921875\n",
      "Epoch: 9. Training data loss:  4946.38525390625\n",
      "Epoch: 10. Training data loss:  4907.9833984375\n",
      "Epoch: 11. Training data loss:  4871.5400390625\n",
      "Epoch: 12. Training data loss:  4838.54736328125\n",
      "Epoch: 13. Training data loss:  4805.322265625\n",
      "Epoch: 14. Training data loss:  4780.8994140625\n",
      "Epoch: 15. Training data loss:  4750.45458984375\n",
      "Epoch: 16. Training data loss:  4723.99951171875\n",
      "Epoch: 17. Training data loss:  4701.31396484375\n",
      "Epoch: 18. Training data loss:  4676.8173828125\n",
      "Epoch: 19. Training data loss:  4653.60546875\n",
      "Epoch: 20. Training data loss:  4637.12548828125\n",
      "Epoch: 21. Training data loss:  4618.369140625\n",
      "Epoch: 22. Training data loss:  4599.67431640625\n",
      "Epoch: 23. Training data loss:  4587.23681640625\n",
      "Epoch: 24. Training data loss:  4575.37744140625\n",
      "Epoch: 25. Training data loss:  4553.6728515625\n",
      "Epoch: 26. Training data loss:  4551.92138671875\n",
      "Epoch: 27. Training data loss:  4527.7060546875\n",
      "Epoch: 28. Training data loss:  4523.19189453125\n",
      "Epoch: 29. Training data loss:  4509.2939453125\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_model = ConvNet()\n",
    "train(cnn_model, trainloader, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7FBozCDk99h-"
   },
   "outputs": [],
   "source": [
    "def valid(net,testloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)   \n",
    "            net.eval()\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    string = 'Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total)\n",
    "    val = 100 * correct / total\n",
    "    return string, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zm7FuYwJ99h-"
   },
   "outputs": [],
   "source": [
    "def valid_class(net,testloader,classes):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    class_correct = list(0. for i in range(len(classes)))\n",
    "    class_total = list(0. for i in range(len(classes)))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            net.eval()\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    outputs=[]\n",
    "    for i in range(len(classes)):\n",
    "        outputs.append('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the network on the 10000 test images: 19 %', 19.43)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Accuracy of apple : 42 %',\n",
       " 'Accuracy of aquarium_fish : 38 %',\n",
       " 'Accuracy of  baby : 22 %',\n",
       " 'Accuracy of  bear : 15 %',\n",
       " 'Accuracy of beaver : 27 %',\n",
       " 'Accuracy of   bed : 23 %',\n",
       " 'Accuracy of   bee :  9 %',\n",
       " 'Accuracy of beetle : 12 %',\n",
       " 'Accuracy of bicycle : 21 %',\n",
       " 'Accuracy of bottle : 32 %',\n",
       " 'Accuracy of  bowl :  8 %',\n",
       " 'Accuracy of   boy :  2 %',\n",
       " 'Accuracy of bridge :  6 %',\n",
       " 'Accuracy of   bus :  9 %',\n",
       " 'Accuracy of butterfly :  7 %',\n",
       " 'Accuracy of camel : 11 %',\n",
       " 'Accuracy of   can :  3 %',\n",
       " 'Accuracy of castle : 38 %',\n",
       " 'Accuracy of caterpillar : 19 %',\n",
       " 'Accuracy of cattle : 10 %',\n",
       " 'Accuracy of chair : 37 %',\n",
       " 'Accuracy of chimpanzee : 25 %',\n",
       " 'Accuracy of clock : 16 %',\n",
       " 'Accuracy of cloud : 53 %',\n",
       " 'Accuracy of cockroach : 27 %',\n",
       " 'Accuracy of couch :  2 %',\n",
       " 'Accuracy of  crab :  9 %',\n",
       " 'Accuracy of crocodile :  5 %',\n",
       " 'Accuracy of   cup : 12 %',\n",
       " 'Accuracy of dinosaur : 11 %',\n",
       " 'Accuracy of dolphin : 50 %',\n",
       " 'Accuracy of elephant : 23 %',\n",
       " 'Accuracy of flatfish : 15 %',\n",
       " 'Accuracy of forest : 14 %',\n",
       " 'Accuracy of   fox :  7 %',\n",
       " 'Accuracy of  girl :  5 %',\n",
       " 'Accuracy of hamster : 15 %',\n",
       " 'Accuracy of house : 12 %',\n",
       " 'Accuracy of kangaroo :  7 %',\n",
       " 'Accuracy of keyboard :  6 %',\n",
       " 'Accuracy of  lamp : 16 %',\n",
       " 'Accuracy of lawn_mower : 29 %',\n",
       " 'Accuracy of leopard :  6 %',\n",
       " 'Accuracy of  lion : 41 %',\n",
       " 'Accuracy of lizard :  3 %',\n",
       " 'Accuracy of lobster :  3 %',\n",
       " 'Accuracy of   man :  3 %',\n",
       " 'Accuracy of maple_tree : 25 %',\n",
       " 'Accuracy of motorcycle : 30 %',\n",
       " 'Accuracy of mountain : 34 %',\n",
       " 'Accuracy of mouse :  0 %',\n",
       " 'Accuracy of mushroom : 22 %',\n",
       " 'Accuracy of oak_tree : 35 %',\n",
       " 'Accuracy of orange : 56 %',\n",
       " 'Accuracy of orchid :  9 %',\n",
       " 'Accuracy of otter :  3 %',\n",
       " 'Accuracy of palm_tree : 10 %',\n",
       " 'Accuracy of  pear : 19 %',\n",
       " 'Accuracy of pickup_truck : 24 %',\n",
       " 'Accuracy of pine_tree :  8 %',\n",
       " 'Accuracy of plain : 67 %',\n",
       " 'Accuracy of plate : 29 %',\n",
       " 'Accuracy of poppy : 32 %',\n",
       " 'Accuracy of porcupine : 39 %',\n",
       " 'Accuracy of possum : 13 %',\n",
       " 'Accuracy of rabbit :  1 %',\n",
       " 'Accuracy of raccoon :  1 %',\n",
       " 'Accuracy of   ray : 21 %',\n",
       " 'Accuracy of  road : 54 %',\n",
       " 'Accuracy of rocket : 27 %',\n",
       " 'Accuracy of  rose : 30 %',\n",
       " 'Accuracy of   sea : 13 %',\n",
       " 'Accuracy of  seal :  1 %',\n",
       " 'Accuracy of shark :  5 %',\n",
       " 'Accuracy of shrew :  1 %',\n",
       " 'Accuracy of skunk : 30 %',\n",
       " 'Accuracy of skyscraper : 42 %',\n",
       " 'Accuracy of snail : 19 %',\n",
       " 'Accuracy of snake :  6 %',\n",
       " 'Accuracy of spider :  1 %',\n",
       " 'Accuracy of squirrel :  3 %',\n",
       " 'Accuracy of streetcar : 16 %',\n",
       " 'Accuracy of sunflower : 55 %',\n",
       " 'Accuracy of sweet_pepper : 30 %',\n",
       " 'Accuracy of table :  0 %',\n",
       " 'Accuracy of  tank : 20 %',\n",
       " 'Accuracy of telephone : 27 %',\n",
       " 'Accuracy of television :  9 %',\n",
       " 'Accuracy of tiger :  3 %',\n",
       " 'Accuracy of tractor : 15 %',\n",
       " 'Accuracy of train : 18 %',\n",
       " 'Accuracy of trout : 34 %',\n",
       " 'Accuracy of tulip : 16 %',\n",
       " 'Accuracy of turtle : 14 %',\n",
       " 'Accuracy of wardrobe : 53 %',\n",
       " 'Accuracy of whale : 25 %',\n",
       " 'Accuracy of willow_tree : 50 %',\n",
       " 'Accuracy of  wolf :  7 %',\n",
       " 'Accuracy of woman :  7 %',\n",
       " 'Accuracy of  worm : 28 %']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = unpickle(path3).get('fine_label_names')\n",
    "\n",
    "#trainee = unpickle(path1).get('data')\n",
    "#print(trainee)\n",
    "\n",
    "print(valid(tln_model,testloader))\n",
    "valid_class(tln_model,testloader,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the network on the 10000 test images: 26 %', 26.41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Accuracy of apple : 44 %',\n",
       " 'Accuracy of aquarium_fish : 44 %',\n",
       " 'Accuracy of  baby : 11 %',\n",
       " 'Accuracy of  bear :  5 %',\n",
       " 'Accuracy of beaver : 12 %',\n",
       " 'Accuracy of   bed : 23 %',\n",
       " 'Accuracy of   bee : 23 %',\n",
       " 'Accuracy of beetle : 25 %',\n",
       " 'Accuracy of bicycle : 23 %',\n",
       " 'Accuracy of bottle : 47 %',\n",
       " 'Accuracy of  bowl :  4 %',\n",
       " 'Accuracy of   boy :  6 %',\n",
       " 'Accuracy of bridge : 22 %',\n",
       " 'Accuracy of   bus : 26 %',\n",
       " 'Accuracy of butterfly : 10 %',\n",
       " 'Accuracy of camel : 23 %',\n",
       " 'Accuracy of   can : 18 %',\n",
       " 'Accuracy of castle : 48 %',\n",
       " 'Accuracy of caterpillar : 22 %',\n",
       " 'Accuracy of cattle :  9 %',\n",
       " 'Accuracy of chair : 59 %',\n",
       " 'Accuracy of chimpanzee : 44 %',\n",
       " 'Accuracy of clock : 18 %',\n",
       " 'Accuracy of cloud : 60 %',\n",
       " 'Accuracy of cockroach : 56 %',\n",
       " 'Accuracy of couch : 15 %',\n",
       " 'Accuracy of  crab : 22 %',\n",
       " 'Accuracy of crocodile : 19 %',\n",
       " 'Accuracy of   cup : 19 %',\n",
       " 'Accuracy of dinosaur : 19 %',\n",
       " 'Accuracy of dolphin : 27 %',\n",
       " 'Accuracy of elephant : 27 %',\n",
       " 'Accuracy of flatfish : 27 %',\n",
       " 'Accuracy of forest : 23 %',\n",
       " 'Accuracy of   fox : 26 %',\n",
       " 'Accuracy of  girl : 10 %',\n",
       " 'Accuracy of hamster : 35 %',\n",
       " 'Accuracy of house : 17 %',\n",
       " 'Accuracy of kangaroo : 20 %',\n",
       " 'Accuracy of keyboard : 26 %',\n",
       " 'Accuracy of  lamp : 12 %',\n",
       " 'Accuracy of lawn_mower : 47 %',\n",
       " 'Accuracy of leopard : 18 %',\n",
       " 'Accuracy of  lion : 53 %',\n",
       " 'Accuracy of lizard :  7 %',\n",
       " 'Accuracy of lobster : 12 %',\n",
       " 'Accuracy of   man : 10 %',\n",
       " 'Accuracy of maple_tree : 27 %',\n",
       " 'Accuracy of motorcycle : 34 %',\n",
       " 'Accuracy of mountain : 46 %',\n",
       " 'Accuracy of mouse :  2 %',\n",
       " 'Accuracy of mushroom : 11 %',\n",
       " 'Accuracy of oak_tree : 73 %',\n",
       " 'Accuracy of orange : 70 %',\n",
       " 'Accuracy of orchid : 45 %',\n",
       " 'Accuracy of otter :  3 %',\n",
       " 'Accuracy of palm_tree : 29 %',\n",
       " 'Accuracy of  pear : 13 %',\n",
       " 'Accuracy of pickup_truck : 25 %',\n",
       " 'Accuracy of pine_tree : 18 %',\n",
       " 'Accuracy of plain : 77 %',\n",
       " 'Accuracy of plate : 29 %',\n",
       " 'Accuracy of poppy : 35 %',\n",
       " 'Accuracy of porcupine : 18 %',\n",
       " 'Accuracy of possum :  2 %',\n",
       " 'Accuracy of rabbit :  4 %',\n",
       " 'Accuracy of raccoon : 24 %',\n",
       " 'Accuracy of   ray :  5 %',\n",
       " 'Accuracy of  road : 61 %',\n",
       " 'Accuracy of rocket : 38 %',\n",
       " 'Accuracy of  rose : 24 %',\n",
       " 'Accuracy of   sea : 38 %',\n",
       " 'Accuracy of  seal :  5 %',\n",
       " 'Accuracy of shark : 36 %',\n",
       " 'Accuracy of shrew :  4 %',\n",
       " 'Accuracy of skunk : 46 %',\n",
       " 'Accuracy of skyscraper : 49 %',\n",
       " 'Accuracy of snail : 19 %',\n",
       " 'Accuracy of snake :  6 %',\n",
       " 'Accuracy of spider : 19 %',\n",
       " 'Accuracy of squirrel :  1 %',\n",
       " 'Accuracy of streetcar : 21 %',\n",
       " 'Accuracy of sunflower : 54 %',\n",
       " 'Accuracy of sweet_pepper : 20 %',\n",
       " 'Accuracy of table :  6 %',\n",
       " 'Accuracy of  tank : 34 %',\n",
       " 'Accuracy of telephone : 37 %',\n",
       " 'Accuracy of television : 28 %',\n",
       " 'Accuracy of tiger : 14 %',\n",
       " 'Accuracy of tractor : 39 %',\n",
       " 'Accuracy of train :  7 %',\n",
       " 'Accuracy of trout : 42 %',\n",
       " 'Accuracy of tulip : 14 %',\n",
       " 'Accuracy of turtle :  3 %',\n",
       " 'Accuracy of wardrobe : 78 %',\n",
       " 'Accuracy of whale : 48 %',\n",
       " 'Accuracy of willow_tree : 29 %',\n",
       " 'Accuracy of  wolf : 32 %',\n",
       " 'Accuracy of woman :  8 %',\n",
       " 'Accuracy of  worm : 18 %']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid(cnn_model,testloader))\n",
    "valid_class(cnn_model,testloader,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQhUzRuQOOSm"
   },
   "source": [
    "# 1.4 Setting up the hyperparameters\n",
    "\n",
    "Some parameters must be set properly before the training of CNNs. These parameters shape the training procedure. They determine how many images are to be processed at each step, how much the weights of the network will be updated, how many iterations will the network run until convergence.  These parameters are called hyperparameters in the machine learning literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhtGVNHIQ8kT"
   },
   "source": [
    "####  **` Q1.4: Setting up the hyperparameters (10-pts)`**  \n",
    "\n",
    "1. Play with ConvNet and TwolayerNet yourself, set up the hyperparameters, and reach the accuracy as high as you can.\n",
    "You can modify the *train*,  *Dataloader*, *transform* and *Optimizer* function as you like. You can also modify the architectures of these two Nets. \n",
    "\n",
    "2. *Let's add 2 more layers in TwolayerNet and ConvNet, and show the results. (You can decide the size of these layers and where to add them.) Will you get higher performances? explain why.*\n",
    " \n",
    "3.  Show the final results and described what you've done to improve the results. Describe and explain the influence of hyperparameters among *TwolayerNet* and *ConvNet*.\n",
    "\n",
    "4. Compare and explain the differences of these two networks regarding the architecture, performances, and learning rates. \n",
    "\n",
    "**Hint:** You can adjust the following parameters and other parameters not listed as you like: *Learning rate, Batch size, Number of epochs, Optimizer, Transform function, Weight decay etc.* You can also change the structure a bit, for instance, adding Batch Normalization layers. Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Puiytn99h_"
   },
   "source": [
    "#### *`Play with convNet and TwolayerNet, set up the hyperparameters and reach the accuracy as high as you can`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes made on\n",
    "    #Transform function\n",
    "    #Random Search : Learning rate, Weight decay, Batch size, Number of epochs, Optimizer and Loss Function\n",
    "#Custom function with changeable hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ROyqHC0199iA"
   },
   "outputs": [],
   "source": [
    "def train2(net, trainloader,hyperparam):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epochs, lr, wd, loss_function, current_optimizer= hyperparam\n",
    "    if current_optimizer==1:\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    net.to(device)\n",
    "    t_ls=[]\n",
    "    \n",
    "    #one epoch -> processing all data once\n",
    "    for epoch in range(epochs):  \n",
    "        loss_ep = 0\n",
    "        net.train()\n",
    "        \n",
    "        #iterating through batches\n",
    "        for x, l in trainloader:\n",
    "            x=x.to(device)\n",
    "            l=l.to(device)\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## Make predictions for this batch\n",
    "            tag_scores = net(x)\n",
    "            \n",
    "            # Compute the loss and its gradients, save the current value\n",
    "            loss = loss_function(tag_scores, l)\n",
    "            loss_ep+=loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch: \" + str(epoch) + \". Training data loss: \", loss_ep.item())\n",
    "        t_ls.append(loss_ep)\n",
    "\n",
    "    # print('Finished Training')\n",
    "    # plt.title('Cross Entropy Loss {}'.format(type(net).__name__))\n",
    "    # plt.xlabel('Epoch')\n",
    "    # if torch.cuda.is_available():\n",
    "    #     t_ls = torch.tensor(t_ls, device = 'cpu') \n",
    "    # plt.plot(t_ls.\n",
    "    #          numpy(),label=\"train\")\n",
    "    # plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_sz, transform_train, transform_test):\n",
    "    transformed_train = CIFAR100_loader(path1, transform = transform_train)\n",
    "    trainloader = DataLoader(transformed_train, batch_size=batch_sz, shuffle=True, num_workers=2)\n",
    "\n",
    "    transformed_test= CIFAR100_loader(path2, transform = transform_test)\n",
    "    testloader = DataLoader(transformed_test, batch_size=batch_sz, shuffle=True, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def valid_both(net,testloader,classes):\n",
    "    final = valid_class(net,testloader,classes)\n",
    "    string, val = valid(net,testloader)\n",
    "    final.append(string)\n",
    "    return final, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified transform: added Horizontal and Vertical flip for rubustness of model, changed with optimal CIFAR-100 values\n",
    "transform_train = transform_train = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomVerticalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'epoch':list(range(20, 80, 20)),\n",
    "    'lr': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'wd': [0,0.01,0.3, 0.001],\n",
    "    'loss_function':[nn.CrossEntropyLoss(),nn.MultiMarginLoss()],\n",
    "    'batch_size':[10,32,64,128],\n",
    "    'optimizers':[0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_config(parameter_distribution):\n",
    "    config = []\n",
    "    for key in parameter_distribution.keys():\n",
    "        pos_val = parameter_distribution.get(key)\n",
    "        config.append(random.choice(pos_val))\n",
    "    return tuple(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_search(model_type,param_dist, classes, n_iter):\n",
    "    best_acc = 0\n",
    "    best_config=()\n",
    "    output=[]\n",
    "    for i in range(n_iter):\n",
    "        if model_type == 'cnn':\n",
    "            model = ConvNet()\n",
    "        elif model_type == 'tln':\n",
    "            model = TwolayerNet(3072, 128, 100)\n",
    "        elif model_type == 'cnn2':\n",
    "            model = ConvNet_2()\n",
    "        elif model_type == 'mln':\n",
    "            model = MultiLayerNet(3072, 128, 100)\n",
    "        print('Starting iteration, {}'.format(i))\n",
    "        epoch, lr, wd, loss_fc, batch_sz, optim = get_random_config(param_dist)\n",
    "        hyper_net = (epoch, lr, wd, loss_fc, optim)\n",
    "        trainloader, testloader = get_loaders(batch_sz, transform_train, transform_test)\n",
    "        train2(model, trainloader, hyper_net)\n",
    "        strings, acc = valid_both(model, testloader, classes)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_config = (epoch, lr, wd, loss_fc, batch_sz, optim)\n",
    "            output = strings\n",
    "        print('Accuracy found: {}'.format(acc))\n",
    "        print(epoch, lr, wd, loss_fc, batch_sz, optim)\n",
    "    return best_acc, best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *` Add layers `*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet_2(nn.Module):\n",
    "    # Complete the code using LeNet-5\n",
    "    # reference: https://ieeexplore.ieee.org/document/726791\n",
    "    def __init__(self):\n",
    "        super(ConvNet_2, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 6, 5)\n",
    "        self.BN2 = nn.BatchNorm2d(6)\n",
    "        self.S3 = nn.AvgPool2d(2,2)\n",
    "        self.C4 = nn.Conv2d(6, 16, 5)\n",
    "        self.S5 = nn.AvgPool2d(2,2)\n",
    "        self.F6 = nn.Linear(400, 120, 5)\n",
    "        self.F7 = nn.Linear(120, 84)\n",
    "        self.D8 = nn.Dropout(0.4)\n",
    "        self.out = nn.Linear(84, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.BN2(self.C1(x))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.S3(x)\n",
    "        x = F.leaky_relu(self.C4(x))\n",
    "        x = self.S5(x)\n",
    "        \n",
    "        #flatten before linear layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.F6(x))\n",
    "        x = F.leaky_relu(self.F7(x))\n",
    "        x = self.D8(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNet(nn.Module):\n",
    "    # assign layer objects to class attributes\n",
    "    # nn.init package contains convenient initialization methods\n",
    "    # http://pytorch.org/docs/master/nn.html#torch-nn-init\n",
    "    def __init__(self,input_size ,hidden_size ,num_classes ):\n",
    "        '''\n",
    "        :param input_size: 3*32*32\n",
    "        :param hidden_size: \n",
    "        :param num_classes: \n",
    "        '''\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.F1 = nn.Linear(input_size, hidden_size)\n",
    "        self.BN2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.D3 = nn.Dropout(0.4)\n",
    "        self.F4 = nn.Linear(hidden_size, 64)\n",
    "        self.F5 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        ##########################Inialization#############################\n",
    "        # Xavier/Glorot initialization for linear layers\n",
    "        init.xavier_uniform_(self.F1.weight)\n",
    "        init.xavier_uniform_(self.F4.weight)\n",
    "        init.xavier_uniform_(self.F5.weight)\n",
    "\n",
    "        # Zero initialization for biases\n",
    "        init.zeros_(self.F1.bias)\n",
    "        init.zeros_(self.F4.bias)\n",
    "        init.zeros_(self.F5.bias)\n",
    "\n",
    "        # Constant initialization for batch normalization layer\n",
    "        init.constant_(self.BN2.weight, 1)\n",
    "        init.constant_(self.BN2.bias, 0)\n",
    "        #################################################################\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # the input is flatten\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         x.to(device)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.BN2(self.F1(x))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.D3(x)\n",
    "        x = F.leaky_relu(self.F4(x))\n",
    "        scores = self.F5(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  6481.53369140625\n",
      "Epoch: 1. Training data loss:  6080.34814453125\n",
      "Epoch: 2. Training data loss:  5979.89794921875\n",
      "Epoch: 3. Training data loss:  5917.36328125\n",
      "Epoch: 4. Training data loss:  5875.8994140625\n",
      "Epoch: 5. Training data loss:  5846.34130859375\n",
      "Epoch: 6. Training data loss:  5815.49853515625\n",
      "Epoch: 7. Training data loss:  5795.3818359375\n",
      "Epoch: 8. Training data loss:  5789.66650390625\n",
      "Epoch: 9. Training data loss:  5769.7685546875\n",
      "Epoch: 10. Training data loss:  5756.2568359375\n",
      "Epoch: 11. Training data loss:  5749.890625\n",
      "Epoch: 12. Training data loss:  5747.79541015625\n",
      "Epoch: 13. Training data loss:  5728.4853515625\n",
      "Epoch: 14. Training data loss:  5733.44873046875\n",
      "Epoch: 15. Training data loss:  5730.7939453125\n",
      "Epoch: 16. Training data loss:  5719.669921875\n",
      "Epoch: 17. Training data loss:  5714.158203125\n",
      "Epoch: 18. Training data loss:  5712.0634765625\n",
      "Epoch: 19. Training data loss:  5706.6533203125\n",
      "Epoch: 20. Training data loss:  5705.689453125\n",
      "Epoch: 21. Training data loss:  5705.1806640625\n",
      "Epoch: 22. Training data loss:  5695.13720703125\n",
      "Epoch: 23. Training data loss:  5695.591796875\n",
      "Epoch: 24. Training data loss:  5699.20556640625\n",
      "Epoch: 25. Training data loss:  5691.34765625\n",
      "Epoch: 26. Training data loss:  5689.134765625\n",
      "Epoch: 27. Training data loss:  5686.5703125\n",
      "Epoch: 28. Training data loss:  5682.0458984375\n",
      "Epoch: 29. Training data loss:  5684.24609375\n",
      "Epoch: 30. Training data loss:  5685.36181640625\n",
      "Epoch: 31. Training data loss:  5678.853515625\n",
      "Epoch: 32. Training data loss:  5682.990234375\n",
      "Epoch: 33. Training data loss:  5683.91650390625\n",
      "Epoch: 34. Training data loss:  5684.31298828125\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_mln = MultiLayerNet(3072, 128, 100)\n",
    "train(model_mln, trainloader, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  6527.65966796875\n",
      "Epoch: 1. Training data loss:  6036.203125\n",
      "Epoch: 2. Training data loss:  5817.775390625\n",
      "Epoch: 3. Training data loss:  5664.41748046875\n",
      "Epoch: 4. Training data loss:  5560.21484375\n",
      "Epoch: 5. Training data loss:  5479.48193359375\n",
      "Epoch: 6. Training data loss:  5416.36669921875\n",
      "Epoch: 7. Training data loss:  5374.66064453125\n",
      "Epoch: 8. Training data loss:  5344.7939453125\n",
      "Epoch: 9. Training data loss:  5311.9306640625\n",
      "Epoch: 10. Training data loss:  5280.85107421875\n",
      "Epoch: 11. Training data loss:  5246.8046875\n",
      "Epoch: 12. Training data loss:  5216.41845703125\n",
      "Epoch: 13. Training data loss:  5183.20751953125\n",
      "Epoch: 14. Training data loss:  5159.96337890625\n",
      "Epoch: 15. Training data loss:  5127.01513671875\n",
      "Epoch: 16. Training data loss:  5111.9482421875\n",
      "Epoch: 17. Training data loss:  5098.5859375\n",
      "Epoch: 18. Training data loss:  5086.06640625\n",
      "Epoch: 19. Training data loss:  5060.2890625\n",
      "Epoch: 20. Training data loss:  5050.97216796875\n",
      "Epoch: 21. Training data loss:  5033.95654296875\n",
      "Epoch: 22. Training data loss:  5016.68505859375\n",
      "Epoch: 23. Training data loss:  5006.96728515625\n",
      "Epoch: 24. Training data loss:  4992.66552734375\n",
      "Epoch: 25. Training data loss:  4974.31640625\n",
      "Epoch: 26. Training data loss:  4985.9033203125\n",
      "Epoch: 27. Training data loss:  4966.70751953125\n",
      "Epoch: 28. Training data loss:  4958.642578125\n",
      "Epoch: 29. Training data loss:  4946.94189453125\n",
      "Epoch: 30. Training data loss:  4945.06982421875\n",
      "Epoch: 31. Training data loss:  4938.06103515625\n",
      "Epoch: 32. Training data loss:  4920.49267578125\n",
      "Epoch: 33. Training data loss:  4919.658203125\n",
      "Epoch: 34. Training data loss:  4904.7216796875\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn2_model = ConvNet_2()\n",
    "train(cnn2_model, trainloader, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the network on the 10000 test images: 17 %', 17.76)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Accuracy of apple : 40 %',\n",
       " 'Accuracy of aquarium_fish : 34 %',\n",
       " 'Accuracy of  baby :  3 %',\n",
       " 'Accuracy of  bear : 34 %',\n",
       " 'Accuracy of beaver :  9 %',\n",
       " 'Accuracy of   bed : 18 %',\n",
       " 'Accuracy of   bee : 15 %',\n",
       " 'Accuracy of beetle : 19 %',\n",
       " 'Accuracy of bicycle :  9 %',\n",
       " 'Accuracy of bottle : 27 %',\n",
       " 'Accuracy of  bowl :  1 %',\n",
       " 'Accuracy of   boy :  2 %',\n",
       " 'Accuracy of bridge :  4 %',\n",
       " 'Accuracy of   bus :  0 %',\n",
       " 'Accuracy of butterfly : 12 %',\n",
       " 'Accuracy of camel :  0 %',\n",
       " 'Accuracy of   can : 16 %',\n",
       " 'Accuracy of castle : 27 %',\n",
       " 'Accuracy of caterpillar : 13 %',\n",
       " 'Accuracy of cattle : 11 %',\n",
       " 'Accuracy of chair : 34 %',\n",
       " 'Accuracy of chimpanzee : 24 %',\n",
       " 'Accuracy of clock : 12 %',\n",
       " 'Accuracy of cloud : 46 %',\n",
       " 'Accuracy of cockroach : 51 %',\n",
       " 'Accuracy of couch :  4 %',\n",
       " 'Accuracy of  crab :  5 %',\n",
       " 'Accuracy of crocodile : 14 %',\n",
       " 'Accuracy of   cup : 13 %',\n",
       " 'Accuracy of dinosaur :  1 %',\n",
       " 'Accuracy of dolphin : 53 %',\n",
       " 'Accuracy of elephant : 13 %',\n",
       " 'Accuracy of flatfish :  6 %',\n",
       " 'Accuracy of forest : 31 %',\n",
       " 'Accuracy of   fox :  5 %',\n",
       " 'Accuracy of  girl :  4 %',\n",
       " 'Accuracy of hamster : 28 %',\n",
       " 'Accuracy of house : 10 %',\n",
       " 'Accuracy of kangaroo : 14 %',\n",
       " 'Accuracy of keyboard :  3 %',\n",
       " 'Accuracy of  lamp :  7 %',\n",
       " 'Accuracy of lawn_mower : 34 %',\n",
       " 'Accuracy of leopard : 19 %',\n",
       " 'Accuracy of  lion : 43 %',\n",
       " 'Accuracy of lizard :  6 %',\n",
       " 'Accuracy of lobster : 11 %',\n",
       " 'Accuracy of   man : 10 %',\n",
       " 'Accuracy of maple_tree : 26 %',\n",
       " 'Accuracy of motorcycle : 18 %',\n",
       " 'Accuracy of mountain : 20 %',\n",
       " 'Accuracy of mouse :  0 %',\n",
       " 'Accuracy of mushroom :  9 %',\n",
       " 'Accuracy of oak_tree : 71 %',\n",
       " 'Accuracy of orange : 69 %',\n",
       " 'Accuracy of orchid : 36 %',\n",
       " 'Accuracy of otter :  1 %',\n",
       " 'Accuracy of palm_tree : 12 %',\n",
       " 'Accuracy of  pear : 16 %',\n",
       " 'Accuracy of pickup_truck : 29 %',\n",
       " 'Accuracy of pine_tree : 11 %',\n",
       " 'Accuracy of plain : 54 %',\n",
       " 'Accuracy of plate : 36 %',\n",
       " 'Accuracy of poppy : 28 %',\n",
       " 'Accuracy of porcupine :  1 %',\n",
       " 'Accuracy of possum :  3 %',\n",
       " 'Accuracy of rabbit :  1 %',\n",
       " 'Accuracy of raccoon :  2 %',\n",
       " 'Accuracy of   ray : 17 %',\n",
       " 'Accuracy of  road : 68 %',\n",
       " 'Accuracy of rocket :  7 %',\n",
       " 'Accuracy of  rose : 24 %',\n",
       " 'Accuracy of   sea : 22 %',\n",
       " 'Accuracy of  seal :  0 %',\n",
       " 'Accuracy of shark : 19 %',\n",
       " 'Accuracy of shrew : 19 %',\n",
       " 'Accuracy of skunk : 28 %',\n",
       " 'Accuracy of skyscraper : 49 %',\n",
       " 'Accuracy of snail :  2 %',\n",
       " 'Accuracy of snake :  4 %',\n",
       " 'Accuracy of spider :  4 %',\n",
       " 'Accuracy of squirrel :  2 %',\n",
       " 'Accuracy of streetcar :  4 %',\n",
       " 'Accuracy of sunflower : 43 %',\n",
       " 'Accuracy of sweet_pepper :  0 %',\n",
       " 'Accuracy of table :  0 %',\n",
       " 'Accuracy of  tank : 14 %',\n",
       " 'Accuracy of telephone : 27 %',\n",
       " 'Accuracy of television : 15 %',\n",
       " 'Accuracy of tiger :  4 %',\n",
       " 'Accuracy of tractor :  4 %',\n",
       " 'Accuracy of train :  3 %',\n",
       " 'Accuracy of trout : 49 %',\n",
       " 'Accuracy of tulip :  7 %',\n",
       " 'Accuracy of turtle :  8 %',\n",
       " 'Accuracy of wardrobe : 30 %',\n",
       " 'Accuracy of whale : 19 %',\n",
       " 'Accuracy of willow_tree : 14 %',\n",
       " 'Accuracy of  wolf :  3 %',\n",
       " 'Accuracy of woman :  4 %',\n",
       " 'Accuracy of  worm : 25 %']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid(model_mln,testloader))\n",
    "valid_class(model_mln,testloader,classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of the network on the 10000 test images: 26 %', 26.57)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Accuracy of apple : 50 %',\n",
       " 'Accuracy of aquarium_fish : 49 %',\n",
       " 'Accuracy of  baby : 12 %',\n",
       " 'Accuracy of  bear : 10 %',\n",
       " 'Accuracy of beaver : 24 %',\n",
       " 'Accuracy of   bed : 10 %',\n",
       " 'Accuracy of   bee : 32 %',\n",
       " 'Accuracy of beetle : 27 %',\n",
       " 'Accuracy of bicycle : 29 %',\n",
       " 'Accuracy of bottle : 29 %',\n",
       " 'Accuracy of  bowl : 14 %',\n",
       " 'Accuracy of   boy :  8 %',\n",
       " 'Accuracy of bridge : 25 %',\n",
       " 'Accuracy of   bus : 25 %',\n",
       " 'Accuracy of butterfly : 18 %',\n",
       " 'Accuracy of camel : 22 %',\n",
       " 'Accuracy of   can : 13 %',\n",
       " 'Accuracy of castle : 60 %',\n",
       " 'Accuracy of caterpillar : 44 %',\n",
       " 'Accuracy of cattle :  2 %',\n",
       " 'Accuracy of chair : 52 %',\n",
       " 'Accuracy of chimpanzee : 49 %',\n",
       " 'Accuracy of clock : 20 %',\n",
       " 'Accuracy of cloud : 65 %',\n",
       " 'Accuracy of cockroach : 68 %',\n",
       " 'Accuracy of couch : 22 %',\n",
       " 'Accuracy of  crab :  4 %',\n",
       " 'Accuracy of crocodile : 17 %',\n",
       " 'Accuracy of   cup : 31 %',\n",
       " 'Accuracy of dinosaur : 19 %',\n",
       " 'Accuracy of dolphin : 39 %',\n",
       " 'Accuracy of elephant : 14 %',\n",
       " 'Accuracy of flatfish : 20 %',\n",
       " 'Accuracy of forest : 25 %',\n",
       " 'Accuracy of   fox : 12 %',\n",
       " 'Accuracy of  girl : 12 %',\n",
       " 'Accuracy of hamster : 36 %',\n",
       " 'Accuracy of house : 10 %',\n",
       " 'Accuracy of kangaroo :  8 %',\n",
       " 'Accuracy of keyboard : 21 %',\n",
       " 'Accuracy of  lamp :  7 %',\n",
       " 'Accuracy of lawn_mower : 53 %',\n",
       " 'Accuracy of leopard : 24 %',\n",
       " 'Accuracy of  lion : 39 %',\n",
       " 'Accuracy of lizard :  3 %',\n",
       " 'Accuracy of lobster : 18 %',\n",
       " 'Accuracy of   man :  6 %',\n",
       " 'Accuracy of maple_tree : 26 %',\n",
       " 'Accuracy of motorcycle : 48 %',\n",
       " 'Accuracy of mountain : 40 %',\n",
       " 'Accuracy of mouse :  0 %',\n",
       " 'Accuracy of mushroom :  3 %',\n",
       " 'Accuracy of oak_tree : 81 %',\n",
       " 'Accuracy of orange : 69 %',\n",
       " 'Accuracy of orchid : 48 %',\n",
       " 'Accuracy of otter :  0 %',\n",
       " 'Accuracy of palm_tree : 29 %',\n",
       " 'Accuracy of  pear : 21 %',\n",
       " 'Accuracy of pickup_truck : 37 %',\n",
       " 'Accuracy of pine_tree : 17 %',\n",
       " 'Accuracy of plain : 83 %',\n",
       " 'Accuracy of plate : 27 %',\n",
       " 'Accuracy of poppy : 47 %',\n",
       " 'Accuracy of porcupine : 19 %',\n",
       " 'Accuracy of possum :  7 %',\n",
       " 'Accuracy of rabbit :  3 %',\n",
       " 'Accuracy of raccoon :  4 %',\n",
       " 'Accuracy of   ray : 15 %',\n",
       " 'Accuracy of  road : 45 %',\n",
       " 'Accuracy of rocket : 36 %',\n",
       " 'Accuracy of  rose : 31 %',\n",
       " 'Accuracy of   sea : 41 %',\n",
       " 'Accuracy of  seal :  4 %',\n",
       " 'Accuracy of shark : 29 %',\n",
       " 'Accuracy of shrew : 10 %',\n",
       " 'Accuracy of skunk : 57 %',\n",
       " 'Accuracy of skyscraper : 50 %',\n",
       " 'Accuracy of snail : 15 %',\n",
       " 'Accuracy of snake :  3 %',\n",
       " 'Accuracy of spider : 15 %',\n",
       " 'Accuracy of squirrel :  0 %',\n",
       " 'Accuracy of streetcar : 16 %',\n",
       " 'Accuracy of sunflower : 63 %',\n",
       " 'Accuracy of sweet_pepper : 23 %',\n",
       " 'Accuracy of table :  3 %',\n",
       " 'Accuracy of  tank : 40 %',\n",
       " 'Accuracy of telephone : 28 %',\n",
       " 'Accuracy of television : 24 %',\n",
       " 'Accuracy of tiger : 18 %',\n",
       " 'Accuracy of tractor : 45 %',\n",
       " 'Accuracy of train : 19 %',\n",
       " 'Accuracy of trout : 57 %',\n",
       " 'Accuracy of tulip : 25 %',\n",
       " 'Accuracy of turtle : 10 %',\n",
       " 'Accuracy of wardrobe : 52 %',\n",
       " 'Accuracy of whale : 36 %',\n",
       " 'Accuracy of willow_tree : 15 %',\n",
       " 'Accuracy of  wolf :  6 %',\n",
       " 'Accuracy of woman : 13 %',\n",
       " 'Accuracy of  worm :  7 %']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid(cnn2_model,testloader))\n",
    "valid_class(cnn2_model,testloader,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 6, 28, 28]           456\n",
      "├─BatchNorm2d: 1-2                       [-1, 6, 28, 28]           12\n",
      "├─AvgPool2d: 1-3                         [-1, 6, 14, 14]           --\n",
      "├─Conv2d: 1-4                            [-1, 16, 10, 10]          2,416\n",
      "├─AvgPool2d: 1-5                         [-1, 16, 5, 5]            --\n",
      "├─Linear: 1-6                            [-1, 120]                 48,120\n",
      "├─Linear: 1-7                            [-1, 84]                  10,164\n",
      "├─Dropout: 1-8                           [-1, 84]                  --\n",
      "├─Linear: 1-9                            [-1, 100]                 8,500\n",
      "==========================================================================================\n",
      "Total params: 69,668\n",
      "Trainable params: 69,668\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.66\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 0.27\n",
      "Estimated Total Size (MB): 0.36\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Linear: 1-1                            [-1, 128]                 393,344\n",
      "├─BatchNorm1d: 1-2                       [-1, 128]                 256\n",
      "├─Dropout: 1-3                           [-1, 128]                 --\n",
      "├─Linear: 1-4                            [-1, 64]                  8,256\n",
      "├─Linear: 1-5                            [-1, 100]                 6,500\n",
      "==========================================================================================\n",
      "Total params: 408,356\n",
      "Trainable params: 408,356\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.41\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 1.56\n",
      "Estimated Total Size (MB): 1.57\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Linear: 1-1                            [-1, 128]                 393,344\n",
       "├─BatchNorm1d: 1-2                       [-1, 128]                 256\n",
       "├─Dropout: 1-3                           [-1, 128]                 --\n",
       "├─Linear: 1-4                            [-1, 64]                  8,256\n",
       "├─Linear: 1-5                            [-1, 100]                 6,500\n",
       "==========================================================================================\n",
       "Total params: 408,356\n",
       "Trainable params: 408,356\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.41\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 1.56\n",
       "Estimated Total Size (MB): 1.57\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(cnn2_model, (3, 32, 32))\n",
    "summary(model_mln, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runing Random Search on all 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration, 0\n",
      "Epoch: 0. Training data loss:  3383.245361328125\n",
      "Epoch: 1. Training data loss:  3256.18017578125\n",
      "Epoch: 2. Training data loss:  3205.184814453125\n",
      "Epoch: 3. Training data loss:  3171.470458984375\n",
      "Epoch: 4. Training data loss:  3144.24853515625\n",
      "Epoch: 5. Training data loss:  3128.993408203125\n",
      "Epoch: 6. Training data loss:  3119.153076171875\n",
      "Epoch: 7. Training data loss:  3112.392822265625\n",
      "Epoch: 8. Training data loss:  3106.319580078125\n",
      "Epoch: 9. Training data loss:  3104.394775390625\n",
      "Epoch: 10. Training data loss:  3098.102294921875\n",
      "Epoch: 11. Training data loss:  3081.782958984375\n",
      "Epoch: 12. Training data loss:  3091.537353515625\n",
      "Epoch: 13. Training data loss:  3073.482421875\n",
      "Epoch: 14. Training data loss:  3074.66162109375\n",
      "Epoch: 15. Training data loss:  3072.939453125\n",
      "Epoch: 16. Training data loss:  3068.962646484375\n",
      "Epoch: 17. Training data loss:  3060.89990234375\n",
      "Epoch: 18. Training data loss:  3061.76318359375\n",
      "Epoch: 19. Training data loss:  3070.16650390625\n",
      "Epoch: 20. Training data loss:  3055.360595703125\n",
      "Epoch: 21. Training data loss:  3057.47802734375\n",
      "Epoch: 22. Training data loss:  3054.412109375\n",
      "Epoch: 23. Training data loss:  3049.185546875\n",
      "Epoch: 24. Training data loss:  3051.02197265625\n",
      "Epoch: 25. Training data loss:  3045.782958984375\n",
      "Epoch: 26. Training data loss:  3043.92431640625\n",
      "Epoch: 27. Training data loss:  3044.529052734375\n",
      "Epoch: 28. Training data loss:  3042.900146484375\n",
      "Epoch: 29. Training data loss:  3033.68310546875\n",
      "Epoch: 30. Training data loss:  3035.9560546875\n",
      "Epoch: 31. Training data loss:  3042.789794921875\n",
      "Epoch: 32. Training data loss:  3039.373779296875\n",
      "Epoch: 33. Training data loss:  3038.6416015625\n",
      "Epoch: 34. Training data loss:  3039.34716796875\n",
      "Epoch: 35. Training data loss:  3035.27587890625\n",
      "Epoch: 36. Training data loss:  3039.09619140625\n",
      "Epoch: 37. Training data loss:  3035.454345703125\n",
      "Epoch: 38. Training data loss:  3027.974609375\n",
      "Epoch: 39. Training data loss:  3034.452880859375\n",
      "Accuracy found: 10.84\n",
      "40 0.01 0.001 CrossEntropyLoss() 64 1\n",
      "Starting iteration, 1\n",
      "Epoch: 0. Training data loss:  1802.01318359375\n",
      "Epoch: 1. Training data loss:  1801.9169921875\n",
      "Epoch: 2. Training data loss:  1801.906982421875\n",
      "Epoch: 3. Training data loss:  1801.9444580078125\n",
      "Epoch: 4. Training data loss:  1801.8699951171875\n",
      "Epoch: 5. Training data loss:  1801.8214111328125\n",
      "Epoch: 6. Training data loss:  1801.773193359375\n",
      "Epoch: 7. Training data loss:  1801.753662109375\n",
      "Epoch: 8. Training data loss:  1801.6768798828125\n",
      "Epoch: 9. Training data loss:  1801.596923828125\n",
      "Epoch: 10. Training data loss:  1801.5948486328125\n",
      "Epoch: 11. Training data loss:  1801.5732421875\n",
      "Epoch: 12. Training data loss:  1801.5091552734375\n",
      "Epoch: 13. Training data loss:  1801.48046875\n",
      "Epoch: 14. Training data loss:  1801.496826171875\n",
      "Epoch: 15. Training data loss:  1801.3634033203125\n",
      "Epoch: 16. Training data loss:  1801.3521728515625\n",
      "Epoch: 17. Training data loss:  1801.29150390625\n",
      "Epoch: 18. Training data loss:  1801.2569580078125\n",
      "Epoch: 19. Training data loss:  1801.2786865234375\n",
      "Epoch: 20. Training data loss:  1801.216064453125\n",
      "Epoch: 21. Training data loss:  1801.1138916015625\n",
      "Epoch: 22. Training data loss:  1801.1834716796875\n",
      "Epoch: 23. Training data loss:  1801.038330078125\n",
      "Epoch: 24. Training data loss:  1801.0079345703125\n",
      "Epoch: 25. Training data loss:  1800.9583740234375\n",
      "Epoch: 26. Training data loss:  1800.9931640625\n",
      "Epoch: 27. Training data loss:  1800.8460693359375\n",
      "Epoch: 28. Training data loss:  1800.8280029296875\n",
      "Epoch: 29. Training data loss:  1800.8282470703125\n",
      "Epoch: 30. Training data loss:  1800.817138671875\n",
      "Epoch: 31. Training data loss:  1800.6751708984375\n",
      "Epoch: 32. Training data loss:  1800.6412353515625\n",
      "Epoch: 33. Training data loss:  1800.601318359375\n",
      "Epoch: 34. Training data loss:  1800.6162109375\n",
      "Epoch: 35. Training data loss:  1800.5030517578125\n",
      "Epoch: 36. Training data loss:  1800.4901123046875\n",
      "Epoch: 37. Training data loss:  1800.429443359375\n",
      "Epoch: 38. Training data loss:  1800.4022216796875\n",
      "Epoch: 39. Training data loss:  1800.3333740234375\n",
      "Epoch: 40. Training data loss:  1800.308349609375\n",
      "Epoch: 41. Training data loss:  1800.314697265625\n",
      "Epoch: 42. Training data loss:  1800.27197265625\n",
      "Epoch: 43. Training data loss:  1800.1695556640625\n",
      "Epoch: 44. Training data loss:  1800.153076171875\n",
      "Epoch: 45. Training data loss:  1800.0985107421875\n",
      "Epoch: 46. Training data loss:  1800.073486328125\n",
      "Epoch: 47. Training data loss:  1799.9561767578125\n",
      "Epoch: 48. Training data loss:  1799.93310546875\n",
      "Epoch: 49. Training data loss:  1799.8487548828125\n",
      "Epoch: 50. Training data loss:  1799.8917236328125\n",
      "Epoch: 51. Training data loss:  1799.7659912109375\n",
      "Epoch: 52. Training data loss:  1799.7589111328125\n",
      "Epoch: 53. Training data loss:  1799.7110595703125\n",
      "Epoch: 54. Training data loss:  1799.6356201171875\n",
      "Epoch: 55. Training data loss:  1799.5653076171875\n",
      "Epoch: 56. Training data loss:  1799.4871826171875\n",
      "Epoch: 57. Training data loss:  1799.5584716796875\n",
      "Epoch: 58. Training data loss:  1799.4149169921875\n",
      "Epoch: 59. Training data loss:  1799.4376220703125\n",
      "Accuracy found: 1.1\n",
      "60 0.0001 0.001 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [10, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  5686.49072265625\n",
      "Epoch: 1. Training data loss:  5802.82177734375\n",
      "Epoch: 2. Training data loss:  5720.640625\n",
      "Epoch: 3. Training data loss:  8140.10693359375\n",
      "Epoch: 4. Training data loss:  5956.185546875\n",
      "Epoch: 5. Training data loss:  5933.30810546875\n",
      "Epoch: 6. Training data loss:  6054.74169921875\n",
      "Epoch: 7. Training data loss:  6005.53955078125\n",
      "Epoch: 8. Training data loss:  7223.82666015625\n",
      "Epoch: 9. Training data loss:  6967.765625\n",
      "Epoch: 10. Training data loss:  6080.21484375\n",
      "Epoch: 11. Training data loss:  6224.810546875\n",
      "Epoch: 12. Training data loss:  6425.63720703125\n",
      "Epoch: 13. Training data loss:  6448.146484375\n",
      "Epoch: 14. Training data loss:  6202.07763671875\n",
      "Epoch: 15. Training data loss:  6449.3623046875\n",
      "Epoch: 16. Training data loss:  6677.62890625\n",
      "Epoch: 17. Training data loss:  6104.10888671875\n",
      "Epoch: 18. Training data loss:  5937.67333984375\n",
      "Epoch: 19. Training data loss:  6468.13916015625\n",
      "Accuracy found: 3.14\n",
      "20 0.01 0 MultiMarginLoss() 10 1\n",
      "Starting iteration, 3\n",
      "Epoch: 0. Training data loss:  7204.66796875\n",
      "Epoch: 1. Training data loss:  7203.93603515625\n",
      "Epoch: 2. Training data loss:  7203.419921875\n",
      "Epoch: 3. Training data loss:  7202.794921875\n",
      "Epoch: 4. Training data loss:  7201.90478515625\n",
      "Epoch: 5. Training data loss:  7201.4833984375\n",
      "Epoch: 6. Training data loss:  7200.7099609375\n",
      "Epoch: 7. Training data loss:  7199.943359375\n",
      "Epoch: 8. Training data loss:  7199.48876953125\n",
      "Epoch: 9. Training data loss:  7198.70751953125\n",
      "Epoch: 10. Training data loss:  7198.296875\n",
      "Epoch: 11. Training data loss:  7197.5107421875\n",
      "Epoch: 12. Training data loss:  7196.9560546875\n",
      "Epoch: 13. Training data loss:  7196.30126953125\n",
      "Epoch: 14. Training data loss:  7195.3212890625\n",
      "Epoch: 15. Training data loss:  7194.81982421875\n",
      "Epoch: 16. Training data loss:  7194.11181640625\n",
      "Epoch: 17. Training data loss:  7193.380859375\n",
      "Epoch: 18. Training data loss:  7192.37890625\n",
      "Epoch: 19. Training data loss:  7191.990234375\n",
      "Epoch: 20. Training data loss:  7191.1513671875\n",
      "Epoch: 21. Training data loss:  7190.173828125\n",
      "Epoch: 22. Training data loss:  7189.35009765625\n",
      "Epoch: 23. Training data loss:  7188.66455078125\n",
      "Epoch: 24. Training data loss:  7187.7626953125\n",
      "Epoch: 25. Training data loss:  7186.9228515625\n",
      "Epoch: 26. Training data loss:  7185.89111328125\n",
      "Epoch: 27. Training data loss:  7185.0693359375\n",
      "Epoch: 28. Training data loss:  7184.193359375\n",
      "Epoch: 29. Training data loss:  7183.4501953125\n",
      "Epoch: 30. Training data loss:  7182.36669921875\n",
      "Epoch: 31. Training data loss:  7180.89501953125\n",
      "Epoch: 32. Training data loss:  7180.10107421875\n",
      "Epoch: 33. Training data loss:  7178.908203125\n",
      "Epoch: 34. Training data loss:  7177.5234375\n",
      "Epoch: 35. Training data loss:  7176.2197265625\n",
      "Epoch: 36. Training data loss:  7175.3076171875\n",
      "Epoch: 37. Training data loss:  7174.00537109375\n",
      "Epoch: 38. Training data loss:  7172.89306640625\n",
      "Epoch: 39. Training data loss:  7171.6474609375\n",
      "Accuracy found: 1.84\n",
      "40 0.0001 0.001 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [128, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  387.0663146972656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [80, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Training data loss:  387.121826171875\n",
      "Epoch: 2. Training data loss:  387.1080017089844\n",
      "Epoch: 3. Training data loss:  387.1147766113281\n",
      "Epoch: 4. Training data loss:  387.1103210449219\n",
      "Epoch: 5. Training data loss:  387.1205139160156\n",
      "Epoch: 6. Training data loss:  387.1171875\n",
      "Epoch: 7. Training data loss:  387.1141357421875\n",
      "Epoch: 8. Training data loss:  387.119140625\n",
      "Epoch: 9. Training data loss:  387.1056213378906\n",
      "Epoch: 10. Training data loss:  387.1069030761719\n",
      "Epoch: 11. Training data loss:  387.10382080078125\n",
      "Epoch: 12. Training data loss:  387.1143798828125\n",
      "Epoch: 13. Training data loss:  387.12060546875\n",
      "Epoch: 14. Training data loss:  387.1211242675781\n",
      "Epoch: 15. Training data loss:  387.119140625\n",
      "Epoch: 16. Training data loss:  387.10626220703125\n",
      "Epoch: 17. Training data loss:  387.1141357421875\n",
      "Epoch: 18. Training data loss:  387.1117248535156\n",
      "Epoch: 19. Training data loss:  387.1081848144531\n",
      "Epoch: 20. Training data loss:  387.1223449707031\n",
      "Epoch: 21. Training data loss:  387.11236572265625\n",
      "Epoch: 22. Training data loss:  387.11883544921875\n",
      "Epoch: 23. Training data loss:  387.1195068359375\n",
      "Epoch: 24. Training data loss:  387.11627197265625\n",
      "Epoch: 25. Training data loss:  387.11285400390625\n",
      "Epoch: 26. Training data loss:  387.109130859375\n",
      "Epoch: 27. Training data loss:  387.1142578125\n",
      "Epoch: 28. Training data loss:  387.1181945800781\n",
      "Epoch: 29. Training data loss:  387.111083984375\n",
      "Epoch: 30. Training data loss:  387.1224060058594\n",
      "Epoch: 31. Training data loss:  387.11407470703125\n",
      "Epoch: 32. Training data loss:  387.1127624511719\n",
      "Epoch: 33. Training data loss:  387.1195373535156\n",
      "Epoch: 34. Training data loss:  387.1143798828125\n",
      "Epoch: 35. Training data loss:  387.1134033203125\n",
      "Epoch: 36. Training data loss:  387.12811279296875\n",
      "Epoch: 37. Training data loss:  387.1134338378906\n",
      "Epoch: 38. Training data loss:  387.1122741699219\n",
      "Epoch: 39. Training data loss:  387.1209411621094\n",
      "Epoch: 40. Training data loss:  387.1167297363281\n",
      "Epoch: 41. Training data loss:  387.1139831542969\n",
      "Epoch: 42. Training data loss:  387.11614990234375\n",
      "Epoch: 43. Training data loss:  387.1042175292969\n",
      "Epoch: 44. Training data loss:  387.1107177734375\n",
      "Epoch: 45. Training data loss:  387.1141357421875\n",
      "Epoch: 46. Training data loss:  387.1142883300781\n",
      "Epoch: 47. Training data loss:  387.1123962402344\n",
      "Epoch: 48. Training data loss:  387.1058044433594\n",
      "Epoch: 49. Training data loss:  387.1172790527344\n",
      "Epoch: 50. Training data loss:  387.1141357421875\n",
      "Epoch: 51. Training data loss:  387.11181640625\n",
      "Epoch: 52. Training data loss:  387.12115478515625\n",
      "Epoch: 53. Training data loss:  387.117919921875\n",
      "Epoch: 54. Training data loss:  387.11260986328125\n",
      "Epoch: 55. Training data loss:  387.1160583496094\n",
      "Epoch: 56. Training data loss:  387.10174560546875\n",
      "Epoch: 57. Training data loss:  387.11376953125\n",
      "Epoch: 58. Training data loss:  387.10498046875\n",
      "Epoch: 59. Training data loss:  387.113037109375\n",
      "Accuracy found: 1.0\n",
      "60 0.001 0.3 MultiMarginLoss() 128 1\n",
      "Starting iteration, 5\n",
      "Epoch: 0. Training data loss:  1722.136474609375\n",
      "Epoch: 1. Training data loss:  1651.1561279296875\n",
      "Epoch: 2. Training data loss:  1620.7752685546875\n",
      "Epoch: 3. Training data loss:  1586.9503173828125\n",
      "Epoch: 4. Training data loss:  1556.22265625\n",
      "Epoch: 5. Training data loss:  1533.96337890625\n",
      "Epoch: 6. Training data loss:  1515.8338623046875\n",
      "Epoch: 7. Training data loss:  1500.4349365234375\n",
      "Epoch: 8. Training data loss:  1486.0814208984375\n",
      "Epoch: 9. Training data loss:  1472.2484130859375\n",
      "Epoch: 10. Training data loss:  1458.6251220703125\n",
      "Epoch: 11. Training data loss:  1445.6676025390625\n",
      "Epoch: 12. Training data loss:  1432.9896240234375\n",
      "Epoch: 13. Training data loss:  1420.8687744140625\n",
      "Epoch: 14. Training data loss:  1409.2322998046875\n",
      "Epoch: 15. Training data loss:  1398.149169921875\n",
      "Epoch: 16. Training data loss:  1387.918701171875\n",
      "Epoch: 17. Training data loss:  1379.0374755859375\n",
      "Epoch: 18. Training data loss:  1370.51513671875\n",
      "Epoch: 19. Training data loss:  1361.8173828125\n",
      "Epoch: 20. Training data loss:  1353.9508056640625\n",
      "Epoch: 21. Training data loss:  1345.838134765625\n",
      "Epoch: 22. Training data loss:  1338.31396484375\n",
      "Epoch: 23. Training data loss:  1331.1488037109375\n",
      "Epoch: 24. Training data loss:  1325.25927734375\n",
      "Epoch: 25. Training data loss:  1318.029541015625\n",
      "Epoch: 26. Training data loss:  1311.14404296875\n",
      "Epoch: 27. Training data loss:  1305.5357666015625\n",
      "Epoch: 28. Training data loss:  1299.8701171875\n",
      "Epoch: 29. Training data loss:  1293.7608642578125\n",
      "Epoch: 30. Training data loss:  1288.8900146484375\n",
      "Epoch: 31. Training data loss:  1283.6456298828125\n",
      "Epoch: 32. Training data loss:  1278.5849609375\n",
      "Epoch: 33. Training data loss:  1273.499755859375\n",
      "Epoch: 34. Training data loss:  1268.864013671875\n",
      "Epoch: 35. Training data loss:  1265.0308837890625\n",
      "Epoch: 36. Training data loss:  1259.8472900390625\n",
      "Epoch: 37. Training data loss:  1255.2103271484375\n",
      "Epoch: 38. Training data loss:  1251.9488525390625\n",
      "Epoch: 39. Training data loss:  1246.94580078125\n",
      "Accuracy found: 21.8\n",
      "40 0.1 0.001 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 6\n",
      "Epoch: 0. Training data loss:  387.79693603515625\n",
      "Epoch: 1. Training data loss:  387.17535400390625\n",
      "Epoch: 2. Training data loss:  386.5803527832031\n",
      "Epoch: 3. Training data loss:  385.99395751953125\n",
      "Epoch: 4. Training data loss:  385.3762512207031\n",
      "Epoch: 5. Training data loss:  384.8196716308594\n",
      "Epoch: 6. Training data loss:  384.1408996582031\n",
      "Epoch: 7. Training data loss:  383.44061279296875\n",
      "Epoch: 8. Training data loss:  382.6462707519531\n",
      "Epoch: 9. Training data loss:  381.766845703125\n",
      "Epoch: 10. Training data loss:  380.7483215332031\n",
      "Epoch: 11. Training data loss:  379.6209411621094\n",
      "Epoch: 12. Training data loss:  378.2148132324219\n",
      "Epoch: 13. Training data loss:  376.7809753417969\n",
      "Epoch: 14. Training data loss:  375.0831604003906\n",
      "Epoch: 15. Training data loss:  373.2937927246094\n",
      "Epoch: 16. Training data loss:  371.2998046875\n",
      "Epoch: 17. Training data loss:  369.0108337402344\n",
      "Epoch: 18. Training data loss:  366.7273864746094\n",
      "Epoch: 19. Training data loss:  364.279541015625\n",
      "Epoch: 20. Training data loss:  361.7737121582031\n",
      "Epoch: 21. Training data loss:  359.2848205566406\n",
      "Epoch: 22. Training data loss:  356.6744079589844\n",
      "Epoch: 23. Training data loss:  354.2626037597656\n",
      "Epoch: 24. Training data loss:  351.8062438964844\n",
      "Epoch: 25. Training data loss:  349.4661560058594\n",
      "Epoch: 26. Training data loss:  347.0738830566406\n",
      "Epoch: 27. Training data loss:  344.9298095703125\n",
      "Epoch: 28. Training data loss:  342.61224365234375\n",
      "Epoch: 29. Training data loss:  340.5091247558594\n",
      "Epoch: 30. Training data loss:  338.6502380371094\n",
      "Epoch: 31. Training data loss:  336.716064453125\n",
      "Epoch: 32. Training data loss:  334.44586181640625\n",
      "Epoch: 33. Training data loss:  332.51361083984375\n",
      "Epoch: 34. Training data loss:  330.51971435546875\n",
      "Epoch: 35. Training data loss:  328.5225830078125\n",
      "Epoch: 36. Training data loss:  326.587646484375\n",
      "Epoch: 37. Training data loss:  324.4866638183594\n",
      "Epoch: 38. Training data loss:  322.5329284667969\n",
      "Epoch: 39. Training data loss:  320.46868896484375\n",
      "Accuracy found: 3.06\n",
      "40 0.001 0.01 MultiMarginLoss() 128 0\n",
      "Starting iteration, 7\n",
      "Epoch: 0. Training data loss:  1802.0977783203125\n",
      "Epoch: 1. Training data loss:  1802.0184326171875\n",
      "Epoch: 2. Training data loss:  1801.9832763671875\n",
      "Epoch: 3. Training data loss:  1801.8702392578125\n",
      "Epoch: 4. Training data loss:  1801.7088623046875\n",
      "Epoch: 5. Training data loss:  1801.6749267578125\n",
      "Epoch: 6. Training data loss:  1801.6124267578125\n",
      "Epoch: 7. Training data loss:  1801.45703125\n",
      "Epoch: 8. Training data loss:  1801.4296875\n",
      "Epoch: 9. Training data loss:  1801.3333740234375\n",
      "Epoch: 10. Training data loss:  1801.283935546875\n",
      "Epoch: 11. Training data loss:  1801.198486328125\n",
      "Epoch: 12. Training data loss:  1801.0654296875\n",
      "Epoch: 13. Training data loss:  1801.0855712890625\n",
      "Epoch: 14. Training data loss:  1801.04296875\n",
      "Epoch: 15. Training data loss:  1801.0029296875\n",
      "Epoch: 16. Training data loss:  1800.938232421875\n",
      "Epoch: 17. Training data loss:  1800.933349609375\n",
      "Epoch: 18. Training data loss:  1800.8984375\n",
      "Epoch: 19. Training data loss:  1800.8184814453125\n",
      "Accuracy found: 0.88\n",
      "20 0.0001 0.3 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 8\n",
      "Epoch: 0. Training data loss:  7203.783203125\n",
      "Epoch: 1. Training data loss:  7194.4892578125\n",
      "Epoch: 2. Training data loss:  7184.34130859375\n",
      "Epoch: 3. Training data loss:  7170.82373046875\n",
      "Epoch: 4. Training data loss:  7152.37548828125\n",
      "Epoch: 5. Training data loss:  7128.59326171875\n",
      "Epoch: 6. Training data loss:  7100.90771484375\n",
      "Epoch: 7. Training data loss:  7071.80859375\n",
      "Epoch: 8. Training data loss:  7042.92236328125\n",
      "Epoch: 9. Training data loss:  7015.89501953125\n",
      "Epoch: 10. Training data loss:  6991.66064453125\n",
      "Epoch: 11. Training data loss:  6969.51416015625\n",
      "Epoch: 12. Training data loss:  6949.80419921875\n",
      "Epoch: 13. Training data loss:  6931.54638671875\n",
      "Epoch: 14. Training data loss:  6915.84228515625\n",
      "Epoch: 15. Training data loss:  6900.43017578125\n",
      "Epoch: 16. Training data loss:  6887.310546875\n",
      "Epoch: 17. Training data loss:  6874.365234375\n",
      "Epoch: 18. Training data loss:  6862.67236328125\n",
      "Epoch: 19. Training data loss:  6851.59619140625\n",
      "Epoch: 20. Training data loss:  6841.421875\n",
      "Epoch: 21. Training data loss:  6831.63818359375\n",
      "Epoch: 22. Training data loss:  6822.00146484375\n",
      "Epoch: 23. Training data loss:  6812.8662109375\n",
      "Epoch: 24. Training data loss:  6804.583984375\n",
      "Epoch: 25. Training data loss:  6796.359375\n",
      "Epoch: 26. Training data loss:  6788.3544921875\n",
      "Epoch: 27. Training data loss:  6781.4248046875\n",
      "Epoch: 28. Training data loss:  6773.73486328125\n",
      "Epoch: 29. Training data loss:  6766.513671875\n",
      "Epoch: 30. Training data loss:  6759.63671875\n",
      "Epoch: 31. Training data loss:  6752.99267578125\n",
      "Epoch: 32. Training data loss:  6746.66162109375\n",
      "Epoch: 33. Training data loss:  6740.654296875\n",
      "Epoch: 34. Training data loss:  6734.2822265625\n",
      "Epoch: 35. Training data loss:  6728.88671875\n",
      "Epoch: 36. Training data loss:  6722.6845703125\n",
      "Epoch: 37. Training data loss:  6717.39013671875\n",
      "Epoch: 38. Training data loss:  6712.18017578125\n",
      "Epoch: 39. Training data loss:  6706.77099609375\n",
      "Accuracy found: 5.21\n",
      "40 0.001 0.001 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 9\n",
      "Epoch: 0. Training data loss:  3603.048828125\n",
      "Epoch: 1. Training data loss:  3602.69384765625\n",
      "Epoch: 2. Training data loss:  3602.510498046875\n",
      "Epoch: 3. Training data loss:  3602.361328125\n",
      "Epoch: 4. Training data loss:  3602.12158203125\n",
      "Epoch: 5. Training data loss:  3601.9794921875\n",
      "Epoch: 6. Training data loss:  3601.69287109375\n",
      "Epoch: 7. Training data loss:  3601.45751953125\n",
      "Epoch: 8. Training data loss:  3601.3427734375\n",
      "Epoch: 9. Training data loss:  3601.06982421875\n",
      "Epoch: 10. Training data loss:  3600.933837890625\n",
      "Epoch: 11. Training data loss:  3600.796875\n",
      "Epoch: 12. Training data loss:  3600.535400390625\n",
      "Epoch: 13. Training data loss:  3600.40966796875\n",
      "Epoch: 14. Training data loss:  3600.1123046875\n",
      "Epoch: 15. Training data loss:  3599.88232421875\n",
      "Epoch: 16. Training data loss:  3599.8154296875\n",
      "Epoch: 17. Training data loss:  3599.5771484375\n",
      "Epoch: 18. Training data loss:  3599.389892578125\n",
      "Epoch: 19. Training data loss:  3599.0859375\n",
      "Epoch: 20. Training data loss:  3598.91357421875\n",
      "Epoch: 21. Training data loss:  3598.672119140625\n",
      "Epoch: 22. Training data loss:  3598.549072265625\n",
      "Epoch: 23. Training data loss:  3598.258056640625\n",
      "Epoch: 24. Training data loss:  3597.953125\n",
      "Epoch: 25. Training data loss:  3597.84521484375\n",
      "Epoch: 26. Training data loss:  3597.658203125\n",
      "Epoch: 27. Training data loss:  3597.468017578125\n",
      "Epoch: 28. Training data loss:  3596.998291015625\n",
      "Epoch: 29. Training data loss:  3596.90869140625\n",
      "Epoch: 30. Training data loss:  3596.4814453125\n",
      "Epoch: 31. Training data loss:  3596.42138671875\n",
      "Epoch: 32. Training data loss:  3596.234375\n",
      "Epoch: 33. Training data loss:  3595.8515625\n",
      "Epoch: 34. Training data loss:  3595.506591796875\n",
      "Epoch: 35. Training data loss:  3595.379150390625\n",
      "Epoch: 36. Training data loss:  3594.939453125\n",
      "Epoch: 37. Training data loss:  3594.9208984375\n",
      "Epoch: 38. Training data loss:  3594.598876953125\n",
      "Epoch: 39. Training data loss:  3594.143798828125\n",
      "Accuracy found: 1.77\n",
      "40 0.0001 0.01 CrossEntropyLoss() 64 0\n",
      "Starting iteration, 10\n",
      "Epoch: 0. Training data loss:  21915.66015625\n",
      "Epoch: 1. Training data loss:  21649.8203125\n",
      "Epoch: 2. Training data loss:  21583.505859375\n",
      "Epoch: 3. Training data loss:  21556.5\n",
      "Epoch: 4. Training data loss:  21538.904296875\n",
      "Epoch: 5. Training data loss:  21519.80078125\n",
      "Epoch: 6. Training data loss:  21521.169921875\n",
      "Epoch: 7. Training data loss:  21500.376953125\n",
      "Epoch: 8. Training data loss:  21510.7109375\n",
      "Epoch: 9. Training data loss:  21496.30859375\n",
      "Epoch: 10. Training data loss:  21498.587890625\n",
      "Epoch: 11. Training data loss:  21487.037109375\n",
      "Epoch: 12. Training data loss:  21493.62109375\n",
      "Epoch: 13. Training data loss:  21496.177734375\n",
      "Epoch: 14. Training data loss:  21490.072265625\n",
      "Epoch: 15. Training data loss:  21482.076171875\n",
      "Epoch: 16. Training data loss:  21489.64453125\n",
      "Epoch: 17. Training data loss:  21488.044921875\n",
      "Epoch: 18. Training data loss:  21487.412109375\n",
      "Epoch: 19. Training data loss:  21483.203125\n",
      "Epoch: 20. Training data loss:  21492.166015625\n",
      "Epoch: 21. Training data loss:  21486.681640625\n",
      "Epoch: 22. Training data loss:  21486.763671875\n",
      "Epoch: 23. Training data loss:  21477.5390625\n",
      "Epoch: 24. Training data loss:  21482.02734375\n",
      "Epoch: 25. Training data loss:  21476.021484375\n",
      "Epoch: 26. Training data loss:  21477.25\n",
      "Epoch: 27. Training data loss:  21472.583984375\n",
      "Epoch: 28. Training data loss:  21485.416015625\n",
      "Epoch: 29. Training data loss:  21482.052734375\n",
      "Epoch: 30. Training data loss:  21476.83203125\n",
      "Epoch: 31. Training data loss:  21473.630859375\n",
      "Epoch: 32. Training data loss:  21474.203125\n",
      "Epoch: 33. Training data loss:  21464.23828125\n",
      "Epoch: 34. Training data loss:  21462.779296875\n",
      "Epoch: 35. Training data loss:  21466.068359375\n",
      "Epoch: 36. Training data loss:  21446.5625\n",
      "Epoch: 37. Training data loss:  21434.2578125\n",
      "Epoch: 38. Training data loss:  21416.369140625\n",
      "Epoch: 39. Training data loss:  21379.93359375\n",
      "Accuracy found: 6.46\n",
      "40 0.1 0.01 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 11\n",
      "Epoch: 0. Training data loss:  387.0903015136719\n",
      "Epoch: 1. Training data loss:  387.1045837402344\n",
      "Epoch: 2. Training data loss:  387.1078796386719\n",
      "Epoch: 3. Training data loss:  387.10565185546875\n",
      "Epoch: 4. Training data loss:  387.1068115234375\n",
      "Epoch: 5. Training data loss:  387.1053466796875\n",
      "Epoch: 6. Training data loss:  387.10894775390625\n",
      "Epoch: 7. Training data loss:  387.1076354980469\n",
      "Epoch: 8. Training data loss:  387.10845947265625\n",
      "Epoch: 9. Training data loss:  387.1053161621094\n",
      "Epoch: 10. Training data loss:  387.1089172363281\n",
      "Epoch: 11. Training data loss:  387.108154296875\n",
      "Epoch: 12. Training data loss:  387.1092529296875\n",
      "Epoch: 13. Training data loss:  387.1069641113281\n",
      "Epoch: 14. Training data loss:  387.1083068847656\n",
      "Epoch: 15. Training data loss:  387.1082763671875\n",
      "Epoch: 16. Training data loss:  387.11077880859375\n",
      "Epoch: 17. Training data loss:  387.106201171875\n",
      "Epoch: 18. Training data loss:  387.1078186035156\n",
      "Epoch: 19. Training data loss:  387.10968017578125\n",
      "Epoch: 20. Training data loss:  387.1075744628906\n",
      "Epoch: 21. Training data loss:  387.1068115234375\n",
      "Epoch: 22. Training data loss:  387.10791015625\n",
      "Epoch: 23. Training data loss:  387.1095275878906\n",
      "Epoch: 24. Training data loss:  387.1087951660156\n",
      "Epoch: 25. Training data loss:  387.1081848144531\n",
      "Epoch: 26. Training data loss:  387.107421875\n",
      "Epoch: 27. Training data loss:  387.10986328125\n",
      "Epoch: 28. Training data loss:  387.1100158691406\n",
      "Epoch: 29. Training data loss:  387.1085205078125\n",
      "Epoch: 30. Training data loss:  387.1085205078125\n",
      "Epoch: 31. Training data loss:  387.1058654785156\n",
      "Epoch: 32. Training data loss:  387.10760498046875\n",
      "Epoch: 33. Training data loss:  387.1093444824219\n",
      "Epoch: 34. Training data loss:  387.1064758300781\n",
      "Epoch: 35. Training data loss:  387.1064758300781\n",
      "Epoch: 36. Training data loss:  387.1075439453125\n",
      "Epoch: 37. Training data loss:  387.11114501953125\n",
      "Epoch: 38. Training data loss:  387.1088562011719\n",
      "Epoch: 39. Training data loss:  387.1068420410156\n",
      "Accuracy found: 1.0\n",
      "40 0.1 0.3 MultiMarginLoss() 128 0\n",
      "Starting iteration, 12\n",
      "Epoch: 0. Training data loss:  1067.3287353515625\n",
      "Epoch: 1. Training data loss:  1316.5733642578125\n",
      "Epoch: 2. Training data loss:  1392.637451171875\n",
      "Epoch: 3. Training data loss:  1806.64990234375\n",
      "Epoch: 4. Training data loss:  1239.840087890625\n",
      "Epoch: 5. Training data loss:  2340.3173828125\n",
      "Epoch: 6. Training data loss:  2242.46533203125\n",
      "Epoch: 7. Training data loss:  3037.655029296875\n",
      "Epoch: 8. Training data loss:  1548.8421630859375\n",
      "Epoch: 9. Training data loss:  2126.822265625\n",
      "Epoch: 10. Training data loss:  2419.086181640625\n",
      "Epoch: 11. Training data loss:  1582.7095947265625\n",
      "Epoch: 12. Training data loss:  2525.964111328125\n",
      "Epoch: 13. Training data loss:  3464.480712890625\n",
      "Epoch: 14. Training data loss:  4797.4462890625\n",
      "Epoch: 15. Training data loss:  3961.549072265625\n",
      "Epoch: 16. Training data loss:  4266.42822265625\n",
      "Epoch: 17. Training data loss:  4264.01171875\n",
      "Epoch: 18. Training data loss:  3185.126220703125\n",
      "Epoch: 19. Training data loss:  4303.51416015625\n",
      "Accuracy found: 1.3\n",
      "20 0.1 0 MultiMarginLoss() 128 1\n",
      "Starting iteration, 13\n",
      "Epoch: 0. Training data loss:  387.00787353515625\n",
      "Epoch: 1. Training data loss:  387.1081237792969\n",
      "Epoch: 2. Training data loss:  387.1069641113281\n",
      "Epoch: 3. Training data loss:  387.10833740234375\n",
      "Epoch: 4. Training data loss:  387.1091613769531\n",
      "Epoch: 5. Training data loss:  387.1091613769531\n",
      "Epoch: 6. Training data loss:  387.1075439453125\n",
      "Epoch: 7. Training data loss:  387.1090393066406\n",
      "Epoch: 8. Training data loss:  387.1090393066406\n",
      "Epoch: 9. Training data loss:  387.10955810546875\n",
      "Epoch: 10. Training data loss:  387.1090393066406\n",
      "Epoch: 11. Training data loss:  387.1081848144531\n",
      "Epoch: 12. Training data loss:  387.1085205078125\n",
      "Epoch: 13. Training data loss:  387.10955810546875\n",
      "Epoch: 14. Training data loss:  387.1073913574219\n",
      "Epoch: 15. Training data loss:  387.1108093261719\n",
      "Epoch: 16. Training data loss:  387.1087951660156\n",
      "Epoch: 17. Training data loss:  387.1104431152344\n",
      "Epoch: 18. Training data loss:  387.1074523925781\n",
      "Epoch: 19. Training data loss:  387.10791015625\n",
      "Epoch: 20. Training data loss:  387.10614013671875\n",
      "Epoch: 21. Training data loss:  387.1077880859375\n",
      "Epoch: 22. Training data loss:  387.1075134277344\n",
      "Epoch: 23. Training data loss:  387.1067810058594\n",
      "Epoch: 24. Training data loss:  387.11077880859375\n",
      "Epoch: 25. Training data loss:  387.10870361328125\n",
      "Epoch: 26. Training data loss:  387.10919189453125\n",
      "Epoch: 27. Training data loss:  387.10675048828125\n",
      "Epoch: 28. Training data loss:  387.108642578125\n",
      "Epoch: 29. Training data loss:  387.10302734375\n",
      "Epoch: 30. Training data loss:  387.10888671875\n",
      "Epoch: 31. Training data loss:  387.1099548339844\n",
      "Epoch: 32. Training data loss:  387.1069030761719\n",
      "Epoch: 33. Training data loss:  387.10699462890625\n",
      "Epoch: 34. Training data loss:  387.1053161621094\n",
      "Epoch: 35. Training data loss:  387.10650634765625\n",
      "Epoch: 36. Training data loss:  387.1095275878906\n",
      "Epoch: 37. Training data loss:  387.10943603515625\n",
      "Epoch: 38. Training data loss:  387.11077880859375\n",
      "Epoch: 39. Training data loss:  387.1115417480469\n",
      "Accuracy found: 1.0\n",
      "40 0.1 0.3 MultiMarginLoss() 128 0\n",
      "Starting iteration, 14\n",
      "Epoch: 0. Training data loss:  386.639892578125\n",
      "Epoch: 1. Training data loss:  386.1610107421875\n",
      "Epoch: 2. Training data loss:  385.5699462890625\n",
      "Epoch: 3. Training data loss:  385.00518798828125\n",
      "Epoch: 4. Training data loss:  384.33514404296875\n",
      "Epoch: 5. Training data loss:  383.6361083984375\n",
      "Epoch: 6. Training data loss:  382.8054504394531\n",
      "Epoch: 7. Training data loss:  381.9384460449219\n",
      "Epoch: 8. Training data loss:  380.88763427734375\n",
      "Epoch: 9. Training data loss:  379.6429138183594\n",
      "Epoch: 10. Training data loss:  378.23486328125\n",
      "Epoch: 11. Training data loss:  376.41595458984375\n",
      "Epoch: 12. Training data loss:  374.47442626953125\n",
      "Epoch: 13. Training data loss:  372.0721435546875\n",
      "Epoch: 14. Training data loss:  369.4778137207031\n",
      "Epoch: 15. Training data loss:  366.4718322753906\n",
      "Epoch: 16. Training data loss:  363.12457275390625\n",
      "Epoch: 17. Training data loss:  359.5526123046875\n",
      "Epoch: 18. Training data loss:  355.6052551269531\n",
      "Epoch: 19. Training data loss:  351.5846862792969\n",
      "Epoch: 20. Training data loss:  347.3147277832031\n",
      "Epoch: 21. Training data loss:  343.1728820800781\n",
      "Epoch: 22. Training data loss:  338.9708557128906\n",
      "Epoch: 23. Training data loss:  334.8846435546875\n",
      "Epoch: 24. Training data loss:  330.8153381347656\n",
      "Epoch: 25. Training data loss:  326.9568176269531\n",
      "Epoch: 26. Training data loss:  323.1852111816406\n",
      "Epoch: 27. Training data loss:  319.7554931640625\n",
      "Epoch: 28. Training data loss:  316.1632080078125\n",
      "Epoch: 29. Training data loss:  313.0772399902344\n",
      "Epoch: 30. Training data loss:  310.095703125\n",
      "Epoch: 31. Training data loss:  307.2181396484375\n",
      "Epoch: 32. Training data loss:  304.3642578125\n",
      "Epoch: 33. Training data loss:  301.8578186035156\n",
      "Epoch: 34. Training data loss:  299.4787902832031\n",
      "Epoch: 35. Training data loss:  297.05499267578125\n",
      "Epoch: 36. Training data loss:  294.9700622558594\n",
      "Epoch: 37. Training data loss:  292.9425354003906\n",
      "Epoch: 38. Training data loss:  291.0589599609375\n",
      "Epoch: 39. Training data loss:  289.4168701171875\n",
      "Accuracy found: 3.63\n",
      "40 0.001 0.001 MultiMarginLoss() 128 0\n",
      "Starting iteration, 15\n",
      "Epoch: 0. Training data loss:  3601.589599609375\n",
      "Epoch: 1. Training data loss:  3601.28515625\n",
      "Epoch: 2. Training data loss:  3601.277587890625\n",
      "Epoch: 3. Training data loss:  3601.277099609375\n",
      "Epoch: 4. Training data loss:  3601.278076171875\n",
      "Epoch: 5. Training data loss:  3601.29052734375\n",
      "Epoch: 6. Training data loss:  3601.28076171875\n",
      "Epoch: 7. Training data loss:  3601.286865234375\n",
      "Epoch: 8. Training data loss:  3601.283447265625\n",
      "Epoch: 9. Training data loss:  3601.287109375\n",
      "Epoch: 10. Training data loss:  3601.283935546875\n",
      "Epoch: 11. Training data loss:  3601.277587890625\n",
      "Epoch: 12. Training data loss:  3601.283935546875\n",
      "Epoch: 13. Training data loss:  3601.281982421875\n",
      "Epoch: 14. Training data loss:  3601.28515625\n",
      "Epoch: 15. Training data loss:  3601.28271484375\n",
      "Epoch: 16. Training data loss:  3601.28564453125\n",
      "Epoch: 17. Training data loss:  3601.275634765625\n",
      "Epoch: 18. Training data loss:  3601.275146484375\n",
      "Epoch: 19. Training data loss:  3601.275634765625\n",
      "Epoch: 20. Training data loss:  3601.28662109375\n",
      "Epoch: 21. Training data loss:  3601.28369140625\n",
      "Epoch: 22. Training data loss:  3601.294921875\n",
      "Epoch: 23. Training data loss:  3601.28173828125\n",
      "Epoch: 24. Training data loss:  3601.279296875\n",
      "Epoch: 25. Training data loss:  3601.28173828125\n",
      "Epoch: 26. Training data loss:  3601.27685546875\n",
      "Epoch: 27. Training data loss:  3601.281005859375\n",
      "Epoch: 28. Training data loss:  3601.285888671875\n",
      "Epoch: 29. Training data loss:  3601.2880859375\n",
      "Epoch: 30. Training data loss:  3601.285400390625\n",
      "Epoch: 31. Training data loss:  3601.281982421875\n",
      "Epoch: 32. Training data loss:  3601.279296875\n",
      "Epoch: 33. Training data loss:  3601.27880859375\n",
      "Epoch: 34. Training data loss:  3601.28125\n",
      "Epoch: 35. Training data loss:  3601.283935546875\n",
      "Epoch: 36. Training data loss:  3601.28173828125\n",
      "Epoch: 37. Training data loss:  3601.281494140625\n",
      "Epoch: 38. Training data loss:  3601.28564453125\n",
      "Epoch: 39. Training data loss:  3601.281494140625\n",
      "Epoch: 40. Training data loss:  3601.283203125\n",
      "Epoch: 41. Training data loss:  3601.28369140625\n",
      "Epoch: 42. Training data loss:  3601.286376953125\n",
      "Epoch: 43. Training data loss:  3601.2822265625\n",
      "Epoch: 44. Training data loss:  3601.283203125\n",
      "Epoch: 45. Training data loss:  3601.29248046875\n",
      "Epoch: 46. Training data loss:  3601.292236328125\n",
      "Epoch: 47. Training data loss:  3601.281494140625\n",
      "Epoch: 48. Training data loss:  3601.285888671875\n",
      "Epoch: 49. Training data loss:  3601.28759765625\n",
      "Epoch: 50. Training data loss:  3601.285400390625\n",
      "Epoch: 51. Training data loss:  3601.282958984375\n",
      "Epoch: 52. Training data loss:  3601.286865234375\n",
      "Epoch: 53. Training data loss:  3601.2841796875\n",
      "Epoch: 54. Training data loss:  3601.281494140625\n",
      "Epoch: 55. Training data loss:  3601.288818359375\n",
      "Epoch: 56. Training data loss:  3601.287841796875\n",
      "Epoch: 57. Training data loss:  3601.279296875\n",
      "Epoch: 58. Training data loss:  3601.28369140625\n",
      "Epoch: 59. Training data loss:  3601.28857421875\n",
      "Accuracy found: 1.0\n",
      "60 0.1 0.3 CrossEntropyLoss() 64 0\n",
      "Starting iteration, 16\n",
      "Epoch: 0. Training data loss:  23038.16796875\n",
      "Epoch: 1. Training data loss:  23033.037109375\n",
      "Epoch: 2. Training data loss:  23026.482421875\n",
      "Epoch: 3. Training data loss:  23021.4609375\n",
      "Epoch: 4. Training data loss:  23015.35546875\n",
      "Epoch: 5. Training data loss:  23010.12109375\n",
      "Epoch: 6. Training data loss:  23004.611328125\n",
      "Epoch: 7. Training data loss:  22997.92578125\n",
      "Epoch: 8. Training data loss:  22991.62109375\n",
      "Epoch: 9. Training data loss:  22984.90234375\n",
      "Epoch: 10. Training data loss:  22977.01953125\n",
      "Epoch: 11. Training data loss:  22969.123046875\n",
      "Epoch: 12. Training data loss:  22961.267578125\n",
      "Epoch: 13. Training data loss:  22953.32421875\n",
      "Epoch: 14. Training data loss:  22943.515625\n",
      "Epoch: 15. Training data loss:  22934.3515625\n",
      "Epoch: 16. Training data loss:  22923.26953125\n",
      "Epoch: 17. Training data loss:  22911.486328125\n",
      "Epoch: 18. Training data loss:  22900.439453125\n",
      "Epoch: 19. Training data loss:  22886.447265625\n",
      "Epoch: 20. Training data loss:  22872.904296875\n",
      "Epoch: 21. Training data loss:  22859.216796875\n",
      "Epoch: 22. Training data loss:  22846.154296875\n",
      "Epoch: 23. Training data loss:  22828.93359375\n",
      "Epoch: 24. Training data loss:  22813.8125\n",
      "Epoch: 25. Training data loss:  22797.923828125\n",
      "Epoch: 26. Training data loss:  22780.97265625\n",
      "Epoch: 27. Training data loss:  22764.36328125\n",
      "Epoch: 28. Training data loss:  22748.287109375\n",
      "Epoch: 29. Training data loss:  22731.1875\n",
      "Epoch: 30. Training data loss:  22713.498046875\n",
      "Epoch: 31. Training data loss:  22697.53515625\n",
      "Epoch: 32. Training data loss:  22680.802734375\n",
      "Epoch: 33. Training data loss:  22664.23046875\n",
      "Epoch: 34. Training data loss:  22647.8515625\n",
      "Epoch: 35. Training data loss:  22630.736328125\n",
      "Epoch: 36. Training data loss:  22614.853515625\n",
      "Epoch: 37. Training data loss:  22600.240234375\n",
      "Epoch: 38. Training data loss:  22584.595703125\n",
      "Epoch: 39. Training data loss:  22570.29296875\n",
      "Epoch: 40. Training data loss:  22556.634765625\n",
      "Epoch: 41. Training data loss:  22542.572265625\n",
      "Epoch: 42. Training data loss:  22528.32421875\n",
      "Epoch: 43. Training data loss:  22516.21875\n",
      "Epoch: 44. Training data loss:  22503.271484375\n",
      "Epoch: 45. Training data loss:  22491.513671875\n",
      "Epoch: 46. Training data loss:  22478.2734375\n",
      "Epoch: 47. Training data loss:  22468.056640625\n",
      "Epoch: 48. Training data loss:  22454.6171875\n",
      "Epoch: 49. Training data loss:  22443.873046875\n",
      "Epoch: 50. Training data loss:  22433.310546875\n",
      "Epoch: 51. Training data loss:  22424.525390625\n",
      "Epoch: 52. Training data loss:  22413.24609375\n",
      "Epoch: 53. Training data loss:  22402.748046875\n",
      "Epoch: 54. Training data loss:  22392.259765625\n",
      "Epoch: 55. Training data loss:  22384.896484375\n",
      "Epoch: 56. Training data loss:  22375.185546875\n",
      "Epoch: 57. Training data loss:  22365.94921875\n",
      "Epoch: 58. Training data loss:  22357.44921875\n",
      "Epoch: 59. Training data loss:  22348.12109375\n",
      "Accuracy found: 3.42\n",
      "60 0.0001 0.01 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 17\n",
      "Epoch: 0. Training data loss:  23034.943359375\n",
      "Epoch: 1. Training data loss:  23029.76171875\n",
      "Epoch: 2. Training data loss:  23023.548828125\n",
      "Epoch: 3. Training data loss:  23018.322265625\n",
      "Epoch: 4. Training data loss:  23011.130859375\n",
      "Epoch: 5. Training data loss:  23005.04296875\n",
      "Epoch: 6. Training data loss:  22999.33203125\n",
      "Epoch: 7. Training data loss:  22991.984375\n",
      "Epoch: 8. Training data loss:  22985.193359375\n",
      "Epoch: 9. Training data loss:  22977.421875\n",
      "Epoch: 10. Training data loss:  22969.59765625\n",
      "Epoch: 11. Training data loss:  22958.796875\n",
      "Epoch: 12. Training data loss:  22948.91015625\n",
      "Epoch: 13. Training data loss:  22938.5\n",
      "Epoch: 14. Training data loss:  22927.41796875\n",
      "Epoch: 15. Training data loss:  22916.490234375\n",
      "Epoch: 16. Training data loss:  22904.513671875\n",
      "Epoch: 17. Training data loss:  22890.81640625\n",
      "Epoch: 18. Training data loss:  22878.646484375\n",
      "Epoch: 19. Training data loss:  22863.9921875\n",
      "Epoch: 20. Training data loss:  22848.263671875\n",
      "Epoch: 21. Training data loss:  22832.0078125\n",
      "Epoch: 22. Training data loss:  22817.275390625\n",
      "Epoch: 23. Training data loss:  22802.380859375\n",
      "Epoch: 24. Training data loss:  22785.599609375\n",
      "Epoch: 25. Training data loss:  22769.203125\n",
      "Epoch: 26. Training data loss:  22753.1796875\n",
      "Epoch: 27. Training data loss:  22736.150390625\n",
      "Epoch: 28. Training data loss:  22718.125\n",
      "Epoch: 29. Training data loss:  22701.4140625\n",
      "Epoch: 30. Training data loss:  22685.73046875\n",
      "Epoch: 31. Training data loss:  22668.28515625\n",
      "Epoch: 32. Training data loss:  22650.97265625\n",
      "Epoch: 33. Training data loss:  22635.189453125\n",
      "Epoch: 34. Training data loss:  22620.478515625\n",
      "Epoch: 35. Training data loss:  22601.78515625\n",
      "Epoch: 36. Training data loss:  22586.21875\n",
      "Epoch: 37. Training data loss:  22570.59765625\n",
      "Epoch: 38. Training data loss:  22556.423828125\n",
      "Epoch: 39. Training data loss:  22539.912109375\n",
      "Accuracy found: 2.7\n",
      "40 0.0001 0 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [32, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [16, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  1078.3450927734375\n",
      "Epoch: 1. Training data loss:  1329.6683349609375\n",
      "Epoch: 2. Training data loss:  1157.0615234375\n",
      "Epoch: 3. Training data loss:  1096.3782958984375\n",
      "Epoch: 4. Training data loss:  1092.595703125\n",
      "Epoch: 5. Training data loss:  1092.9395751953125\n",
      "Epoch: 6. Training data loss:  1324.6036376953125\n",
      "Epoch: 7. Training data loss:  1317.08203125\n",
      "Epoch: 8. Training data loss:  1281.5523681640625\n",
      "Epoch: 9. Training data loss:  1109.228759765625\n",
      "Epoch: 10. Training data loss:  1106.516845703125\n",
      "Epoch: 11. Training data loss:  1131.8765869140625\n",
      "Epoch: 12. Training data loss:  1161.1214599609375\n",
      "Epoch: 13. Training data loss:  1122.2703857421875\n",
      "Epoch: 14. Training data loss:  1080.681884765625\n",
      "Epoch: 15. Training data loss:  1031.1123046875\n",
      "Epoch: 16. Training data loss:  1192.9666748046875\n",
      "Epoch: 17. Training data loss:  1091.271240234375\n",
      "Epoch: 18. Training data loss:  1093.2066650390625\n",
      "Epoch: 19. Training data loss:  1184.7755126953125\n",
      "Epoch: 20. Training data loss:  1147.8956298828125\n",
      "Epoch: 21. Training data loss:  1192.9674072265625\n",
      "Epoch: 22. Training data loss:  1557.5828857421875\n",
      "Epoch: 23. Training data loss:  1483.88232421875\n",
      "Epoch: 24. Training data loss:  1378.3009033203125\n",
      "Epoch: 25. Training data loss:  1362.3609619140625\n",
      "Epoch: 26. Training data loss:  1248.095703125\n",
      "Epoch: 27. Training data loss:  1092.34228515625\n",
      "Epoch: 28. Training data loss:  1143.7960205078125\n",
      "Epoch: 29. Training data loss:  1164.0123291015625\n",
      "Epoch: 30. Training data loss:  1173.9239501953125\n",
      "Epoch: 31. Training data loss:  1248.9697265625\n",
      "Epoch: 32. Training data loss:  1138.685302734375\n",
      "Epoch: 33. Training data loss:  1146.651123046875\n",
      "Epoch: 34. Training data loss:  1219.3153076171875\n",
      "Epoch: 35. Training data loss:  1353.8402099609375\n",
      "Epoch: 36. Training data loss:  1480.5399169921875\n",
      "Epoch: 37. Training data loss:  1220.63232421875\n",
      "Epoch: 38. Training data loss:  1127.854248046875\n",
      "Epoch: 39. Training data loss:  1226.9385986328125\n",
      "Accuracy found: 1.25\n",
      "40 0.001 0.01 MultiMarginLoss() 32 1\n",
      "Starting iteration, 19\n",
      "Epoch: 0. Training data loss:  1017.9786376953125\n",
      "Epoch: 1. Training data loss:  1459.7232666015625\n",
      "Epoch: 2. Training data loss:  1564.6031494140625\n",
      "Epoch: 3. Training data loss:  1649.0584716796875\n",
      "Epoch: 4. Training data loss:  1846.7225341796875\n",
      "Epoch: 5. Training data loss:  2455.18310546875\n",
      "Epoch: 6. Training data loss:  1654.2261962890625\n",
      "Epoch: 7. Training data loss:  2092.172607421875\n",
      "Epoch: 8. Training data loss:  2684.3544921875\n",
      "Epoch: 9. Training data loss:  2740.85986328125\n",
      "Epoch: 10. Training data loss:  3082.61962890625\n",
      "Epoch: 11. Training data loss:  3803.151611328125\n",
      "Epoch: 12. Training data loss:  3771.85595703125\n",
      "Epoch: 13. Training data loss:  3491.45556640625\n",
      "Epoch: 14. Training data loss:  3487.492919921875\n",
      "Epoch: 15. Training data loss:  3787.37548828125\n",
      "Epoch: 16. Training data loss:  3519.709228515625\n",
      "Epoch: 17. Training data loss:  3601.19140625\n",
      "Epoch: 18. Training data loss:  4188.15283203125\n",
      "Epoch: 19. Training data loss:  4020.187255859375\n",
      "Epoch: 20. Training data loss:  4464.13134765625\n",
      "Epoch: 21. Training data loss:  3141.858642578125\n",
      "Epoch: 22. Training data loss:  3919.401123046875\n",
      "Epoch: 23. Training data loss:  4057.96875\n",
      "Epoch: 24. Training data loss:  3281.688232421875\n",
      "Epoch: 25. Training data loss:  3180.5703125\n",
      "Epoch: 26. Training data loss:  3834.943115234375\n",
      "Epoch: 27. Training data loss:  2791.30859375\n",
      "Epoch: 28. Training data loss:  4264.59375\n",
      "Epoch: 29. Training data loss:  5254.2138671875\n",
      "Epoch: 30. Training data loss:  4346.5068359375\n",
      "Epoch: 31. Training data loss:  4990.8291015625\n",
      "Epoch: 32. Training data loss:  4210.55615234375\n",
      "Epoch: 33. Training data loss:  4297.103515625\n",
      "Epoch: 34. Training data loss:  3721.392578125\n",
      "Epoch: 35. Training data loss:  2671.875732421875\n",
      "Epoch: 36. Training data loss:  4123.705078125\n",
      "Epoch: 37. Training data loss:  4895.76611328125\n",
      "Epoch: 38. Training data loss:  5308.3984375\n",
      "Epoch: 39. Training data loss:  5159.435546875\n",
      "Epoch: 40. Training data loss:  6142.8359375\n",
      "Epoch: 41. Training data loss:  6749.98095703125\n",
      "Epoch: 42. Training data loss:  5730.7021484375\n",
      "Epoch: 43. Training data loss:  4598.4052734375\n",
      "Epoch: 44. Training data loss:  5682.24853515625\n",
      "Epoch: 45. Training data loss:  6271.716796875\n",
      "Epoch: 46. Training data loss:  5062.29296875\n",
      "Epoch: 47. Training data loss:  4649.208984375\n",
      "Epoch: 48. Training data loss:  6725.3818359375\n",
      "Epoch: 49. Training data loss:  7707.2119140625\n",
      "Epoch: 50. Training data loss:  7284.15966796875\n",
      "Epoch: 51. Training data loss:  7666.84716796875\n",
      "Epoch: 52. Training data loss:  8449.7255859375\n",
      "Epoch: 53. Training data loss:  9081.142578125\n",
      "Epoch: 54. Training data loss:  10543.6376953125\n",
      "Epoch: 55. Training data loss:  9856.486328125\n",
      "Epoch: 56. Training data loss:  9691.3720703125\n",
      "Epoch: 57. Training data loss:  10102.970703125\n",
      "Epoch: 58. Training data loss:  9728.6083984375\n",
      "Epoch: 59. Training data loss:  8301.181640625\n",
      "Accuracy found: 1.06\n",
      "60 0.1 0 MultiMarginLoss() 128 1\n",
      "Starting iteration, 20\n",
      "Epoch: 0. Training data loss:  1074.0926513671875\n",
      "Epoch: 1. Training data loss:  1248.546630859375\n",
      "Epoch: 2. Training data loss:  1206.9957275390625\n",
      "Epoch: 3. Training data loss:  1216.5531005859375\n",
      "Epoch: 4. Training data loss:  1139.1112060546875\n",
      "Epoch: 5. Training data loss:  1109.7713623046875\n",
      "Epoch: 6. Training data loss:  1200.3731689453125\n",
      "Epoch: 7. Training data loss:  1260.2791748046875\n",
      "Epoch: 8. Training data loss:  1136.5858154296875\n",
      "Epoch: 9. Training data loss:  1157.8897705078125\n",
      "Epoch: 10. Training data loss:  1108.333740234375\n",
      "Epoch: 11. Training data loss:  1115.2608642578125\n",
      "Epoch: 12. Training data loss:  1160.96826171875\n",
      "Epoch: 13. Training data loss:  1071.32666015625\n",
      "Epoch: 14. Training data loss:  1043.8251953125\n",
      "Epoch: 15. Training data loss:  1105.63525390625\n",
      "Epoch: 16. Training data loss:  1057.7635498046875\n",
      "Epoch: 17. Training data loss:  1083.00341796875\n",
      "Epoch: 18. Training data loss:  1100.65625\n",
      "Epoch: 19. Training data loss:  1134.2008056640625\n",
      "Epoch: 20. Training data loss:  1086.51171875\n",
      "Epoch: 21. Training data loss:  1246.0068359375\n",
      "Epoch: 22. Training data loss:  1358.2950439453125\n",
      "Epoch: 23. Training data loss:  1222.1214599609375\n",
      "Epoch: 24. Training data loss:  1161.6663818359375\n",
      "Epoch: 25. Training data loss:  1267.3204345703125\n",
      "Epoch: 26. Training data loss:  1215.20947265625\n",
      "Epoch: 27. Training data loss:  1129.45654296875\n",
      "Epoch: 28. Training data loss:  1129.245849609375\n",
      "Epoch: 29. Training data loss:  1230.3880615234375\n",
      "Epoch: 30. Training data loss:  1200.396240234375\n",
      "Epoch: 31. Training data loss:  1191.858154296875\n",
      "Epoch: 32. Training data loss:  1154.099365234375\n",
      "Epoch: 33. Training data loss:  1187.115966796875\n",
      "Epoch: 34. Training data loss:  1080.8486328125\n",
      "Epoch: 35. Training data loss:  1230.8612060546875\n",
      "Epoch: 36. Training data loss:  1121.347412109375\n",
      "Epoch: 37. Training data loss:  1300.5582275390625\n",
      "Epoch: 38. Training data loss:  1148.1302490234375\n",
      "Epoch: 39. Training data loss:  1180.8377685546875\n",
      "Accuracy found: 2.72\n",
      "40 0.001 0.01 MultiMarginLoss() 32 1\n",
      "Starting iteration, 21\n",
      "Epoch: 0. Training data loss:  7200.1943359375\n",
      "Epoch: 1. Training data loss:  7200.197265625\n",
      "Epoch: 2. Training data loss:  7200.30224609375\n",
      "Epoch: 3. Training data loss:  7200.20263671875\n",
      "Epoch: 4. Training data loss:  7199.9208984375\n",
      "Epoch: 5. Training data loss:  7199.71875\n",
      "Epoch: 6. Training data loss:  7199.73681640625\n",
      "Epoch: 7. Training data loss:  7199.38916015625\n",
      "Epoch: 8. Training data loss:  7200.3212890625\n",
      "Epoch: 9. Training data loss:  7199.24267578125\n",
      "Epoch: 10. Training data loss:  7199.900390625\n",
      "Epoch: 11. Training data loss:  7199.77392578125\n",
      "Epoch: 12. Training data loss:  7199.38671875\n",
      "Epoch: 13. Training data loss:  7200.1484375\n",
      "Epoch: 14. Training data loss:  7199.8662109375\n",
      "Epoch: 15. Training data loss:  7199.646484375\n",
      "Epoch: 16. Training data loss:  7199.48046875\n",
      "Epoch: 17. Training data loss:  7199.4208984375\n",
      "Epoch: 18. Training data loss:  7200.12158203125\n",
      "Epoch: 19. Training data loss:  7199.36962890625\n",
      "Accuracy found: 1.0\n",
      "20 0.1 0.3 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 22\n",
      "Epoch: 0. Training data loss:  1547.4957275390625\n",
      "Epoch: 1. Training data loss:  1547.4410400390625\n",
      "Epoch: 2. Training data loss:  1547.4554443359375\n",
      "Epoch: 3. Training data loss:  1547.4515380859375\n",
      "Epoch: 4. Training data loss:  1547.459716796875\n",
      "Epoch: 5. Training data loss:  1547.434814453125\n",
      "Epoch: 6. Training data loss:  1547.4647216796875\n",
      "Epoch: 7. Training data loss:  1547.4608154296875\n",
      "Epoch: 8. Training data loss:  1547.4659423828125\n",
      "Epoch: 9. Training data loss:  1547.433349609375\n",
      "Epoch: 10. Training data loss:  1547.449462890625\n",
      "Epoch: 11. Training data loss:  1547.473388671875\n",
      "Epoch: 12. Training data loss:  1547.4368896484375\n",
      "Epoch: 13. Training data loss:  1547.4619140625\n",
      "Epoch: 14. Training data loss:  1547.43994140625\n",
      "Epoch: 15. Training data loss:  1547.4365234375\n",
      "Epoch: 16. Training data loss:  1547.4708251953125\n",
      "Epoch: 17. Training data loss:  1547.4434814453125\n",
      "Epoch: 18. Training data loss:  1547.4532470703125\n",
      "Epoch: 19. Training data loss:  1547.46435546875\n",
      "Accuracy found: 1.0\n",
      "20 0.1 0.3 MultiMarginLoss() 32 0\n",
      "Starting iteration, 23\n",
      "Epoch: 0. Training data loss:  5263.54150390625\n",
      "Epoch: 1. Training data loss:  5293.779296875\n",
      "Epoch: 2. Training data loss:  5403.50341796875\n",
      "Epoch: 3. Training data loss:  6497.736328125\n",
      "Epoch: 4. Training data loss:  5368.1015625\n",
      "Epoch: 5. Training data loss:  5298.07470703125\n",
      "Epoch: 6. Training data loss:  5356.99365234375\n",
      "Epoch: 7. Training data loss:  5413.126953125\n",
      "Epoch: 8. Training data loss:  5419.478515625\n",
      "Epoch: 9. Training data loss:  5567.7470703125\n",
      "Epoch: 10. Training data loss:  5476.11328125\n",
      "Epoch: 11. Training data loss:  5425.6943359375\n",
      "Epoch: 12. Training data loss:  5354.3203125\n",
      "Epoch: 13. Training data loss:  5464.59033203125\n",
      "Epoch: 14. Training data loss:  5496.6435546875\n",
      "Epoch: 15. Training data loss:  5490.0390625\n",
      "Epoch: 16. Training data loss:  5498.7587890625\n",
      "Epoch: 17. Training data loss:  5452.80712890625\n",
      "Epoch: 18. Training data loss:  5392.41455078125\n",
      "Epoch: 19. Training data loss:  5490.06494140625\n",
      "Epoch: 20. Training data loss:  5421.1572265625\n",
      "Epoch: 21. Training data loss:  5323.44580078125\n",
      "Epoch: 22. Training data loss:  5440.77294921875\n",
      "Epoch: 23. Training data loss:  5519.3291015625\n",
      "Epoch: 24. Training data loss:  5288.74072265625\n",
      "Epoch: 25. Training data loss:  5375.859375\n",
      "Epoch: 26. Training data loss:  5473.59814453125\n",
      "Epoch: 27. Training data loss:  5450.66650390625\n",
      "Epoch: 28. Training data loss:  5412.73681640625\n",
      "Epoch: 29. Training data loss:  5496.62255859375\n",
      "Epoch: 30. Training data loss:  5270.9892578125\n",
      "Epoch: 31. Training data loss:  5272.99169921875\n",
      "Epoch: 32. Training data loss:  5415.57861328125\n",
      "Epoch: 33. Training data loss:  5317.6806640625\n",
      "Epoch: 34. Training data loss:  5439.40087890625\n",
      "Epoch: 35. Training data loss:  5502.259765625\n",
      "Epoch: 36. Training data loss:  6264.59130859375\n",
      "Epoch: 37. Training data loss:  7731.24658203125\n",
      "Epoch: 38. Training data loss:  7261.97314453125\n",
      "Epoch: 39. Training data loss:  7112.36962890625\n",
      "Epoch: 40. Training data loss:  16147.4365234375\n",
      "Epoch: 41. Training data loss:  21743.076171875\n",
      "Epoch: 42. Training data loss:  25225.689453125\n",
      "Epoch: 43. Training data loss:  29028.82421875\n",
      "Epoch: 44. Training data loss:  30555.58203125\n",
      "Epoch: 45. Training data loss:  30365.3046875\n",
      "Epoch: 46. Training data loss:  34461.08203125\n",
      "Epoch: 47. Training data loss:  36809.3125\n",
      "Epoch: 48. Training data loss:  31772.185546875\n",
      "Epoch: 49. Training data loss:  32966.578125\n",
      "Epoch: 50. Training data loss:  30943.38671875\n",
      "Epoch: 51. Training data loss:  31677.154296875\n",
      "Epoch: 52. Training data loss:  40076.34765625\n",
      "Epoch: 53. Training data loss:  37716.203125\n",
      "Epoch: 54. Training data loss:  31377.74609375\n",
      "Epoch: 55. Training data loss:  25589.525390625\n",
      "Epoch: 56. Training data loss:  25259.9765625\n",
      "Epoch: 57. Training data loss:  25632.423828125\n",
      "Epoch: 58. Training data loss:  33915.7890625\n",
      "Epoch: 59. Training data loss:  32803.37890625\n",
      "Accuracy found: 1.83\n",
      "60 0.01 0.001 MultiMarginLoss() 10 1\n",
      "Starting iteration, 24\n",
      "Epoch: 0. Training data loss:  21480.884765625\n",
      "Epoch: 1. Training data loss:  20553.126953125\n",
      "Epoch: 2. Training data loss:  20042.7890625\n",
      "Epoch: 3. Training data loss:  19620.84765625\n",
      "Epoch: 4. Training data loss:  19248.63671875\n",
      "Epoch: 5. Training data loss:  18978.994140625\n",
      "Epoch: 6. Training data loss:  18756.4921875\n",
      "Epoch: 7. Training data loss:  18563.361328125\n",
      "Epoch: 8. Training data loss:  18382.21484375\n",
      "Epoch: 9. Training data loss:  18230.2578125\n",
      "Epoch: 10. Training data loss:  18081.306640625\n",
      "Epoch: 11. Training data loss:  17934.71875\n",
      "Epoch: 12. Training data loss:  17828.318359375\n",
      "Epoch: 13. Training data loss:  17717.23828125\n",
      "Epoch: 14. Training data loss:  17624.296875\n",
      "Epoch: 15. Training data loss:  17535.56640625\n",
      "Epoch: 16. Training data loss:  17465.453125\n",
      "Epoch: 17. Training data loss:  17397.24609375\n",
      "Epoch: 18. Training data loss:  17323.169921875\n",
      "Epoch: 19. Training data loss:  17262.279296875\n",
      "Epoch: 20. Training data loss:  17209.421875\n",
      "Epoch: 21. Training data loss:  17150.5546875\n",
      "Epoch: 22. Training data loss:  17112.06640625\n",
      "Epoch: 23. Training data loss:  17074.22265625\n",
      "Epoch: 24. Training data loss:  17011.876953125\n",
      "Epoch: 25. Training data loss:  16961.564453125\n",
      "Epoch: 26. Training data loss:  16922.0\n",
      "Epoch: 27. Training data loss:  16879.578125\n",
      "Epoch: 28. Training data loss:  16845.716796875\n",
      "Epoch: 29. Training data loss:  16803.7421875\n",
      "Epoch: 30. Training data loss:  16775.05859375\n",
      "Epoch: 31. Training data loss:  16738.197265625\n",
      "Epoch: 32. Training data loss:  16710.166015625\n",
      "Epoch: 33. Training data loss:  16667.51953125\n",
      "Epoch: 34. Training data loss:  16630.546875\n",
      "Epoch: 35. Training data loss:  16600.6640625\n",
      "Epoch: 36. Training data loss:  16573.658203125\n",
      "Epoch: 37. Training data loss:  16563.591796875\n",
      "Epoch: 38. Training data loss:  16534.38671875\n",
      "Epoch: 39. Training data loss:  16499.720703125\n",
      "Accuracy found: 23.53\n",
      "40 0.1 0.001 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [64, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  773.474853515625\n",
      "Epoch: 1. Training data loss:  772.1598510742188\n",
      "Epoch: 2. Training data loss:  770.791259765625\n",
      "Epoch: 3. Training data loss:  769.2608642578125\n",
      "Epoch: 4. Training data loss:  767.2986450195312\n",
      "Epoch: 5. Training data loss:  764.9620971679688\n",
      "Epoch: 6. Training data loss:  761.928955078125\n",
      "Epoch: 7. Training data loss:  758.1475219726562\n",
      "Epoch: 8. Training data loss:  753.4310302734375\n",
      "Epoch: 9. Training data loss:  747.2982788085938\n",
      "Epoch: 10. Training data loss:  739.5778198242188\n",
      "Epoch: 11. Training data loss:  730.4500732421875\n",
      "Epoch: 12. Training data loss:  719.528564453125\n",
      "Epoch: 13. Training data loss:  707.4334716796875\n",
      "Epoch: 14. Training data loss:  694.7566528320312\n",
      "Epoch: 15. Training data loss:  682.4548950195312\n",
      "Epoch: 16. Training data loss:  670.8936157226562\n",
      "Epoch: 17. Training data loss:  660.4077758789062\n",
      "Epoch: 18. Training data loss:  651.4174194335938\n",
      "Epoch: 19. Training data loss:  643.1332397460938\n",
      "Accuracy found: 2.9\n",
      "20 0.001 0.01 MultiMarginLoss() 64 0\n",
      "Starting iteration, 26\n",
      "Epoch: 0. Training data loss:  7139.7080078125\n",
      "Epoch: 1. Training data loss:  6975.31591796875\n",
      "Epoch: 2. Training data loss:  6848.66259765625\n",
      "Epoch: 3. Training data loss:  6763.7666015625\n",
      "Epoch: 4. Training data loss:  6702.94775390625\n",
      "Epoch: 5. Training data loss:  6656.18408203125\n",
      "Epoch: 6. Training data loss:  6617.14990234375\n",
      "Epoch: 7. Training data loss:  6583.5390625\n",
      "Epoch: 8. Training data loss:  6553.05517578125\n",
      "Epoch: 9. Training data loss:  6523.09130859375\n",
      "Epoch: 10. Training data loss:  6494.90771484375\n",
      "Epoch: 11. Training data loss:  6467.73779296875\n",
      "Epoch: 12. Training data loss:  6439.39404296875\n",
      "Epoch: 13. Training data loss:  6412.69873046875\n",
      "Epoch: 14. Training data loss:  6386.95068359375\n",
      "Epoch: 15. Training data loss:  6361.05810546875\n",
      "Epoch: 16. Training data loss:  6336.6025390625\n",
      "Epoch: 17. Training data loss:  6313.1416015625\n",
      "Epoch: 18. Training data loss:  6290.0537109375\n",
      "Epoch: 19. Training data loss:  6267.67529296875\n",
      "Epoch: 20. Training data loss:  6246.14599609375\n",
      "Epoch: 21. Training data loss:  6225.53173828125\n",
      "Epoch: 22. Training data loss:  6205.51513671875\n",
      "Epoch: 23. Training data loss:  6186.3642578125\n",
      "Epoch: 24. Training data loss:  6168.14794921875\n",
      "Epoch: 25. Training data loss:  6152.0166015625\n",
      "Epoch: 26. Training data loss:  6135.2099609375\n",
      "Epoch: 27. Training data loss:  6118.96630859375\n",
      "Epoch: 28. Training data loss:  6103.09326171875\n",
      "Epoch: 29. Training data loss:  6088.21875\n",
      "Epoch: 30. Training data loss:  6073.4267578125\n",
      "Epoch: 31. Training data loss:  6059.900390625\n",
      "Epoch: 32. Training data loss:  6045.26318359375\n",
      "Epoch: 33. Training data loss:  6032.97216796875\n",
      "Epoch: 34. Training data loss:  6018.78369140625\n",
      "Epoch: 35. Training data loss:  6004.9501953125\n",
      "Epoch: 36. Training data loss:  5992.75341796875\n",
      "Epoch: 37. Training data loss:  5979.708984375\n",
      "Epoch: 38. Training data loss:  5966.94140625\n",
      "Epoch: 39. Training data loss:  5955.6669921875\n",
      "Accuracy found: 12.67\n",
      "40 0.01 0.001 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 27\n",
      "Epoch: 0. Training data loss:  1802.272216796875\n",
      "Epoch: 1. Training data loss:  1802.162353515625\n",
      "Epoch: 2. Training data loss:  1802.0882568359375\n",
      "Epoch: 3. Training data loss:  1802.01123046875\n",
      "Epoch: 4. Training data loss:  1801.8912353515625\n",
      "Epoch: 5. Training data loss:  1801.80419921875\n",
      "Epoch: 6. Training data loss:  1801.7530517578125\n",
      "Epoch: 7. Training data loss:  1801.6846923828125\n",
      "Epoch: 8. Training data loss:  1801.5018310546875\n",
      "Epoch: 9. Training data loss:  1801.4542236328125\n",
      "Epoch: 10. Training data loss:  1801.3160400390625\n",
      "Epoch: 11. Training data loss:  1801.28564453125\n",
      "Epoch: 12. Training data loss:  1801.1807861328125\n",
      "Epoch: 13. Training data loss:  1800.978271484375\n",
      "Epoch: 14. Training data loss:  1801.003662109375\n",
      "Epoch: 15. Training data loss:  1800.9022216796875\n",
      "Epoch: 16. Training data loss:  1800.840087890625\n",
      "Epoch: 17. Training data loss:  1800.680908203125\n",
      "Epoch: 18. Training data loss:  1800.6297607421875\n",
      "Epoch: 19. Training data loss:  1800.574462890625\n",
      "Epoch: 20. Training data loss:  1800.41650390625\n",
      "Epoch: 21. Training data loss:  1800.4146728515625\n",
      "Epoch: 22. Training data loss:  1800.3419189453125\n",
      "Epoch: 23. Training data loss:  1800.1737060546875\n",
      "Epoch: 24. Training data loss:  1800.2613525390625\n",
      "Epoch: 25. Training data loss:  1800.0567626953125\n",
      "Epoch: 26. Training data loss:  1799.897216796875\n",
      "Epoch: 27. Training data loss:  1799.8074951171875\n",
      "Epoch: 28. Training data loss:  1799.7138671875\n",
      "Epoch: 29. Training data loss:  1799.704833984375\n",
      "Epoch: 30. Training data loss:  1799.621826171875\n",
      "Epoch: 31. Training data loss:  1799.5263671875\n",
      "Epoch: 32. Training data loss:  1799.4576416015625\n",
      "Epoch: 33. Training data loss:  1799.357421875\n",
      "Epoch: 34. Training data loss:  1799.3009033203125\n",
      "Epoch: 35. Training data loss:  1799.1446533203125\n",
      "Epoch: 36. Training data loss:  1799.04638671875\n",
      "Epoch: 37. Training data loss:  1798.8651123046875\n",
      "Epoch: 38. Training data loss:  1798.892333984375\n",
      "Epoch: 39. Training data loss:  1798.81689453125\n",
      "Accuracy found: 1.2\n",
      "40 0.0001 0.01 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 28\n",
      "Epoch: 0. Training data loss:  1086.9256591796875\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17264/909083236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_17264/2746405155.py\u001b[0m in \u001b[0;36mrand_search\u001b[0;34m(model_type, param_dist, classes, n_iter)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mhyper_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mstrings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17264/3556796506.py\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(net, trainloader, hyperparam)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m## Make predictions for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Compute the loss and its gradients, save the current value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17264/3005411290.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "print(rand_search('cnn',param_dist, classes, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [10, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  4357.9130859375\n",
      "Epoch: 1. Training data loss:  3657.0693359375\n",
      "Epoch: 2. Training data loss:  3498.6259765625\n",
      "Epoch: 3. Training data loss:  3442.236328125\n",
      "Epoch: 4. Training data loss:  3415.665283203125\n",
      "Epoch: 5. Training data loss:  3400.51904296875\n",
      "Epoch: 6. Training data loss:  3392.177001953125\n",
      "Epoch: 7. Training data loss:  3388.744384765625\n",
      "Epoch: 8. Training data loss:  3378.097412109375\n",
      "Epoch: 9. Training data loss:  3369.974609375\n",
      "Epoch: 10. Training data loss:  3365.143310546875\n",
      "Epoch: 11. Training data loss:  3360.560791015625\n",
      "Epoch: 12. Training data loss:  3353.3359375\n",
      "Epoch: 13. Training data loss:  3344.362548828125\n",
      "Epoch: 14. Training data loss:  3334.498046875\n",
      "Epoch: 15. Training data loss:  3323.3779296875\n",
      "Epoch: 16. Training data loss:  3313.024658203125\n",
      "Epoch: 17. Training data loss:  3306.869140625\n",
      "Epoch: 18. Training data loss:  3296.20703125\n",
      "Epoch: 19. Training data loss:  3285.71240234375\n",
      "Epoch: 20. Training data loss:  3273.778076171875\n",
      "Epoch: 21. Training data loss:  3259.06640625\n",
      "Epoch: 22. Training data loss:  3251.518310546875\n",
      "Epoch: 23. Training data loss:  3245.63232421875\n",
      "Epoch: 24. Training data loss:  3236.916748046875\n",
      "Epoch: 25. Training data loss:  3226.357421875\n",
      "Epoch: 26. Training data loss:  3223.487060546875\n",
      "Epoch: 27. Training data loss:  3216.968017578125\n",
      "Epoch: 28. Training data loss:  3217.3564453125\n",
      "Epoch: 29. Training data loss:  3214.289306640625\n",
      "Epoch: 30. Training data loss:  3214.38623046875\n",
      "Epoch: 31. Training data loss:  3209.366943359375\n",
      "Epoch: 32. Training data loss:  3206.65966796875\n",
      "Epoch: 33. Training data loss:  3207.31787109375\n",
      "Epoch: 34. Training data loss:  3206.12255859375\n",
      "Epoch: 35. Training data loss:  3204.383544921875\n",
      "Epoch: 36. Training data loss:  3204.413330078125\n",
      "Epoch: 37. Training data loss:  3200.1943359375\n",
      "Epoch: 38. Training data loss:  3202.351806640625\n",
      "Epoch: 39. Training data loss:  3202.981201171875\n",
      "Accuracy found: 6.22\n",
      "40 0.01 0.01 MultiMarginLoss() 10 0\n",
      "Starting iteration, 1\n",
      "Epoch: 0. Training data loss:  3421.510009765625\n",
      "Epoch: 1. Training data loss:  3284.73828125\n",
      "Epoch: 2. Training data loss:  3217.6533203125\n",
      "Epoch: 3. Training data loss:  3157.8076171875\n",
      "Epoch: 4. Training data loss:  3109.251220703125\n",
      "Epoch: 5. Training data loss:  3069.72509765625\n",
      "Epoch: 6. Training data loss:  3038.6162109375\n",
      "Epoch: 7. Training data loss:  3009.54638671875\n",
      "Epoch: 8. Training data loss:  2982.671142578125\n",
      "Epoch: 9. Training data loss:  2956.724853515625\n",
      "Epoch: 10. Training data loss:  2933.832763671875\n",
      "Epoch: 11. Training data loss:  2911.49169921875\n",
      "Epoch: 12. Training data loss:  2888.426513671875\n",
      "Epoch: 13. Training data loss:  2868.234375\n",
      "Epoch: 14. Training data loss:  2847.520263671875\n",
      "Epoch: 15. Training data loss:  2829.310791015625\n",
      "Epoch: 16. Training data loss:  2811.58349609375\n",
      "Epoch: 17. Training data loss:  2796.51611328125\n",
      "Epoch: 18. Training data loss:  2780.044921875\n",
      "Epoch: 19. Training data loss:  2766.0625\n",
      "Accuracy found: 16.66\n",
      "20 0.0001 0.001 CrossEntropyLoss() 64 1\n",
      "Starting iteration, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [32, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  1544.572998046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [16, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Training data loss:  1545.61279296875\n",
      "Epoch: 2. Training data loss:  1546.91943359375\n",
      "Epoch: 3. Training data loss:  1547.299560546875\n",
      "Epoch: 4. Training data loss:  1547.3594970703125\n",
      "Epoch: 5. Training data loss:  1547.37451171875\n",
      "Epoch: 6. Training data loss:  1547.378173828125\n",
      "Epoch: 7. Training data loss:  1547.3736572265625\n",
      "Epoch: 8. Training data loss:  1547.379638671875\n",
      "Epoch: 9. Training data loss:  1547.3721923828125\n",
      "Epoch: 10. Training data loss:  1547.374755859375\n",
      "Epoch: 11. Training data loss:  1547.3712158203125\n",
      "Epoch: 12. Training data loss:  1547.373046875\n",
      "Epoch: 13. Training data loss:  1547.3681640625\n",
      "Epoch: 14. Training data loss:  1547.3671875\n",
      "Epoch: 15. Training data loss:  1547.3743896484375\n",
      "Epoch: 16. Training data loss:  1547.3746337890625\n",
      "Epoch: 17. Training data loss:  1547.3756103515625\n",
      "Epoch: 18. Training data loss:  1547.3704833984375\n",
      "Epoch: 19. Training data loss:  1547.3740234375\n",
      "Accuracy found: 1.0\n",
      "20 0.001 0.3 MultiMarginLoss() 32 0\n",
      "Starting iteration, 3\n",
      "Epoch: 0. Training data loss:  3679.29931640625\n",
      "Epoch: 1. Training data loss:  5114.298828125\n",
      "Epoch: 2. Training data loss:  4697.93408203125\n",
      "Epoch: 3. Training data loss:  4197.80224609375\n",
      "Epoch: 4. Training data loss:  3851.688720703125\n",
      "Epoch: 5. Training data loss:  3501.193115234375\n",
      "Epoch: 6. Training data loss:  6031.0810546875\n",
      "Epoch: 7. Training data loss:  5913.1533203125\n",
      "Epoch: 8. Training data loss:  3807.24755859375\n",
      "Epoch: 9. Training data loss:  3387.44189453125\n",
      "Epoch: 10. Training data loss:  5183.37939453125\n",
      "Epoch: 11. Training data loss:  5244.6005859375\n",
      "Epoch: 12. Training data loss:  3606.01904296875\n",
      "Epoch: 13. Training data loss:  3452.3505859375\n",
      "Epoch: 14. Training data loss:  3485.203857421875\n",
      "Epoch: 15. Training data loss:  3345.491455078125\n",
      "Epoch: 16. Training data loss:  6966.5908203125\n",
      "Epoch: 17. Training data loss:  4086.66552734375\n",
      "Epoch: 18. Training data loss:  3838.4248046875\n",
      "Epoch: 19. Training data loss:  4860.7578125\n",
      "Epoch: 20. Training data loss:  3767.733642578125\n",
      "Epoch: 21. Training data loss:  3723.314453125\n",
      "Epoch: 22. Training data loss:  4977.78759765625\n",
      "Epoch: 23. Training data loss:  3741.146240234375\n",
      "Epoch: 24. Training data loss:  3598.783203125\n",
      "Epoch: 25. Training data loss:  3640.938720703125\n",
      "Epoch: 26. Training data loss:  3638.707275390625\n",
      "Epoch: 27. Training data loss:  3584.258544921875\n",
      "Epoch: 28. Training data loss:  4300.80322265625\n",
      "Epoch: 29. Training data loss:  4391.9375\n",
      "Epoch: 30. Training data loss:  3628.884765625\n",
      "Epoch: 31. Training data loss:  3512.651611328125\n",
      "Epoch: 32. Training data loss:  3591.060546875\n",
      "Epoch: 33. Training data loss:  3820.312744140625\n",
      "Epoch: 34. Training data loss:  3850.1123046875\n",
      "Epoch: 35. Training data loss:  3680.06884765625\n",
      "Epoch: 36. Training data loss:  4014.67041015625\n",
      "Epoch: 37. Training data loss:  3929.417724609375\n",
      "Epoch: 38. Training data loss:  3812.3759765625\n",
      "Epoch: 39. Training data loss:  4161.68701171875\n",
      "Epoch: 40. Training data loss:  3783.092529296875\n",
      "Epoch: 41. Training data loss:  6162.66357421875\n",
      "Epoch: 42. Training data loss:  5847.42578125\n",
      "Epoch: 43. Training data loss:  3900.505126953125\n",
      "Epoch: 44. Training data loss:  3534.611083984375\n",
      "Epoch: 45. Training data loss:  5034.642578125\n",
      "Epoch: 46. Training data loss:  4840.6220703125\n",
      "Epoch: 47. Training data loss:  4761.40966796875\n",
      "Epoch: 48. Training data loss:  5309.2578125\n",
      "Epoch: 49. Training data loss:  3962.130126953125\n",
      "Epoch: 50. Training data loss:  5326.837890625\n",
      "Epoch: 51. Training data loss:  3887.928466796875\n",
      "Epoch: 52. Training data loss:  3569.94677734375\n",
      "Epoch: 53. Training data loss:  3603.98681640625\n",
      "Epoch: 54. Training data loss:  3551.197509765625\n",
      "Epoch: 55. Training data loss:  5018.564453125\n",
      "Epoch: 56. Training data loss:  4967.396484375\n",
      "Epoch: 57. Training data loss:  5528.5302734375\n",
      "Epoch: 58. Training data loss:  4574.6533203125\n",
      "Epoch: 59. Training data loss:  5527.49267578125\n",
      "Accuracy found: 1.3\n",
      "60 0.001 0.01 MultiMarginLoss() 10 1\n",
      "Starting iteration, 4\n",
      "Epoch: 0. Training data loss:  1472.442138671875\n",
      "Epoch: 1. Training data loss:  1290.353759765625\n",
      "Epoch: 2. Training data loss:  1175.30322265625\n",
      "Epoch: 3. Training data loss:  1102.7952880859375\n",
      "Epoch: 4. Training data loss:  1056.4512939453125\n",
      "Epoch: 5. Training data loss:  1024.9610595703125\n",
      "Epoch: 6. Training data loss:  1002.2078247070312\n",
      "Epoch: 7. Training data loss:  983.3922729492188\n",
      "Epoch: 8. Training data loss:  967.62158203125\n",
      "Epoch: 9. Training data loss:  956.806884765625\n",
      "Epoch: 10. Training data loss:  951.4229736328125\n",
      "Epoch: 11. Training data loss:  944.2977294921875\n",
      "Epoch: 12. Training data loss:  936.2843627929688\n",
      "Epoch: 13. Training data loss:  928.1366577148438\n",
      "Epoch: 14. Training data loss:  921.0941162109375\n",
      "Epoch: 15. Training data loss:  915.9786987304688\n",
      "Epoch: 16. Training data loss:  910.7235107421875\n",
      "Epoch: 17. Training data loss:  904.5529174804688\n",
      "Epoch: 18. Training data loss:  898.5341796875\n",
      "Epoch: 19. Training data loss:  893.2363891601562\n",
      "Epoch: 20. Training data loss:  889.2009887695312\n",
      "Epoch: 21. Training data loss:  884.6055297851562\n",
      "Epoch: 22. Training data loss:  879.8741455078125\n",
      "Epoch: 23. Training data loss:  874.72021484375\n",
      "Epoch: 24. Training data loss:  869.21875\n",
      "Epoch: 25. Training data loss:  864.6210327148438\n",
      "Epoch: 26. Training data loss:  861.4927978515625\n",
      "Epoch: 27. Training data loss:  857.8990478515625\n",
      "Epoch: 28. Training data loss:  852.1273803710938\n",
      "Epoch: 29. Training data loss:  845.0965576171875\n",
      "Epoch: 30. Training data loss:  839.5823364257812\n",
      "Epoch: 31. Training data loss:  833.482421875\n",
      "Epoch: 32. Training data loss:  825.9356689453125\n",
      "Epoch: 33. Training data loss:  818.2901000976562\n",
      "Epoch: 34. Training data loss:  811.152587890625\n",
      "Epoch: 35. Training data loss:  807.2572631835938\n",
      "Epoch: 36. Training data loss:  805.7058715820312\n",
      "Epoch: 37. Training data loss:  803.8682250976562\n",
      "Epoch: 38. Training data loss:  801.4395141601562\n",
      "Epoch: 39. Training data loss:  797.3574829101562\n",
      "Accuracy found: 9.38\n",
      "40 0.01 0.001 MultiMarginLoss() 32 0\n",
      "(16.66, (20, 0.0001, 0.001, CrossEntropyLoss(), 64, 1))\n"
     ]
    }
   ],
   "source": [
    "print(rand_search('cnn',param_dist, classes, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration, 0\n",
      "Epoch: 0. Training data loss:  1459.75439453125\n",
      "Epoch: 1. Training data loss:  1337.322509765625\n",
      "Epoch: 2. Training data loss:  1261.29296875\n",
      "Epoch: 3. Training data loss:  1206.3465576171875\n",
      "Epoch: 4. Training data loss:  1166.3037109375\n",
      "Epoch: 5. Training data loss:  1129.4825439453125\n",
      "Epoch: 6. Training data loss:  1097.373291015625\n",
      "Epoch: 7. Training data loss:  1070.7666015625\n",
      "Epoch: 8. Training data loss:  1045.984619140625\n",
      "Epoch: 9. Training data loss:  1023.3833618164062\n",
      "Epoch: 10. Training data loss:  1003.4993286132812\n",
      "Epoch: 11. Training data loss:  985.766845703125\n",
      "Epoch: 12. Training data loss:  972.1051635742188\n",
      "Epoch: 13. Training data loss:  957.6221923828125\n",
      "Epoch: 14. Training data loss:  946.2266845703125\n",
      "Epoch: 15. Training data loss:  933.6317138671875\n",
      "Epoch: 16. Training data loss:  920.5033569335938\n",
      "Epoch: 17. Training data loss:  910.262451171875\n",
      "Epoch: 18. Training data loss:  902.8070678710938\n",
      "Epoch: 19. Training data loss:  893.0336303710938\n",
      "Epoch: 20. Training data loss:  886.3616943359375\n",
      "Epoch: 21. Training data loss:  879.456787109375\n",
      "Epoch: 22. Training data loss:  872.8517456054688\n",
      "Epoch: 23. Training data loss:  865.3350830078125\n",
      "Epoch: 24. Training data loss:  860.7506713867188\n",
      "Epoch: 25. Training data loss:  854.0582275390625\n",
      "Epoch: 26. Training data loss:  848.0364990234375\n",
      "Epoch: 27. Training data loss:  842.9241943359375\n",
      "Epoch: 28. Training data loss:  840.6426391601562\n",
      "Epoch: 29. Training data loss:  835.3284912109375\n",
      "Epoch: 30. Training data loss:  829.843017578125\n",
      "Epoch: 31. Training data loss:  827.966796875\n",
      "Epoch: 32. Training data loss:  824.9202880859375\n",
      "Epoch: 33. Training data loss:  821.1301879882812\n",
      "Epoch: 34. Training data loss:  816.7158813476562\n",
      "Epoch: 35. Training data loss:  813.4253540039062\n",
      "Epoch: 36. Training data loss:  810.507568359375\n",
      "Epoch: 37. Training data loss:  807.5155639648438\n",
      "Epoch: 38. Training data loss:  804.835205078125\n",
      "Epoch: 39. Training data loss:  802.2098999023438\n",
      "Accuracy found: 9.29\n",
      "40 0.001 0.01 MultiMarginLoss() 32 0\n",
      "Starting iteration, 1\n",
      "Epoch: 0. Training data loss:  3589.65380859375\n",
      "Epoch: 1. Training data loss:  3543.74658203125\n",
      "Epoch: 2. Training data loss:  3517.7802734375\n",
      "Epoch: 3. Training data loss:  3499.69921875\n",
      "Epoch: 4. Training data loss:  3484.910888671875\n",
      "Epoch: 5. Training data loss:  3473.53857421875\n",
      "Epoch: 6. Training data loss:  3463.407470703125\n",
      "Epoch: 7. Training data loss:  3455.497802734375\n",
      "Epoch: 8. Training data loss:  3448.414794921875\n",
      "Epoch: 9. Training data loss:  3442.543701171875\n",
      "Epoch: 10. Training data loss:  3438.02197265625\n",
      "Epoch: 11. Training data loss:  3434.39208984375\n",
      "Epoch: 12. Training data loss:  3431.275146484375\n",
      "Epoch: 13. Training data loss:  3429.2353515625\n",
      "Epoch: 14. Training data loss:  3427.37060546875\n",
      "Epoch: 15. Training data loss:  3426.020751953125\n",
      "Epoch: 16. Training data loss:  3424.884521484375\n",
      "Epoch: 17. Training data loss:  3424.049072265625\n",
      "Epoch: 18. Training data loss:  3423.59130859375\n",
      "Epoch: 19. Training data loss:  3423.368408203125\n",
      "Epoch: 20. Training data loss:  3422.887451171875\n",
      "Epoch: 21. Training data loss:  3422.583740234375\n",
      "Epoch: 22. Training data loss:  3422.539306640625\n",
      "Epoch: 23. Training data loss:  3422.548583984375\n",
      "Epoch: 24. Training data loss:  3422.4765625\n",
      "Epoch: 25. Training data loss:  3422.435302734375\n",
      "Epoch: 26. Training data loss:  3422.591552734375\n",
      "Epoch: 27. Training data loss:  3422.73681640625\n",
      "Epoch: 28. Training data loss:  3422.713134765625\n",
      "Epoch: 29. Training data loss:  3423.026611328125\n",
      "Epoch: 30. Training data loss:  3423.1337890625\n",
      "Epoch: 31. Training data loss:  3423.1943359375\n",
      "Epoch: 32. Training data loss:  3423.065673828125\n",
      "Epoch: 33. Training data loss:  3423.379150390625\n",
      "Epoch: 34. Training data loss:  3423.476806640625\n",
      "Epoch: 35. Training data loss:  3423.418212890625\n",
      "Epoch: 36. Training data loss:  3423.43603515625\n",
      "Epoch: 37. Training data loss:  3423.486328125\n",
      "Epoch: 38. Training data loss:  3423.730224609375\n",
      "Epoch: 39. Training data loss:  3423.4228515625\n",
      "Epoch: 40. Training data loss:  3423.650634765625\n",
      "Epoch: 41. Training data loss:  3423.662841796875\n",
      "Epoch: 42. Training data loss:  3423.797119140625\n",
      "Epoch: 43. Training data loss:  3423.886474609375\n",
      "Epoch: 44. Training data loss:  3423.95751953125\n",
      "Epoch: 45. Training data loss:  3423.83154296875\n",
      "Epoch: 46. Training data loss:  3423.77685546875\n",
      "Epoch: 47. Training data loss:  3423.9169921875\n",
      "Epoch: 48. Training data loss:  3423.711181640625\n",
      "Epoch: 49. Training data loss:  3423.616943359375\n",
      "Epoch: 50. Training data loss:  3423.5322265625\n",
      "Epoch: 51. Training data loss:  3423.73876953125\n",
      "Epoch: 52. Training data loss:  3423.36083984375\n",
      "Epoch: 53. Training data loss:  3423.36328125\n",
      "Epoch: 54. Training data loss:  3423.488037109375\n",
      "Epoch: 55. Training data loss:  3423.353759765625\n",
      "Epoch: 56. Training data loss:  3423.369384765625\n",
      "Epoch: 57. Training data loss:  3423.36328125\n",
      "Epoch: 58. Training data loss:  3423.50439453125\n",
      "Epoch: 59. Training data loss:  3423.261474609375\n",
      "Accuracy found: 4.81\n",
      "60 0.001 0.3 CrossEntropyLoss() 64 0\n",
      "Starting iteration, 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [128, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [80, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  359.1629638671875\n",
      "Epoch: 1. Training data loss:  369.9282531738281\n",
      "Epoch: 2. Training data loss:  355.68817138671875\n",
      "Epoch: 3. Training data loss:  359.4193115234375\n",
      "Epoch: 4. Training data loss:  356.3942565917969\n",
      "Epoch: 5. Training data loss:  358.9566955566406\n",
      "Epoch: 6. Training data loss:  358.12261962890625\n",
      "Epoch: 7. Training data loss:  356.73876953125\n",
      "Epoch: 8. Training data loss:  363.5995788574219\n",
      "Epoch: 9. Training data loss:  357.6102600097656\n",
      "Epoch: 10. Training data loss:  363.17047119140625\n",
      "Epoch: 11. Training data loss:  369.6217956542969\n",
      "Epoch: 12. Training data loss:  360.6918029785156\n",
      "Epoch: 13. Training data loss:  359.0041809082031\n",
      "Epoch: 14. Training data loss:  359.0165710449219\n",
      "Epoch: 15. Training data loss:  363.46820068359375\n",
      "Epoch: 16. Training data loss:  357.5591125488281\n",
      "Epoch: 17. Training data loss:  362.93096923828125\n",
      "Epoch: 18. Training data loss:  358.6981201171875\n",
      "Epoch: 19. Training data loss:  361.39385986328125\n",
      "Epoch: 20. Training data loss:  357.065185546875\n",
      "Epoch: 21. Training data loss:  364.1210021972656\n",
      "Epoch: 22. Training data loss:  357.4869384765625\n",
      "Epoch: 23. Training data loss:  357.61578369140625\n",
      "Epoch: 24. Training data loss:  357.6905822753906\n",
      "Epoch: 25. Training data loss:  361.837158203125\n",
      "Epoch: 26. Training data loss:  360.23016357421875\n",
      "Epoch: 27. Training data loss:  357.5557861328125\n",
      "Epoch: 28. Training data loss:  356.04534912109375\n",
      "Epoch: 29. Training data loss:  358.56304931640625\n",
      "Epoch: 30. Training data loss:  359.1722412109375\n",
      "Epoch: 31. Training data loss:  360.324462890625\n",
      "Epoch: 32. Training data loss:  357.9178161621094\n",
      "Epoch: 33. Training data loss:  359.68292236328125\n",
      "Epoch: 34. Training data loss:  359.0768127441406\n",
      "Epoch: 35. Training data loss:  357.81024169921875\n",
      "Epoch: 36. Training data loss:  359.5467834472656\n",
      "Epoch: 37. Training data loss:  361.9908142089844\n",
      "Epoch: 38. Training data loss:  361.49560546875\n",
      "Epoch: 39. Training data loss:  363.72161865234375\n",
      "Accuracy found: 1.4\n",
      "40 0.01 0.3 MultiMarginLoss() 128 1\n",
      "Starting iteration, 3\n",
      "Epoch: 0. Training data loss:  1167.8734130859375\n",
      "Epoch: 1. Training data loss:  942.6537475585938\n",
      "Epoch: 2. Training data loss:  854.9738159179688\n",
      "Epoch: 3. Training data loss:  814.1793823242188\n",
      "Epoch: 4. Training data loss:  785.2435302734375\n",
      "Epoch: 5. Training data loss:  764.9632568359375\n",
      "Epoch: 6. Training data loss:  750.552978515625\n",
      "Epoch: 7. Training data loss:  741.05908203125\n",
      "Epoch: 8. Training data loss:  731.94677734375\n",
      "Epoch: 9. Training data loss:  719.4673461914062\n",
      "Epoch: 10. Training data loss:  709.87548828125\n",
      "Epoch: 11. Training data loss:  711.3008422851562\n",
      "Epoch: 12. Training data loss:  711.5239868164062\n",
      "Epoch: 13. Training data loss:  708.1826782226562\n",
      "Epoch: 14. Training data loss:  700.5420532226562\n",
      "Epoch: 15. Training data loss:  693.8167724609375\n",
      "Epoch: 16. Training data loss:  694.4744873046875\n",
      "Epoch: 17. Training data loss:  697.6710205078125\n",
      "Epoch: 18. Training data loss:  702.670166015625\n",
      "Epoch: 19. Training data loss:  708.0869750976562\n",
      "Epoch: 20. Training data loss:  701.9473266601562\n",
      "Epoch: 21. Training data loss:  695.4180908203125\n",
      "Epoch: 22. Training data loss:  702.4285888671875\n",
      "Epoch: 23. Training data loss:  705.30029296875\n",
      "Epoch: 24. Training data loss:  701.8370361328125\n",
      "Epoch: 25. Training data loss:  701.4529418945312\n",
      "Epoch: 26. Training data loss:  716.6904296875\n",
      "Epoch: 27. Training data loss:  700.291015625\n",
      "Epoch: 28. Training data loss:  705.7420043945312\n",
      "Epoch: 29. Training data loss:  703.10986328125\n",
      "Epoch: 30. Training data loss:  693.3374633789062\n",
      "Epoch: 31. Training data loss:  693.2081298828125\n",
      "Epoch: 32. Training data loss:  697.4530029296875\n",
      "Epoch: 33. Training data loss:  697.05322265625\n",
      "Epoch: 34. Training data loss:  696.5562744140625\n",
      "Epoch: 35. Training data loss:  694.9856567382812\n",
      "Epoch: 36. Training data loss:  703.8038330078125\n",
      "Epoch: 37. Training data loss:  709.1600341796875\n",
      "Epoch: 38. Training data loss:  691.7330322265625\n",
      "Epoch: 39. Training data loss:  703.2350463867188\n",
      "Accuracy found: 14.32\n",
      "40 0.01 0.001 MultiMarginLoss() 32 0\n",
      "Starting iteration, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [64, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  736.576171875\n",
      "Epoch: 1. Training data loss:  671.873291015625\n",
      "Epoch: 2. Training data loss:  632.4443359375\n",
      "Epoch: 3. Training data loss:  604.5315551757812\n",
      "Epoch: 4. Training data loss:  582.8593139648438\n",
      "Epoch: 5. Training data loss:  564.5211181640625\n",
      "Epoch: 6. Training data loss:  548.109375\n",
      "Epoch: 7. Training data loss:  534.0819091796875\n",
      "Epoch: 8. Training data loss:  520.609130859375\n",
      "Epoch: 9. Training data loss:  510.0650634765625\n",
      "Epoch: 10. Training data loss:  499.1435546875\n",
      "Epoch: 11. Training data loss:  490.3523254394531\n",
      "Epoch: 12. Training data loss:  482.46051025390625\n",
      "Epoch: 13. Training data loss:  473.6665344238281\n",
      "Epoch: 14. Training data loss:  466.6354064941406\n",
      "Epoch: 15. Training data loss:  460.0693054199219\n",
      "Epoch: 16. Training data loss:  453.7813415527344\n",
      "Epoch: 17. Training data loss:  448.30572509765625\n",
      "Epoch: 18. Training data loss:  443.3452453613281\n",
      "Epoch: 19. Training data loss:  438.2219543457031\n",
      "Epoch: 20. Training data loss:  433.62921142578125\n",
      "Epoch: 21. Training data loss:  429.3480224609375\n",
      "Epoch: 22. Training data loss:  425.4697570800781\n",
      "Epoch: 23. Training data loss:  421.4226379394531\n",
      "Epoch: 24. Training data loss:  418.0123596191406\n",
      "Epoch: 25. Training data loss:  415.1533203125\n",
      "Epoch: 26. Training data loss:  411.0364074707031\n",
      "Epoch: 27. Training data loss:  408.5629577636719\n",
      "Epoch: 28. Training data loss:  405.5586853027344\n",
      "Epoch: 29. Training data loss:  403.24542236328125\n",
      "Epoch: 30. Training data loss:  400.9018249511719\n",
      "Epoch: 31. Training data loss:  398.5892028808594\n",
      "Epoch: 32. Training data loss:  396.0292663574219\n",
      "Epoch: 33. Training data loss:  394.0805969238281\n",
      "Epoch: 34. Training data loss:  391.6306457519531\n",
      "Epoch: 35. Training data loss:  389.9985046386719\n",
      "Epoch: 36. Training data loss:  387.2391662597656\n",
      "Epoch: 37. Training data loss:  385.6338806152344\n",
      "Epoch: 38. Training data loss:  384.4356384277344\n",
      "Epoch: 39. Training data loss:  381.63494873046875\n",
      "Accuracy found: 10.3\n",
      "40 0.001 0.001 MultiMarginLoss() 64 0\n",
      "Starting iteration, 5\n",
      "Epoch: 0. Training data loss:  22618.625\n",
      "Epoch: 1. Training data loss:  21952.716796875\n",
      "Epoch: 2. Training data loss:  21476.841796875\n",
      "Epoch: 3. Training data loss:  21088.568359375\n",
      "Epoch: 4. Training data loss:  20765.447265625\n",
      "Epoch: 5. Training data loss:  20510.146484375\n",
      "Epoch: 6. Training data loss:  20294.794921875\n",
      "Epoch: 7. Training data loss:  20107.158203125\n",
      "Epoch: 8. Training data loss:  19948.234375\n",
      "Epoch: 9. Training data loss:  19810.2578125\n",
      "Epoch: 10. Training data loss:  19692.03515625\n",
      "Epoch: 11. Training data loss:  19579.814453125\n",
      "Epoch: 12. Training data loss:  19482.98828125\n",
      "Epoch: 13. Training data loss:  19401.5390625\n",
      "Epoch: 14. Training data loss:  19316.3984375\n",
      "Epoch: 15. Training data loss:  19238.3125\n",
      "Epoch: 16. Training data loss:  19174.64453125\n",
      "Epoch: 17. Training data loss:  19110.044921875\n",
      "Epoch: 18. Training data loss:  19049.625\n",
      "Epoch: 19. Training data loss:  18994.1171875\n",
      "Epoch: 20. Training data loss:  18941.55859375\n",
      "Epoch: 21. Training data loss:  18889.078125\n",
      "Epoch: 22. Training data loss:  18845.05859375\n",
      "Epoch: 23. Training data loss:  18800.482421875\n",
      "Epoch: 24. Training data loss:  18761.578125\n",
      "Epoch: 25. Training data loss:  18718.314453125\n",
      "Epoch: 26. Training data loss:  18682.986328125\n",
      "Epoch: 27. Training data loss:  18654.72265625\n",
      "Epoch: 28. Training data loss:  18611.2578125\n",
      "Epoch: 29. Training data loss:  18578.111328125\n",
      "Epoch: 30. Training data loss:  18547.015625\n",
      "Epoch: 31. Training data loss:  18517.111328125\n",
      "Epoch: 32. Training data loss:  18494.0078125\n",
      "Epoch: 33. Training data loss:  18458.458984375\n",
      "Epoch: 34. Training data loss:  18436.16015625\n",
      "Epoch: 35. Training data loss:  18405.94140625\n",
      "Epoch: 36. Training data loss:  18381.888671875\n",
      "Epoch: 37. Training data loss:  18360.7890625\n",
      "Epoch: 38. Training data loss:  18337.169921875\n",
      "Epoch: 39. Training data loss:  18308.931640625\n",
      "Epoch: 40. Training data loss:  18281.9453125\n",
      "Epoch: 41. Training data loss:  18266.01171875\n",
      "Epoch: 42. Training data loss:  18238.80859375\n",
      "Epoch: 43. Training data loss:  18219.2421875\n",
      "Epoch: 44. Training data loss:  18203.509765625\n",
      "Epoch: 45. Training data loss:  18180.396484375\n",
      "Epoch: 46. Training data loss:  18154.1875\n",
      "Epoch: 47. Training data loss:  18140.498046875\n",
      "Epoch: 48. Training data loss:  18123.9609375\n",
      "Epoch: 49. Training data loss:  18101.53125\n",
      "Epoch: 50. Training data loss:  18088.94921875\n",
      "Epoch: 51. Training data loss:  18073.189453125\n",
      "Epoch: 52. Training data loss:  18052.736328125\n",
      "Epoch: 53. Training data loss:  18032.349609375\n",
      "Epoch: 54. Training data loss:  18023.123046875\n",
      "Epoch: 55. Training data loss:  18004.275390625\n",
      "Epoch: 56. Training data loss:  17988.955078125\n",
      "Epoch: 57. Training data loss:  17970.662109375\n",
      "Epoch: 58. Training data loss:  17954.3125\n",
      "Epoch: 59. Training data loss:  17936.080078125\n",
      "Accuracy found: 16.48\n",
      "60 0.001 0.001 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 6\n",
      "Epoch: 0. Training data loss:  3429.884033203125\n",
      "Epoch: 1. Training data loss:  3227.0\n",
      "Epoch: 2. Training data loss:  3119.29443359375\n",
      "Epoch: 3. Training data loss:  3052.595947265625\n",
      "Epoch: 4. Training data loss:  3006.553466796875\n",
      "Epoch: 5. Training data loss:  2970.50244140625\n",
      "Epoch: 6. Training data loss:  2941.15576171875\n",
      "Epoch: 7. Training data loss:  2916.052734375\n",
      "Epoch: 8. Training data loss:  2895.50927734375\n",
      "Epoch: 9. Training data loss:  2877.455078125\n",
      "Epoch: 10. Training data loss:  2859.736083984375\n",
      "Epoch: 11. Training data loss:  2845.077880859375\n",
      "Epoch: 12. Training data loss:  2830.74951171875\n",
      "Epoch: 13. Training data loss:  2818.585205078125\n",
      "Epoch: 14. Training data loss:  2806.268310546875\n",
      "Epoch: 15. Training data loss:  2795.172119140625\n",
      "Epoch: 16. Training data loss:  2783.04541015625\n",
      "Epoch: 17. Training data loss:  2775.93603515625\n",
      "Epoch: 18. Training data loss:  2765.718017578125\n",
      "Epoch: 19. Training data loss:  2756.491455078125\n",
      "Epoch: 20. Training data loss:  2746.66357421875\n",
      "Epoch: 21. Training data loss:  2740.44970703125\n",
      "Epoch: 22. Training data loss:  2731.88427734375\n",
      "Epoch: 23. Training data loss:  2724.76171875\n",
      "Epoch: 24. Training data loss:  2718.406005859375\n",
      "Epoch: 25. Training data loss:  2711.501953125\n",
      "Epoch: 26. Training data loss:  2703.93310546875\n",
      "Epoch: 27. Training data loss:  2696.718505859375\n",
      "Epoch: 28. Training data loss:  2692.083251953125\n",
      "Epoch: 29. Training data loss:  2686.2041015625\n",
      "Epoch: 30. Training data loss:  2679.468505859375\n",
      "Epoch: 31. Training data loss:  2674.8046875\n",
      "Epoch: 32. Training data loss:  2667.845947265625\n",
      "Epoch: 33. Training data loss:  2662.348388671875\n",
      "Epoch: 34. Training data loss:  2657.098388671875\n",
      "Epoch: 35. Training data loss:  2652.163818359375\n",
      "Epoch: 36. Training data loss:  2647.969970703125\n",
      "Epoch: 37. Training data loss:  2642.475830078125\n",
      "Epoch: 38. Training data loss:  2637.747314453125\n",
      "Epoch: 39. Training data loss:  2634.341796875\n",
      "Accuracy found: 18.99\n",
      "40 0.01 0.001 CrossEntropyLoss() 64 0\n",
      "Starting iteration, 7\n",
      "Epoch: 0. Training data loss:  6227.2890625\n",
      "Epoch: 1. Training data loss:  5809.25146484375\n",
      "Epoch: 2. Training data loss:  5676.72412109375\n",
      "Epoch: 3. Training data loss:  5596.1904296875\n",
      "Epoch: 4. Training data loss:  5535.4267578125\n",
      "Epoch: 5. Training data loss:  5489.3427734375\n",
      "Epoch: 6. Training data loss:  5457.876953125\n",
      "Epoch: 7. Training data loss:  5433.04541015625\n",
      "Epoch: 8. Training data loss:  5411.0\n",
      "Epoch: 9. Training data loss:  5400.21826171875\n",
      "Epoch: 10. Training data loss:  5387.48779296875\n",
      "Epoch: 11. Training data loss:  5381.240234375\n",
      "Epoch: 12. Training data loss:  5369.58203125\n",
      "Epoch: 13. Training data loss:  5364.5859375\n",
      "Epoch: 14. Training data loss:  5359.66357421875\n",
      "Epoch: 15. Training data loss:  5359.05419921875\n",
      "Epoch: 16. Training data loss:  5359.95166015625\n",
      "Epoch: 17. Training data loss:  5357.494140625\n",
      "Epoch: 18. Training data loss:  5361.4560546875\n",
      "Epoch: 19. Training data loss:  5359.69921875\n",
      "Epoch: 20. Training data loss:  5356.63720703125\n",
      "Epoch: 21. Training data loss:  5359.06201171875\n",
      "Epoch: 22. Training data loss:  5362.56005859375\n",
      "Epoch: 23. Training data loss:  5358.07177734375\n",
      "Epoch: 24. Training data loss:  5363.20751953125\n",
      "Epoch: 25. Training data loss:  5363.10302734375\n",
      "Epoch: 26. Training data loss:  5365.1435546875\n",
      "Epoch: 27. Training data loss:  5364.4384765625\n",
      "Epoch: 28. Training data loss:  5364.54638671875\n",
      "Epoch: 29. Training data loss:  5370.09130859375\n",
      "Epoch: 30. Training data loss:  5368.3076171875\n",
      "Epoch: 31. Training data loss:  5370.2001953125\n",
      "Epoch: 32. Training data loss:  5372.4013671875\n",
      "Epoch: 33. Training data loss:  5375.99609375\n",
      "Epoch: 34. Training data loss:  5378.81298828125\n",
      "Epoch: 35. Training data loss:  5382.58544921875\n",
      "Epoch: 36. Training data loss:  5381.55224609375\n",
      "Epoch: 37. Training data loss:  5382.841796875\n",
      "Epoch: 38. Training data loss:  5387.31591796875\n",
      "Epoch: 39. Training data loss:  5386.5068359375\n",
      "Accuracy found: 18.75\n",
      "40 0.1 0.01 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 8\n",
      "Epoch: 0. Training data loss:  21090.9609375\n",
      "Epoch: 1. Training data loss:  19502.982421875\n",
      "Epoch: 2. Training data loss:  18940.845703125\n",
      "Epoch: 3. Training data loss:  18609.537109375\n",
      "Epoch: 4. Training data loss:  18361.201171875\n",
      "Epoch: 5. Training data loss:  18173.697265625\n",
      "Epoch: 6. Training data loss:  18016.826171875\n",
      "Epoch: 7. Training data loss:  17888.33203125\n",
      "Epoch: 8. Training data loss:  17787.28125\n",
      "Epoch: 9. Training data loss:  17686.396484375\n",
      "Epoch: 10. Training data loss:  17607.90234375\n",
      "Epoch: 11. Training data loss:  17522.037109375\n",
      "Epoch: 12. Training data loss:  17444.060546875\n",
      "Epoch: 13. Training data loss:  17378.1484375\n",
      "Epoch: 14. Training data loss:  17323.1953125\n",
      "Epoch: 15. Training data loss:  17243.61328125\n",
      "Epoch: 16. Training data loss:  17189.533203125\n",
      "Epoch: 17. Training data loss:  17134.337890625\n",
      "Epoch: 18. Training data loss:  17083.384765625\n",
      "Epoch: 19. Training data loss:  17024.900390625\n",
      "Epoch: 20. Training data loss:  16981.11328125\n",
      "Epoch: 21. Training data loss:  16937.09765625\n",
      "Epoch: 22. Training data loss:  16895.640625\n",
      "Epoch: 23. Training data loss:  16842.990234375\n",
      "Epoch: 24. Training data loss:  16816.9609375\n",
      "Epoch: 25. Training data loss:  16774.833984375\n",
      "Epoch: 26. Training data loss:  16735.12109375\n",
      "Epoch: 27. Training data loss:  16719.58203125\n",
      "Epoch: 28. Training data loss:  16688.0\n",
      "Epoch: 29. Training data loss:  16655.5390625\n",
      "Epoch: 30. Training data loss:  16621.724609375\n",
      "Epoch: 31. Training data loss:  16605.107421875\n",
      "Epoch: 32. Training data loss:  16582.462890625\n",
      "Epoch: 33. Training data loss:  16565.111328125\n",
      "Epoch: 34. Training data loss:  16528.724609375\n",
      "Epoch: 35. Training data loss:  16496.037109375\n",
      "Epoch: 36. Training data loss:  16485.5078125\n",
      "Epoch: 37. Training data loss:  16464.7578125\n",
      "Epoch: 38. Training data loss:  16441.8984375\n",
      "Epoch: 39. Training data loss:  16428.41796875\n",
      "Epoch: 40. Training data loss:  16402.4375\n",
      "Epoch: 41. Training data loss:  16381.884765625\n",
      "Epoch: 42. Training data loss:  16363.564453125\n",
      "Epoch: 43. Training data loss:  16337.5966796875\n",
      "Epoch: 44. Training data loss:  16325.4677734375\n",
      "Epoch: 45. Training data loss:  16318.892578125\n",
      "Epoch: 46. Training data loss:  16301.5263671875\n",
      "Epoch: 47. Training data loss:  16279.662109375\n",
      "Epoch: 48. Training data loss:  16271.578125\n",
      "Epoch: 49. Training data loss:  16248.158203125\n",
      "Epoch: 50. Training data loss:  16237.572265625\n",
      "Epoch: 51. Training data loss:  16223.3681640625\n",
      "Epoch: 52. Training data loss:  16223.3994140625\n",
      "Epoch: 53. Training data loss:  16206.1201171875\n",
      "Epoch: 54. Training data loss:  16199.0078125\n",
      "Epoch: 55. Training data loss:  16179.21875\n",
      "Epoch: 56. Training data loss:  16177.544921875\n",
      "Epoch: 57. Training data loss:  16146.9677734375\n",
      "Epoch: 58. Training data loss:  16130.87109375\n",
      "Epoch: 59. Training data loss:  16124.5830078125\n",
      "Accuracy found: 20.63\n",
      "60 0.01 0.001 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 9\n",
      "Epoch: 0. Training data loss:  22602.26953125\n",
      "Epoch: 1. Training data loss:  21877.12890625\n",
      "Epoch: 2. Training data loss:  21378.330078125\n",
      "Epoch: 3. Training data loss:  21010.96484375\n",
      "Epoch: 4. Training data loss:  20700.78515625\n",
      "Epoch: 5. Training data loss:  20462.9375\n",
      "Epoch: 6. Training data loss:  20249.650390625\n",
      "Epoch: 7. Training data loss:  20075.525390625\n",
      "Epoch: 8. Training data loss:  19926.970703125\n",
      "Epoch: 9. Training data loss:  19783.806640625\n",
      "Epoch: 10. Training data loss:  19668.1640625\n",
      "Epoch: 11. Training data loss:  19558.3359375\n",
      "Epoch: 12. Training data loss:  19461.328125\n",
      "Epoch: 13. Training data loss:  19378.21484375\n",
      "Epoch: 14. Training data loss:  19291.39453125\n",
      "Epoch: 15. Training data loss:  19219.82421875\n",
      "Epoch: 16. Training data loss:  19150.310546875\n",
      "Epoch: 17. Training data loss:  19084.8046875\n",
      "Epoch: 18. Training data loss:  19026.783203125\n",
      "Epoch: 19. Training data loss:  18975.193359375\n",
      "Accuracy found: 13.52\n",
      "20 0.001 0.001 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 10\n",
      "Epoch: 0. Training data loss:  3432.39453125\n",
      "Epoch: 1. Training data loss:  3404.49755859375\n",
      "Epoch: 2. Training data loss:  3407.465576171875\n",
      "Epoch: 3. Training data loss:  3408.58056640625\n",
      "Epoch: 4. Training data loss:  3409.73095703125\n",
      "Epoch: 5. Training data loss:  3410.79541015625\n",
      "Epoch: 6. Training data loss:  3411.224853515625\n",
      "Epoch: 7. Training data loss:  3410.524658203125\n",
      "Epoch: 8. Training data loss:  3411.9599609375\n",
      "Epoch: 9. Training data loss:  3411.023193359375\n",
      "Epoch: 10. Training data loss:  3412.13427734375\n",
      "Epoch: 11. Training data loss:  3411.708251953125\n",
      "Epoch: 12. Training data loss:  3411.205322265625\n",
      "Epoch: 13. Training data loss:  3412.09033203125\n",
      "Epoch: 14. Training data loss:  3410.953857421875\n",
      "Epoch: 15. Training data loss:  3411.89013671875\n",
      "Epoch: 16. Training data loss:  3412.02978515625\n",
      "Epoch: 17. Training data loss:  3411.865478515625\n",
      "Epoch: 18. Training data loss:  3411.22021484375\n",
      "Epoch: 19. Training data loss:  3410.867919921875\n",
      "Epoch: 20. Training data loss:  3411.8076171875\n",
      "Epoch: 21. Training data loss:  3411.530517578125\n",
      "Epoch: 22. Training data loss:  3411.341552734375\n",
      "Epoch: 23. Training data loss:  3411.14501953125\n",
      "Epoch: 24. Training data loss:  3411.426025390625\n",
      "Epoch: 25. Training data loss:  3411.423583984375\n",
      "Epoch: 26. Training data loss:  3411.2802734375\n",
      "Epoch: 27. Training data loss:  3410.738037109375\n",
      "Epoch: 28. Training data loss:  3411.224609375\n",
      "Epoch: 29. Training data loss:  3411.087158203125\n",
      "Epoch: 30. Training data loss:  3410.44189453125\n",
      "Epoch: 31. Training data loss:  3411.010986328125\n",
      "Epoch: 32. Training data loss:  3410.71142578125\n",
      "Epoch: 33. Training data loss:  3410.683349609375\n",
      "Epoch: 34. Training data loss:  3410.16357421875\n",
      "Epoch: 35. Training data loss:  3411.16357421875\n",
      "Epoch: 36. Training data loss:  3411.379638671875\n",
      "Epoch: 37. Training data loss:  3410.573486328125\n",
      "Epoch: 38. Training data loss:  3410.297119140625\n",
      "Epoch: 39. Training data loss:  3410.599365234375\n",
      "Epoch: 40. Training data loss:  3410.45068359375\n",
      "Epoch: 41. Training data loss:  3410.0419921875\n",
      "Epoch: 42. Training data loss:  3411.130126953125\n",
      "Epoch: 43. Training data loss:  3410.23046875\n",
      "Epoch: 44. Training data loss:  3410.7529296875\n",
      "Epoch: 45. Training data loss:  3410.921142578125\n",
      "Epoch: 46. Training data loss:  3410.661376953125\n",
      "Epoch: 47. Training data loss:  3410.00341796875\n",
      "Epoch: 48. Training data loss:  3410.10546875\n",
      "Epoch: 49. Training data loss:  3410.111083984375\n",
      "Epoch: 50. Training data loss:  3410.83349609375\n",
      "Epoch: 51. Training data loss:  3410.98779296875\n",
      "Epoch: 52. Training data loss:  3409.52001953125\n",
      "Epoch: 53. Training data loss:  3410.75244140625\n",
      "Epoch: 54. Training data loss:  3410.3349609375\n",
      "Epoch: 55. Training data loss:  3410.27783203125\n",
      "Epoch: 56. Training data loss:  3410.0498046875\n",
      "Epoch: 57. Training data loss:  3409.787109375\n",
      "Epoch: 58. Training data loss:  3411.084228515625\n",
      "Epoch: 59. Training data loss:  3409.683349609375\n",
      "Accuracy found: 4.63\n",
      "60 0.0001 0.3 CrossEntropyLoss() 64 1\n",
      "Starting iteration, 11\n",
      "Epoch: 0. Training data loss:  375.23638916015625\n",
      "Epoch: 1. Training data loss:  353.1705627441406\n",
      "Epoch: 2. Training data loss:  336.7646179199219\n",
      "Epoch: 3. Training data loss:  324.10711669921875\n",
      "Epoch: 4. Training data loss:  313.9071350097656\n",
      "Epoch: 5. Training data loss:  305.3821716308594\n",
      "Epoch: 6. Training data loss:  298.0249328613281\n",
      "Epoch: 7. Training data loss:  291.766845703125\n",
      "Epoch: 8. Training data loss:  285.63037109375\n",
      "Epoch: 9. Training data loss:  280.6576232910156\n",
      "Epoch: 10. Training data loss:  275.7695617675781\n",
      "Epoch: 11. Training data loss:  271.4219665527344\n",
      "Epoch: 12. Training data loss:  267.3359375\n",
      "Epoch: 13. Training data loss:  263.374755859375\n",
      "Epoch: 14. Training data loss:  259.7218933105469\n",
      "Epoch: 15. Training data loss:  256.5704345703125\n",
      "Epoch: 16. Training data loss:  253.11422729492188\n",
      "Epoch: 17. Training data loss:  250.2275848388672\n",
      "Epoch: 18. Training data loss:  246.90203857421875\n",
      "Epoch: 19. Training data loss:  244.60531616210938\n",
      "Accuracy found: 6.36\n",
      "20 0.001 0.001 MultiMarginLoss() 128 0\n",
      "Starting iteration, 12\n",
      "Epoch: 0. Training data loss:  104270.875\n",
      "Epoch: 1. Training data loss:  87917.640625\n",
      "Epoch: 2. Training data loss:  125026.1484375\n",
      "Epoch: 3. Training data loss:  58529.7734375\n",
      "Epoch: 4. Training data loss:  149856.796875\n",
      "Epoch: 5. Training data loss:  118298.2578125\n",
      "Epoch: 6. Training data loss:  17693.484375\n",
      "Epoch: 7. Training data loss:  162994.453125\n",
      "Epoch: 8. Training data loss:  227383.53125\n",
      "Epoch: 9. Training data loss:  13165.279296875\n",
      "Epoch: 10. Training data loss:  2953.324462890625\n",
      "Epoch: 11. Training data loss:  300932.15625\n",
      "Epoch: 12. Training data loss:  181707.21875\n",
      "Epoch: 13. Training data loss:  8457.5048828125\n",
      "Epoch: 14. Training data loss:  2623.4775390625\n",
      "Epoch: 15. Training data loss:  253605.65625\n",
      "Epoch: 16. Training data loss:  189283.03125\n",
      "Epoch: 17. Training data loss:  9355.052734375\n",
      "Epoch: 18. Training data loss:  2600.312255859375\n",
      "Epoch: 19. Training data loss:  368289.65625\n",
      "Accuracy found: 2.37\n",
      "20 0.1 0.001 CrossEntropyLoss() 128 1\n",
      "Starting iteration, 13\n",
      "Epoch: 0. Training data loss:  55034152.0\n",
      "Epoch: 1. Training data loss:  155912960.0\n",
      "Epoch: 2. Training data loss:  302568224.0\n",
      "Epoch: 3. Training data loss:  373733472.0\n",
      "Epoch: 4. Training data loss:  502685408.0\n",
      "Epoch: 5. Training data loss:  520352768.0\n",
      "Epoch: 6. Training data loss:  433205344.0\n",
      "Epoch: 7. Training data loss:  433935296.0\n",
      "Epoch: 8. Training data loss:  452469824.0\n",
      "Epoch: 9. Training data loss:  599024256.0\n",
      "Epoch: 10. Training data loss:  721244352.0\n",
      "Epoch: 11. Training data loss:  551668800.0\n",
      "Epoch: 12. Training data loss:  528603680.0\n",
      "Epoch: 13. Training data loss:  679484800.0\n",
      "Epoch: 14. Training data loss:  556243328.0\n",
      "Epoch: 15. Training data loss:  611965312.0\n",
      "Epoch: 16. Training data loss:  630565696.0\n",
      "Epoch: 17. Training data loss:  744967744.0\n",
      "Epoch: 18. Training data loss:  850955200.0\n",
      "Epoch: 19. Training data loss:  863982528.0\n",
      "Epoch: 20. Training data loss:  932074752.0\n",
      "Epoch: 21. Training data loss:  898698304.0\n",
      "Epoch: 22. Training data loss:  711393408.0\n",
      "Epoch: 23. Training data loss:  643777280.0\n",
      "Epoch: 24. Training data loss:  658550720.0\n",
      "Epoch: 25. Training data loss:  658031744.0\n",
      "Epoch: 26. Training data loss:  647473216.0\n",
      "Epoch: 27. Training data loss:  990240832.0\n",
      "Epoch: 28. Training data loss:  991469568.0\n",
      "Epoch: 29. Training data loss:  725691584.0\n",
      "Epoch: 30. Training data loss:  701640384.0\n",
      "Epoch: 31. Training data loss:  841409408.0\n",
      "Epoch: 32. Training data loss:  967293888.0\n",
      "Epoch: 33. Training data loss:  900957824.0\n",
      "Epoch: 34. Training data loss:  922986816.0\n",
      "Epoch: 35. Training data loss:  1061586880.0\n",
      "Epoch: 36. Training data loss:  1305684736.0\n",
      "Epoch: 37. Training data loss:  1362028288.0\n",
      "Epoch: 38. Training data loss:  1335964032.0\n",
      "Epoch: 39. Training data loss:  1332387968.0\n",
      "Accuracy found: 1.4\n",
      "40 0.1 0.3 MultiMarginLoss() 32 1\n",
      "Starting iteration, 14\n",
      "Epoch: 0. Training data loss:  3124.68310546875\n",
      "Epoch: 1. Training data loss:  2902.128173828125\n",
      "Epoch: 2. Training data loss:  2812.39208984375\n",
      "Epoch: 3. Training data loss:  2752.294921875\n",
      "Epoch: 4. Training data loss:  2703.021484375\n",
      "Epoch: 5. Training data loss:  2665.547607421875\n",
      "Epoch: 6. Training data loss:  2631.793212890625\n",
      "Epoch: 7. Training data loss:  2601.970458984375\n",
      "Epoch: 8. Training data loss:  2574.765380859375\n",
      "Epoch: 9. Training data loss:  2553.222900390625\n",
      "Epoch: 10. Training data loss:  2532.208740234375\n",
      "Epoch: 11. Training data loss:  2513.339111328125\n",
      "Epoch: 12. Training data loss:  2497.53662109375\n",
      "Epoch: 13. Training data loss:  2479.120361328125\n",
      "Epoch: 14. Training data loss:  2464.775634765625\n",
      "Epoch: 15. Training data loss:  2451.89599609375\n",
      "Epoch: 16. Training data loss:  2437.176025390625\n",
      "Epoch: 17. Training data loss:  2426.928466796875\n",
      "Epoch: 18. Training data loss:  2413.248779296875\n",
      "Epoch: 19. Training data loss:  2401.421142578125\n",
      "Epoch: 20. Training data loss:  2393.280517578125\n",
      "Epoch: 21. Training data loss:  2384.03076171875\n",
      "Epoch: 22. Training data loss:  2376.343017578125\n",
      "Epoch: 23. Training data loss:  2364.3427734375\n",
      "Epoch: 24. Training data loss:  2359.773681640625\n",
      "Epoch: 25. Training data loss:  2349.08349609375\n",
      "Epoch: 26. Training data loss:  2341.1513671875\n",
      "Epoch: 27. Training data loss:  2333.772216796875\n",
      "Epoch: 28. Training data loss:  2325.09130859375\n",
      "Epoch: 29. Training data loss:  2319.71533203125\n",
      "Epoch: 30. Training data loss:  2312.66552734375\n",
      "Epoch: 31. Training data loss:  2305.0703125\n",
      "Epoch: 32. Training data loss:  2304.740966796875\n",
      "Epoch: 33. Training data loss:  2295.571533203125\n",
      "Epoch: 34. Training data loss:  2286.9833984375\n",
      "Epoch: 35. Training data loss:  2280.890625\n",
      "Epoch: 36. Training data loss:  2274.850830078125\n",
      "Epoch: 37. Training data loss:  2271.41845703125\n",
      "Epoch: 38. Training data loss:  2266.855712890625\n",
      "Epoch: 39. Training data loss:  2263.248046875\n",
      "Epoch: 40. Training data loss:  2257.912841796875\n",
      "Epoch: 41. Training data loss:  2249.302978515625\n",
      "Epoch: 42. Training data loss:  2247.23388671875\n",
      "Epoch: 43. Training data loss:  2243.13671875\n",
      "Epoch: 44. Training data loss:  2238.067138671875\n",
      "Epoch: 45. Training data loss:  2234.49072265625\n",
      "Epoch: 46. Training data loss:  2229.133056640625\n",
      "Epoch: 47. Training data loss:  2228.565185546875\n",
      "Epoch: 48. Training data loss:  2223.575927734375\n",
      "Epoch: 49. Training data loss:  2222.539306640625\n",
      "Epoch: 50. Training data loss:  2215.486572265625\n",
      "Epoch: 51. Training data loss:  2209.5009765625\n",
      "Epoch: 52. Training data loss:  2208.359375\n",
      "Epoch: 53. Training data loss:  2207.326904296875\n",
      "Epoch: 54. Training data loss:  2203.712646484375\n",
      "Epoch: 55. Training data loss:  2201.177978515625\n",
      "Epoch: 56. Training data loss:  2199.064208984375\n",
      "Epoch: 57. Training data loss:  2197.465087890625\n",
      "Epoch: 58. Training data loss:  2190.007080078125\n",
      "Epoch: 59. Training data loss:  2186.181640625\n",
      "Accuracy found: 23.22\n",
      "60 0.1 0 CrossEntropyLoss() 64 0\n",
      "Starting iteration, 15\n",
      "Epoch: 0. Training data loss:  1180.607666015625\n",
      "Epoch: 1. Training data loss:  946.2782592773438\n",
      "Epoch: 2. Training data loss:  857.3378295898438\n",
      "Epoch: 3. Training data loss:  808.0911865234375\n",
      "Epoch: 4. Training data loss:  779.5407104492188\n",
      "Epoch: 5. Training data loss:  758.98876953125\n",
      "Epoch: 6. Training data loss:  747.0227661132812\n",
      "Epoch: 7. Training data loss:  734.8906860351562\n",
      "Epoch: 8. Training data loss:  726.1517333984375\n",
      "Epoch: 9. Training data loss:  716.429443359375\n",
      "Epoch: 10. Training data loss:  708.2162475585938\n",
      "Epoch: 11. Training data loss:  702.136474609375\n",
      "Epoch: 12. Training data loss:  704.8443603515625\n",
      "Epoch: 13. Training data loss:  701.670654296875\n",
      "Epoch: 14. Training data loss:  691.5595703125\n",
      "Epoch: 15. Training data loss:  682.7852783203125\n",
      "Epoch: 16. Training data loss:  687.3035278320312\n",
      "Epoch: 17. Training data loss:  692.6422729492188\n",
      "Epoch: 18. Training data loss:  689.76318359375\n",
      "Epoch: 19. Training data loss:  690.4481201171875\n",
      "Epoch: 20. Training data loss:  694.3048706054688\n",
      "Epoch: 21. Training data loss:  687.7718505859375\n",
      "Epoch: 22. Training data loss:  691.1470336914062\n",
      "Epoch: 23. Training data loss:  688.19384765625\n",
      "Epoch: 24. Training data loss:  692.0066528320312\n",
      "Epoch: 25. Training data loss:  699.0597534179688\n",
      "Epoch: 26. Training data loss:  700.4918212890625\n",
      "Epoch: 27. Training data loss:  714.3637084960938\n",
      "Epoch: 28. Training data loss:  728.742919921875\n",
      "Epoch: 29. Training data loss:  713.6094360351562\n",
      "Epoch: 30. Training data loss:  711.228515625\n",
      "Epoch: 31. Training data loss:  701.8391723632812\n",
      "Epoch: 32. Training data loss:  703.3681030273438\n",
      "Epoch: 33. Training data loss:  699.7086791992188\n",
      "Epoch: 34. Training data loss:  701.97998046875\n",
      "Epoch: 35. Training data loss:  706.8566284179688\n",
      "Epoch: 36. Training data loss:  690.9758911132812\n",
      "Epoch: 37. Training data loss:  680.3591918945312\n",
      "Epoch: 38. Training data loss:  682.8073120117188\n",
      "Epoch: 39. Training data loss:  699.3035888671875\n",
      "Epoch: 40. Training data loss:  711.1295166015625\n",
      "Epoch: 41. Training data loss:  708.0064697265625\n",
      "Epoch: 42. Training data loss:  711.2418212890625\n",
      "Epoch: 43. Training data loss:  709.6507568359375\n",
      "Epoch: 44. Training data loss:  703.5817260742188\n",
      "Epoch: 45. Training data loss:  741.2116088867188\n",
      "Epoch: 46. Training data loss:  731.591064453125\n",
      "Epoch: 47. Training data loss:  700.7450561523438\n",
      "Epoch: 48. Training data loss:  705.6554565429688\n",
      "Epoch: 49. Training data loss:  685.4325561523438\n",
      "Epoch: 50. Training data loss:  685.3035888671875\n",
      "Epoch: 51. Training data loss:  698.3200073242188\n",
      "Epoch: 52. Training data loss:  722.1768188476562\n",
      "Epoch: 53. Training data loss:  736.8636474609375\n",
      "Epoch: 54. Training data loss:  742.5855712890625\n",
      "Epoch: 55. Training data loss:  770.6656494140625\n",
      "Epoch: 56. Training data loss:  800.6419067382812\n",
      "Epoch: 57. Training data loss:  790.5593872070312\n",
      "Epoch: 58. Training data loss:  725.7664184570312\n",
      "Epoch: 59. Training data loss:  713.984619140625\n",
      "Accuracy found: 14.31\n",
      "60 0.01 0.001 MultiMarginLoss() 32 0\n",
      "Starting iteration, 16\n",
      "Epoch: 0. Training data loss:  1065.775146484375\n",
      "Epoch: 1. Training data loss:  979.633544921875\n",
      "Epoch: 2. Training data loss:  1023.8213500976562\n",
      "Epoch: 3. Training data loss:  1041.8123779296875\n",
      "Epoch: 4. Training data loss:  1045.349853515625\n",
      "Epoch: 5. Training data loss:  1014.6751708984375\n",
      "Epoch: 6. Training data loss:  1032.81005859375\n",
      "Epoch: 7. Training data loss:  1029.28857421875\n",
      "Epoch: 8. Training data loss:  1038.60009765625\n",
      "Epoch: 9. Training data loss:  998.5917358398438\n",
      "Epoch: 10. Training data loss:  996.3512573242188\n",
      "Epoch: 11. Training data loss:  1017.8385620117188\n",
      "Epoch: 12. Training data loss:  1048.181640625\n",
      "Epoch: 13. Training data loss:  1022.04345703125\n",
      "Epoch: 14. Training data loss:  1062.54150390625\n",
      "Epoch: 15. Training data loss:  1069.232177734375\n",
      "Epoch: 16. Training data loss:  1132.155029296875\n",
      "Epoch: 17. Training data loss:  1102.0079345703125\n",
      "Epoch: 18. Training data loss:  1084.3095703125\n",
      "Epoch: 19. Training data loss:  1219.1229248046875\n",
      "Epoch: 20. Training data loss:  1372.6915283203125\n",
      "Epoch: 21. Training data loss:  1543.0780029296875\n",
      "Epoch: 22. Training data loss:  1623.177734375\n",
      "Epoch: 23. Training data loss:  1801.6787109375\n",
      "Epoch: 24. Training data loss:  1875.510986328125\n",
      "Epoch: 25. Training data loss:  1844.5263671875\n",
      "Epoch: 26. Training data loss:  1788.363525390625\n",
      "Epoch: 27. Training data loss:  1997.05029296875\n",
      "Epoch: 28. Training data loss:  1837.2659912109375\n",
      "Epoch: 29. Training data loss:  1720.6463623046875\n",
      "Epoch: 30. Training data loss:  1714.79345703125\n",
      "Epoch: 31. Training data loss:  1843.083740234375\n",
      "Epoch: 32. Training data loss:  1841.536376953125\n",
      "Epoch: 33. Training data loss:  1795.3809814453125\n",
      "Epoch: 34. Training data loss:  1618.6396484375\n",
      "Epoch: 35. Training data loss:  1647.5028076171875\n",
      "Epoch: 36. Training data loss:  1695.1571044921875\n",
      "Epoch: 37. Training data loss:  1856.097900390625\n",
      "Epoch: 38. Training data loss:  1760.5740966796875\n",
      "Epoch: 39. Training data loss:  1716.84423828125\n",
      "Epoch: 40. Training data loss:  1832.242431640625\n",
      "Epoch: 41. Training data loss:  1908.66015625\n",
      "Epoch: 42. Training data loss:  1813.4708251953125\n",
      "Epoch: 43. Training data loss:  1764.3331298828125\n",
      "Epoch: 44. Training data loss:  1782.38818359375\n",
      "Epoch: 45. Training data loss:  1790.8858642578125\n",
      "Epoch: 46. Training data loss:  1871.2027587890625\n",
      "Epoch: 47. Training data loss:  1923.1243896484375\n",
      "Epoch: 48. Training data loss:  1948.670654296875\n",
      "Epoch: 49. Training data loss:  2048.867919921875\n",
      "Epoch: 50. Training data loss:  2077.863525390625\n",
      "Epoch: 51. Training data loss:  1997.2724609375\n",
      "Epoch: 52. Training data loss:  2114.8095703125\n",
      "Epoch: 53. Training data loss:  2268.449951171875\n",
      "Epoch: 54. Training data loss:  2491.57958984375\n",
      "Epoch: 55. Training data loss:  2343.996337890625\n",
      "Epoch: 56. Training data loss:  2593.130859375\n",
      "Epoch: 57. Training data loss:  2862.116455078125\n",
      "Epoch: 58. Training data loss:  2821.047119140625\n",
      "Epoch: 59. Training data loss:  2739.8203125\n",
      "Accuracy found: 2.7\n",
      "60 0.0001 0.01 MultiMarginLoss() 32 1\n",
      "Starting iteration, 17\n",
      "Epoch: 0. Training data loss:  600.8084716796875\n",
      "Epoch: 1. Training data loss:  475.9705505371094\n",
      "Epoch: 2. Training data loss:  425.7313232421875\n",
      "Epoch: 3. Training data loss:  398.1533203125\n",
      "Epoch: 4. Training data loss:  380.4502868652344\n",
      "Epoch: 5. Training data loss:  366.3968200683594\n",
      "Epoch: 6. Training data loss:  355.8798828125\n",
      "Epoch: 7. Training data loss:  347.6922912597656\n",
      "Epoch: 8. Training data loss:  340.2616271972656\n",
      "Epoch: 9. Training data loss:  334.0035095214844\n",
      "Epoch: 10. Training data loss:  328.5327453613281\n",
      "Epoch: 11. Training data loss:  323.4537658691406\n",
      "Epoch: 12. Training data loss:  319.3908996582031\n",
      "Epoch: 13. Training data loss:  315.2151184082031\n",
      "Epoch: 14. Training data loss:  311.9365234375\n",
      "Epoch: 15. Training data loss:  307.91900634765625\n",
      "Epoch: 16. Training data loss:  304.89886474609375\n",
      "Epoch: 17. Training data loss:  302.1567687988281\n",
      "Epoch: 18. Training data loss:  299.34661865234375\n",
      "Epoch: 19. Training data loss:  296.97894287109375\n",
      "Epoch: 20. Training data loss:  295.3143310546875\n",
      "Epoch: 21. Training data loss:  292.51995849609375\n",
      "Epoch: 22. Training data loss:  290.4771728515625\n",
      "Epoch: 23. Training data loss:  288.3030090332031\n",
      "Epoch: 24. Training data loss:  286.361328125\n",
      "Epoch: 25. Training data loss:  284.4566955566406\n",
      "Epoch: 26. Training data loss:  282.49652099609375\n",
      "Epoch: 27. Training data loss:  281.1495056152344\n",
      "Epoch: 28. Training data loss:  279.4739990234375\n",
      "Epoch: 29. Training data loss:  277.564453125\n",
      "Epoch: 30. Training data loss:  275.94818115234375\n",
      "Epoch: 31. Training data loss:  274.5435485839844\n",
      "Epoch: 32. Training data loss:  273.3914489746094\n",
      "Epoch: 33. Training data loss:  271.58251953125\n",
      "Epoch: 34. Training data loss:  270.22735595703125\n",
      "Epoch: 35. Training data loss:  268.9472351074219\n",
      "Epoch: 36. Training data loss:  267.5664978027344\n",
      "Epoch: 37. Training data loss:  266.372314453125\n",
      "Epoch: 38. Training data loss:  264.6473693847656\n",
      "Epoch: 39. Training data loss:  263.77740478515625\n",
      "Accuracy found: 15.52\n",
      "40 0.01 0.001 MultiMarginLoss() 64 0\n",
      "Starting iteration, 18\n",
      "Epoch: 0. Training data loss:  1531.0941162109375\n",
      "Epoch: 1. Training data loss:  1511.5211181640625\n",
      "Epoch: 2. Training data loss:  1491.6180419921875\n",
      "Epoch: 3. Training data loss:  1473.8460693359375\n",
      "Epoch: 4. Training data loss:  1457.8060302734375\n",
      "Epoch: 5. Training data loss:  1441.884521484375\n",
      "Epoch: 6. Training data loss:  1425.8343505859375\n",
      "Epoch: 7. Training data loss:  1414.8221435546875\n",
      "Epoch: 8. Training data loss:  1400.67431640625\n",
      "Epoch: 9. Training data loss:  1386.9501953125\n",
      "Epoch: 10. Training data loss:  1373.7125244140625\n",
      "Epoch: 11. Training data loss:  1363.5819091796875\n",
      "Epoch: 12. Training data loss:  1352.1053466796875\n",
      "Epoch: 13. Training data loss:  1339.845703125\n",
      "Epoch: 14. Training data loss:  1331.1778564453125\n",
      "Epoch: 15. Training data loss:  1322.11328125\n",
      "Epoch: 16. Training data loss:  1312.265869140625\n",
      "Epoch: 17. Training data loss:  1305.6656494140625\n",
      "Epoch: 18. Training data loss:  1294.8004150390625\n",
      "Epoch: 19. Training data loss:  1289.177490234375\n",
      "Epoch: 20. Training data loss:  1282.8853759765625\n",
      "Epoch: 21. Training data loss:  1275.5843505859375\n",
      "Epoch: 22. Training data loss:  1267.426025390625\n",
      "Epoch: 23. Training data loss:  1261.7786865234375\n",
      "Epoch: 24. Training data loss:  1255.5821533203125\n",
      "Epoch: 25. Training data loss:  1248.7344970703125\n",
      "Epoch: 26. Training data loss:  1241.4339599609375\n",
      "Epoch: 27. Training data loss:  1238.2471923828125\n",
      "Epoch: 28. Training data loss:  1231.5511474609375\n",
      "Epoch: 29. Training data loss:  1225.89306640625\n",
      "Epoch: 30. Training data loss:  1223.2647705078125\n",
      "Epoch: 31. Training data loss:  1216.209716796875\n",
      "Epoch: 32. Training data loss:  1212.082275390625\n",
      "Epoch: 33. Training data loss:  1205.2369384765625\n",
      "Epoch: 34. Training data loss:  1199.31884765625\n",
      "Epoch: 35. Training data loss:  1194.7022705078125\n",
      "Epoch: 36. Training data loss:  1192.25732421875\n",
      "Epoch: 37. Training data loss:  1186.6417236328125\n",
      "Epoch: 38. Training data loss:  1181.209228515625\n",
      "Epoch: 39. Training data loss:  1179.0020751953125\n",
      "Accuracy found: 4.32\n",
      "40 0.0001 0 MultiMarginLoss() 32 0\n",
      "Starting iteration, 19\n",
      "Epoch: 0. Training data loss:  512.8358154296875\n",
      "Epoch: 1. Training data loss:  459.0810852050781\n",
      "Epoch: 2. Training data loss:  341.28314208984375\n",
      "Epoch: 3. Training data loss:  310.6554870605469\n",
      "Epoch: 4. Training data loss:  294.6228332519531\n",
      "Epoch: 5. Training data loss:  316.6814270019531\n",
      "Epoch: 6. Training data loss:  285.6199951171875\n",
      "Epoch: 7. Training data loss:  305.1065673828125\n",
      "Epoch: 8. Training data loss:  373.9665222167969\n",
      "Epoch: 9. Training data loss:  364.1554260253906\n",
      "Epoch: 10. Training data loss:  304.99407958984375\n",
      "Epoch: 11. Training data loss:  328.7180480957031\n",
      "Epoch: 12. Training data loss:  355.00433349609375\n",
      "Epoch: 13. Training data loss:  277.1087646484375\n",
      "Epoch: 14. Training data loss:  256.5871276855469\n",
      "Epoch: 15. Training data loss:  409.15679931640625\n",
      "Epoch: 16. Training data loss:  310.7887268066406\n",
      "Epoch: 17. Training data loss:  290.1434020996094\n",
      "Epoch: 18. Training data loss:  306.5382995605469\n",
      "Epoch: 19. Training data loss:  288.46246337890625\n",
      "Epoch: 20. Training data loss:  255.19271850585938\n",
      "Epoch: 21. Training data loss:  243.99032592773438\n",
      "Epoch: 22. Training data loss:  278.2685546875\n",
      "Epoch: 23. Training data loss:  254.330322265625\n",
      "Epoch: 24. Training data loss:  242.909912109375\n",
      "Epoch: 25. Training data loss:  238.39259338378906\n",
      "Epoch: 26. Training data loss:  269.6123046875\n",
      "Epoch: 27. Training data loss:  274.505615234375\n",
      "Epoch: 28. Training data loss:  250.72952270507812\n",
      "Epoch: 29. Training data loss:  240.5162353515625\n",
      "Epoch: 30. Training data loss:  234.8737030029297\n",
      "Epoch: 31. Training data loss:  227.8525390625\n",
      "Epoch: 32. Training data loss:  225.67433166503906\n",
      "Epoch: 33. Training data loss:  225.6722412109375\n",
      "Epoch: 34. Training data loss:  223.4768829345703\n",
      "Epoch: 35. Training data loss:  325.7047119140625\n",
      "Epoch: 36. Training data loss:  253.8746795654297\n",
      "Epoch: 37. Training data loss:  231.6956024169922\n",
      "Epoch: 38. Training data loss:  222.4182586669922\n",
      "Epoch: 39. Training data loss:  216.87966918945312\n",
      "Epoch: 40. Training data loss:  216.15260314941406\n",
      "Epoch: 41. Training data loss:  216.2016143798828\n",
      "Epoch: 42. Training data loss:  215.97625732421875\n",
      "Epoch: 43. Training data loss:  213.7449493408203\n",
      "Epoch: 44. Training data loss:  331.1271057128906\n",
      "Epoch: 45. Training data loss:  255.31439208984375\n",
      "Epoch: 46. Training data loss:  234.43309020996094\n",
      "Epoch: 47. Training data loss:  223.3748779296875\n",
      "Epoch: 48. Training data loss:  268.23944091796875\n",
      "Epoch: 49. Training data loss:  222.7469024658203\n",
      "Epoch: 50. Training data loss:  211.56231689453125\n",
      "Epoch: 51. Training data loss:  281.7978210449219\n",
      "Epoch: 52. Training data loss:  282.1673889160156\n",
      "Epoch: 53. Training data loss:  248.40122985839844\n",
      "Epoch: 54. Training data loss:  264.7856750488281\n",
      "Epoch: 55. Training data loss:  309.7957763671875\n",
      "Epoch: 56. Training data loss:  256.0974426269531\n",
      "Epoch: 57. Training data loss:  236.5839385986328\n",
      "Epoch: 58. Training data loss:  223.7404327392578\n",
      "Epoch: 59. Training data loss:  519.44775390625\n",
      "Accuracy found: 12.85\n",
      "60 0.001 0 MultiMarginLoss() 64 1\n",
      "Starting iteration, 20\n",
      "Epoch: 0. Training data loss:  4455.62060546875\n",
      "Epoch: 1. Training data loss:  4050.507568359375\n",
      "Epoch: 2. Training data loss:  3938.65283203125\n",
      "Epoch: 3. Training data loss:  3915.3701171875\n",
      "Epoch: 4. Training data loss:  3917.765380859375\n",
      "Epoch: 5. Training data loss:  3937.255615234375\n",
      "Epoch: 6. Training data loss:  3957.29248046875\n",
      "Epoch: 7. Training data loss:  3985.501708984375\n",
      "Epoch: 8. Training data loss:  4009.084228515625\n",
      "Epoch: 9. Training data loss:  4034.456298828125\n",
      "Epoch: 10. Training data loss:  4054.48876953125\n",
      "Epoch: 11. Training data loss:  4068.134033203125\n",
      "Epoch: 12. Training data loss:  4080.29052734375\n",
      "Epoch: 13. Training data loss:  4108.421875\n",
      "Epoch: 14. Training data loss:  4110.724609375\n",
      "Epoch: 15. Training data loss:  4124.71240234375\n",
      "Epoch: 16. Training data loss:  4150.0166015625\n",
      "Epoch: 17. Training data loss:  4172.34228515625\n",
      "Epoch: 18. Training data loss:  4191.228515625\n",
      "Epoch: 19. Training data loss:  4209.43798828125\n",
      "Epoch: 20. Training data loss:  4222.64111328125\n",
      "Epoch: 21. Training data loss:  4228.90625\n",
      "Epoch: 22. Training data loss:  4222.13037109375\n",
      "Epoch: 23. Training data loss:  4235.32958984375\n",
      "Epoch: 24. Training data loss:  4243.662109375\n",
      "Epoch: 25. Training data loss:  4256.26416015625\n",
      "Epoch: 26. Training data loss:  4273.3251953125\n",
      "Epoch: 27. Training data loss:  4286.2890625\n",
      "Epoch: 28. Training data loss:  4291.7802734375\n",
      "Epoch: 29. Training data loss:  4299.87353515625\n",
      "Epoch: 30. Training data loss:  4306.00341796875\n",
      "Epoch: 31. Training data loss:  4298.9541015625\n",
      "Epoch: 32. Training data loss:  4294.900390625\n",
      "Epoch: 33. Training data loss:  4297.9482421875\n",
      "Epoch: 34. Training data loss:  4312.3662109375\n",
      "Epoch: 35. Training data loss:  4323.8037109375\n",
      "Epoch: 36. Training data loss:  4330.90380859375\n",
      "Epoch: 37. Training data loss:  4338.24365234375\n",
      "Epoch: 38. Training data loss:  4347.974609375\n",
      "Epoch: 39. Training data loss:  4352.13037109375\n",
      "Epoch: 40. Training data loss:  4339.9599609375\n",
      "Epoch: 41. Training data loss:  4337.25\n",
      "Epoch: 42. Training data loss:  4349.177734375\n",
      "Epoch: 43. Training data loss:  4352.44921875\n",
      "Epoch: 44. Training data loss:  4359.8076171875\n",
      "Epoch: 45. Training data loss:  4362.6962890625\n",
      "Epoch: 46. Training data loss:  4368.13134765625\n",
      "Epoch: 47. Training data loss:  4371.98828125\n",
      "Epoch: 48. Training data loss:  4373.052734375\n",
      "Epoch: 49. Training data loss:  4378.68359375\n",
      "Epoch: 50. Training data loss:  4381.54638671875\n",
      "Epoch: 51. Training data loss:  4385.29931640625\n",
      "Epoch: 52. Training data loss:  4391.45263671875\n",
      "Epoch: 53. Training data loss:  4392.7001953125\n",
      "Epoch: 54. Training data loss:  4396.67919921875\n",
      "Epoch: 55. Training data loss:  4389.544921875\n",
      "Epoch: 56. Training data loss:  4390.20361328125\n",
      "Epoch: 57. Training data loss:  4393.2919921875\n",
      "Epoch: 58. Training data loss:  4398.0732421875\n",
      "Epoch: 59. Training data loss:  4403.861328125\n",
      "Accuracy found: 3.09\n",
      "60 0.001 0.3 MultiMarginLoss() 10 0\n",
      "Starting iteration, 21\n",
      "Epoch: 0. Training data loss:  3134.003173828125\n",
      "Epoch: 1. Training data loss:  2903.957275390625\n",
      "Epoch: 2. Training data loss:  2815.079833984375\n",
      "Epoch: 3. Training data loss:  2755.01220703125\n",
      "Epoch: 4. Training data loss:  2707.635498046875\n",
      "Epoch: 5. Training data loss:  2667.637451171875\n",
      "Epoch: 6. Training data loss:  2635.61474609375\n",
      "Epoch: 7. Training data loss:  2607.314453125\n",
      "Epoch: 8. Training data loss:  2579.287353515625\n",
      "Epoch: 9. Training data loss:  2556.580322265625\n",
      "Epoch: 10. Training data loss:  2534.44970703125\n",
      "Epoch: 11. Training data loss:  2517.781982421875\n",
      "Epoch: 12. Training data loss:  2499.2587890625\n",
      "Epoch: 13. Training data loss:  2481.9755859375\n",
      "Epoch: 14. Training data loss:  2471.1982421875\n",
      "Epoch: 15. Training data loss:  2454.10107421875\n",
      "Epoch: 16. Training data loss:  2440.508056640625\n",
      "Epoch: 17. Training data loss:  2427.94482421875\n",
      "Epoch: 18. Training data loss:  2420.608642578125\n",
      "Epoch: 19. Training data loss:  2408.0791015625\n",
      "Epoch: 20. Training data loss:  2399.041259765625\n",
      "Epoch: 21. Training data loss:  2389.0126953125\n",
      "Epoch: 22. Training data loss:  2380.935302734375\n",
      "Epoch: 23. Training data loss:  2368.413330078125\n",
      "Epoch: 24. Training data loss:  2360.601806640625\n",
      "Epoch: 25. Training data loss:  2351.79248046875\n",
      "Epoch: 26. Training data loss:  2346.59521484375\n",
      "Epoch: 27. Training data loss:  2340.856689453125\n",
      "Epoch: 28. Training data loss:  2330.799072265625\n",
      "Epoch: 29. Training data loss:  2329.333984375\n",
      "Epoch: 30. Training data loss:  2318.1826171875\n",
      "Epoch: 31. Training data loss:  2314.351806640625\n",
      "Epoch: 32. Training data loss:  2302.69580078125\n",
      "Epoch: 33. Training data loss:  2300.834228515625\n",
      "Epoch: 34. Training data loss:  2294.673095703125\n",
      "Epoch: 35. Training data loss:  2286.736083984375\n",
      "Epoch: 36. Training data loss:  2286.535888671875\n",
      "Epoch: 37. Training data loss:  2276.630859375\n",
      "Epoch: 38. Training data loss:  2273.338623046875\n",
      "Epoch: 39. Training data loss:  2268.556884765625\n",
      "Epoch: 40. Training data loss:  2264.652099609375\n",
      "Epoch: 41. Training data loss:  2258.306884765625\n",
      "Epoch: 42. Training data loss:  2255.045166015625\n",
      "Epoch: 43. Training data loss:  2251.212158203125\n",
      "Epoch: 44. Training data loss:  2244.823974609375\n",
      "Epoch: 45. Training data loss:  2244.008544921875\n",
      "Epoch: 46. Training data loss:  2238.025146484375\n",
      "Epoch: 47. Training data loss:  2231.182861328125\n",
      "Epoch: 48. Training data loss:  2231.037841796875\n",
      "Epoch: 49. Training data loss:  2224.206298828125\n",
      "Epoch: 50. Training data loss:  2220.92919921875\n",
      "Epoch: 51. Training data loss:  2219.378662109375\n",
      "Epoch: 52. Training data loss:  2210.163330078125\n",
      "Epoch: 53. Training data loss:  2208.0322265625\n",
      "Epoch: 54. Training data loss:  2205.17431640625\n",
      "Epoch: 55. Training data loss:  2203.35009765625\n",
      "Epoch: 56. Training data loss:  2200.80908203125\n",
      "Epoch: 57. Training data loss:  2195.0869140625\n",
      "Epoch: 58. Training data loss:  2196.58740234375\n",
      "Epoch: 59. Training data loss:  2190.926513671875\n",
      "Accuracy found: 22.55\n",
      "60 0.1 0 CrossEntropyLoss() 64 0\n",
      "Starting iteration, 22\n",
      "Epoch: 0. Training data loss:  22648.53125\n",
      "Epoch: 1. Training data loss:  22004.1796875\n",
      "Epoch: 2. Training data loss:  21536.748046875\n",
      "Epoch: 3. Training data loss:  21152.93359375\n",
      "Epoch: 4. Training data loss:  20832.927734375\n",
      "Epoch: 5. Training data loss:  20569.837890625\n",
      "Epoch: 6. Training data loss:  20351.50390625\n",
      "Epoch: 7. Training data loss:  20160.908203125\n",
      "Epoch: 8. Training data loss:  20001.54296875\n",
      "Epoch: 9. Training data loss:  19865.052734375\n",
      "Epoch: 10. Training data loss:  19754.958984375\n",
      "Epoch: 11. Training data loss:  19647.7734375\n",
      "Epoch: 12. Training data loss:  19551.69140625\n",
      "Epoch: 13. Training data loss:  19462.849609375\n",
      "Epoch: 14. Training data loss:  19382.15625\n",
      "Epoch: 15. Training data loss:  19301.9140625\n",
      "Epoch: 16. Training data loss:  19228.59375\n",
      "Epoch: 17. Training data loss:  19166.513671875\n",
      "Epoch: 18. Training data loss:  19102.162109375\n",
      "Epoch: 19. Training data loss:  19045.935546875\n",
      "Epoch: 20. Training data loss:  18986.955078125\n",
      "Epoch: 21. Training data loss:  18938.076171875\n",
      "Epoch: 22. Training data loss:  18889.529296875\n",
      "Epoch: 23. Training data loss:  18844.451171875\n",
      "Epoch: 24. Training data loss:  18804.595703125\n",
      "Epoch: 25. Training data loss:  18754.40625\n",
      "Epoch: 26. Training data loss:  18723.12109375\n",
      "Epoch: 27. Training data loss:  18682.001953125\n",
      "Epoch: 28. Training data loss:  18647.294921875\n",
      "Epoch: 29. Training data loss:  18605.4375\n",
      "Epoch: 30. Training data loss:  18570.765625\n",
      "Epoch: 31. Training data loss:  18539.873046875\n",
      "Epoch: 32. Training data loss:  18512.93359375\n",
      "Epoch: 33. Training data loss:  18477.365234375\n",
      "Epoch: 34. Training data loss:  18440.943359375\n",
      "Epoch: 35. Training data loss:  18414.59375\n",
      "Epoch: 36. Training data loss:  18387.92578125\n",
      "Epoch: 37. Training data loss:  18367.2265625\n",
      "Epoch: 38. Training data loss:  18334.90234375\n",
      "Epoch: 39. Training data loss:  18311.66015625\n",
      "Epoch: 40. Training data loss:  18290.755859375\n",
      "Epoch: 41. Training data loss:  18264.109375\n",
      "Epoch: 42. Training data loss:  18243.822265625\n",
      "Epoch: 43. Training data loss:  18215.90234375\n",
      "Epoch: 44. Training data loss:  18198.388671875\n",
      "Epoch: 45. Training data loss:  18173.212890625\n",
      "Epoch: 46. Training data loss:  18145.787109375\n",
      "Epoch: 47. Training data loss:  18134.205078125\n",
      "Epoch: 48. Training data loss:  18111.0703125\n",
      "Epoch: 49. Training data loss:  18100.236328125\n",
      "Epoch: 50. Training data loss:  18071.599609375\n",
      "Epoch: 51. Training data loss:  18053.404296875\n",
      "Epoch: 52. Training data loss:  18035.029296875\n",
      "Epoch: 53. Training data loss:  18017.59765625\n",
      "Epoch: 54. Training data loss:  18002.580078125\n",
      "Epoch: 55. Training data loss:  17979.96484375\n",
      "Epoch: 56. Training data loss:  17960.513671875\n",
      "Epoch: 57. Training data loss:  17946.341796875\n",
      "Epoch: 58. Training data loss:  17932.6796875\n",
      "Epoch: 59. Training data loss:  17923.251953125\n",
      "Accuracy found: 16.73\n",
      "60 0.001 0.001 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 23\n",
      "Epoch: 0. Training data loss:  630449.5\n",
      "Epoch: 1. Training data loss:  2310818.0\n",
      "Epoch: 2. Training data loss:  3294507.75\n",
      "Epoch: 3. Training data loss:  3190206.75\n",
      "Epoch: 4. Training data loss:  3872133.25\n",
      "Epoch: 5. Training data loss:  3823028.0\n",
      "Epoch: 6. Training data loss:  3451481.5\n",
      "Epoch: 7. Training data loss:  4552451.0\n",
      "Epoch: 8. Training data loss:  4073465.75\n",
      "Epoch: 9. Training data loss:  4613880.5\n",
      "Epoch: 10. Training data loss:  4547786.5\n",
      "Epoch: 11. Training data loss:  5216734.5\n",
      "Epoch: 12. Training data loss:  5868753.5\n",
      "Epoch: 13. Training data loss:  7118382.5\n",
      "Epoch: 14. Training data loss:  8273773.0\n",
      "Epoch: 15. Training data loss:  8799178.0\n",
      "Epoch: 16. Training data loss:  7805117.5\n",
      "Epoch: 17. Training data loss:  7439941.5\n",
      "Epoch: 18. Training data loss:  8555497.0\n",
      "Epoch: 19. Training data loss:  8204337.0\n",
      "Epoch: 20. Training data loss:  11896258.0\n",
      "Epoch: 21. Training data loss:  11430214.0\n",
      "Epoch: 22. Training data loss:  8747018.0\n",
      "Epoch: 23. Training data loss:  8515223.0\n",
      "Epoch: 24. Training data loss:  8770114.0\n",
      "Epoch: 25. Training data loss:  9667445.0\n",
      "Epoch: 26. Training data loss:  10191258.0\n",
      "Epoch: 27. Training data loss:  10488071.0\n",
      "Epoch: 28. Training data loss:  12004724.0\n",
      "Epoch: 29. Training data loss:  11270978.0\n",
      "Epoch: 30. Training data loss:  9771498.0\n",
      "Epoch: 31. Training data loss:  10612064.0\n",
      "Epoch: 32. Training data loss:  11840357.0\n",
      "Epoch: 33. Training data loss:  10715148.0\n",
      "Epoch: 34. Training data loss:  9585711.0\n",
      "Epoch: 35. Training data loss:  9879369.0\n",
      "Epoch: 36. Training data loss:  10506050.0\n",
      "Epoch: 37. Training data loss:  10857440.0\n",
      "Epoch: 38. Training data loss:  12108907.0\n",
      "Epoch: 39. Training data loss:  13027562.0\n",
      "Epoch: 40. Training data loss:  14206721.0\n",
      "Epoch: 41. Training data loss:  16780926.0\n",
      "Epoch: 42. Training data loss:  18771498.0\n",
      "Epoch: 43. Training data loss:  21082404.0\n",
      "Epoch: 44. Training data loss:  21754528.0\n",
      "Epoch: 45. Training data loss:  21446686.0\n",
      "Epoch: 46. Training data loss:  18402960.0\n",
      "Epoch: 47. Training data loss:  19996354.0\n",
      "Epoch: 48. Training data loss:  19661450.0\n",
      "Epoch: 49. Training data loss:  20458276.0\n",
      "Epoch: 50. Training data loss:  22419890.0\n",
      "Epoch: 51. Training data loss:  24885030.0\n",
      "Epoch: 52. Training data loss:  29242448.0\n",
      "Epoch: 53. Training data loss:  26317246.0\n",
      "Epoch: 54. Training data loss:  23586626.0\n",
      "Epoch: 55. Training data loss:  22190756.0\n",
      "Epoch: 56. Training data loss:  21842062.0\n",
      "Epoch: 57. Training data loss:  24839730.0\n",
      "Epoch: 58. Training data loss:  20458594.0\n",
      "Epoch: 59. Training data loss:  17099658.0\n",
      "Accuracy found: 1.37\n",
      "60 0.01 0.01 MultiMarginLoss() 32 1\n",
      "Starting iteration, 24\n",
      "Epoch: 0. Training data loss:  19900.67578125\n",
      "Epoch: 1. Training data loss:  18424.001953125\n",
      "Epoch: 2. Training data loss:  17851.982421875\n",
      "Epoch: 3. Training data loss:  17454.494140625\n",
      "Epoch: 4. Training data loss:  17138.12109375\n",
      "Epoch: 5. Training data loss:  16877.49609375\n",
      "Epoch: 6. Training data loss:  16661.197265625\n",
      "Epoch: 7. Training data loss:  16462.951171875\n",
      "Epoch: 8. Training data loss:  16278.6396484375\n",
      "Epoch: 9. Training data loss:  16142.5703125\n",
      "Epoch: 10. Training data loss:  15995.6064453125\n",
      "Epoch: 11. Training data loss:  15865.5390625\n",
      "Epoch: 12. Training data loss:  15751.1337890625\n",
      "Epoch: 13. Training data loss:  15645.2177734375\n",
      "Epoch: 14. Training data loss:  15547.0302734375\n",
      "Epoch: 15. Training data loss:  15441.666015625\n",
      "Epoch: 16. Training data loss:  15359.2666015625\n",
      "Epoch: 17. Training data loss:  15269.951171875\n",
      "Epoch: 18. Training data loss:  15186.9443359375\n",
      "Epoch: 19. Training data loss:  15110.4462890625\n",
      "Epoch: 20. Training data loss:  15036.603515625\n",
      "Epoch: 21. Training data loss:  14963.529296875\n",
      "Epoch: 22. Training data loss:  14891.240234375\n",
      "Epoch: 23. Training data loss:  14845.7587890625\n",
      "Epoch: 24. Training data loss:  14782.068359375\n",
      "Epoch: 25. Training data loss:  14715.2158203125\n",
      "Epoch: 26. Training data loss:  14658.7412109375\n",
      "Epoch: 27. Training data loss:  14624.625\n",
      "Epoch: 28. Training data loss:  14546.8408203125\n",
      "Epoch: 29. Training data loss:  14499.7578125\n",
      "Epoch: 30. Training data loss:  14469.5859375\n",
      "Epoch: 31. Training data loss:  14419.63671875\n",
      "Epoch: 32. Training data loss:  14355.2646484375\n",
      "Epoch: 33. Training data loss:  14324.3994140625\n",
      "Epoch: 34. Training data loss:  14281.67578125\n",
      "Epoch: 35. Training data loss:  14247.814453125\n",
      "Epoch: 36. Training data loss:  14206.833984375\n",
      "Epoch: 37. Training data loss:  14175.2548828125\n",
      "Epoch: 38. Training data loss:  14133.94921875\n",
      "Epoch: 39. Training data loss:  14081.8505859375\n",
      "Accuracy found: 24.44\n",
      "40 0.0001 0 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 25\n",
      "Epoch: 0. Training data loss:  776.5142211914062\n",
      "Epoch: 1. Training data loss:  766.746826171875\n",
      "Epoch: 2. Training data loss:  757.0490112304688\n",
      "Epoch: 3. Training data loss:  747.2594604492188\n",
      "Epoch: 4. Training data loss:  739.87939453125\n",
      "Epoch: 5. Training data loss:  730.6150512695312\n",
      "Epoch: 6. Training data loss:  722.4780883789062\n",
      "Epoch: 7. Training data loss:  714.795654296875\n",
      "Epoch: 8. Training data loss:  708.484130859375\n",
      "Epoch: 9. Training data loss:  701.3966064453125\n",
      "Epoch: 10. Training data loss:  695.3264770507812\n",
      "Epoch: 11. Training data loss:  689.0442504882812\n",
      "Epoch: 12. Training data loss:  684.0419921875\n",
      "Epoch: 13. Training data loss:  677.8946533203125\n",
      "Epoch: 14. Training data loss:  673.3878784179688\n",
      "Epoch: 15. Training data loss:  667.2391967773438\n",
      "Epoch: 16. Training data loss:  662.6822509765625\n",
      "Epoch: 17. Training data loss:  658.4051513671875\n",
      "Epoch: 18. Training data loss:  654.6889038085938\n",
      "Epoch: 19. Training data loss:  650.779541015625\n",
      "Accuracy found: 3.14\n",
      "20 0.0001 0.01 MultiMarginLoss() 64 0\n",
      "Starting iteration, 26\n",
      "Epoch: 0. Training data loss:  357.232421875\n",
      "Epoch: 1. Training data loss:  356.1454772949219\n",
      "Epoch: 2. Training data loss:  359.922607421875\n",
      "Epoch: 3. Training data loss:  357.7637634277344\n",
      "Epoch: 4. Training data loss:  361.9674072265625\n",
      "Epoch: 5. Training data loss:  357.1377258300781\n",
      "Epoch: 6. Training data loss:  357.4118957519531\n",
      "Epoch: 7. Training data loss:  359.48272705078125\n",
      "Epoch: 8. Training data loss:  363.1373291015625\n",
      "Epoch: 9. Training data loss:  354.6630859375\n",
      "Epoch: 10. Training data loss:  354.2088317871094\n",
      "Epoch: 11. Training data loss:  354.48883056640625\n",
      "Epoch: 12. Training data loss:  358.13262939453125\n",
      "Epoch: 13. Training data loss:  356.91790771484375\n",
      "Epoch: 14. Training data loss:  358.13134765625\n",
      "Epoch: 15. Training data loss:  360.4113464355469\n",
      "Epoch: 16. Training data loss:  356.2959899902344\n",
      "Epoch: 17. Training data loss:  360.615234375\n",
      "Epoch: 18. Training data loss:  358.5710754394531\n",
      "Epoch: 19. Training data loss:  353.6955261230469\n",
      "Epoch: 20. Training data loss:  363.2550964355469\n",
      "Epoch: 21. Training data loss:  357.7914733886719\n",
      "Epoch: 22. Training data loss:  358.2606506347656\n",
      "Epoch: 23. Training data loss:  363.0633850097656\n",
      "Epoch: 24. Training data loss:  369.46929931640625\n",
      "Epoch: 25. Training data loss:  359.7325439453125\n",
      "Epoch: 26. Training data loss:  355.3925476074219\n",
      "Epoch: 27. Training data loss:  358.3221740722656\n",
      "Epoch: 28. Training data loss:  356.69903564453125\n",
      "Epoch: 29. Training data loss:  362.26470947265625\n",
      "Epoch: 30. Training data loss:  357.5339660644531\n",
      "Epoch: 31. Training data loss:  362.0791320800781\n",
      "Epoch: 32. Training data loss:  359.7418518066406\n",
      "Epoch: 33. Training data loss:  359.1187744140625\n",
      "Epoch: 34. Training data loss:  357.361083984375\n",
      "Epoch: 35. Training data loss:  364.57427978515625\n",
      "Epoch: 36. Training data loss:  360.77667236328125\n",
      "Epoch: 37. Training data loss:  355.9833679199219\n",
      "Epoch: 38. Training data loss:  354.929931640625\n",
      "Epoch: 39. Training data loss:  361.7149353027344\n",
      "Epoch: 40. Training data loss:  358.75567626953125\n",
      "Epoch: 41. Training data loss:  380.9568786621094\n",
      "Epoch: 42. Training data loss:  361.1619567871094\n",
      "Epoch: 43. Training data loss:  357.49676513671875\n",
      "Epoch: 44. Training data loss:  361.2965393066406\n",
      "Epoch: 45. Training data loss:  357.357177734375\n",
      "Epoch: 46. Training data loss:  356.1370849609375\n",
      "Epoch: 47. Training data loss:  358.9532165527344\n",
      "Epoch: 48. Training data loss:  359.8408203125\n",
      "Epoch: 49. Training data loss:  359.68341064453125\n",
      "Epoch: 50. Training data loss:  358.8945007324219\n",
      "Epoch: 51. Training data loss:  354.2552490234375\n",
      "Epoch: 52. Training data loss:  354.9776916503906\n",
      "Epoch: 53. Training data loss:  360.2354736328125\n",
      "Epoch: 54. Training data loss:  358.4055480957031\n",
      "Epoch: 55. Training data loss:  361.5740966796875\n",
      "Epoch: 56. Training data loss:  360.18927001953125\n",
      "Epoch: 57. Training data loss:  358.0076904296875\n",
      "Epoch: 58. Training data loss:  357.72894287109375\n",
      "Epoch: 59. Training data loss:  355.2575988769531\n",
      "Accuracy found: 2.04\n",
      "60 0.01 0.3 MultiMarginLoss() 128 1\n",
      "Starting iteration, 27\n",
      "Epoch: 0. Training data loss:  2143.9873046875\n",
      "Epoch: 1. Training data loss:  2314.406494140625\n",
      "Epoch: 2. Training data loss:  2173.49755859375\n",
      "Epoch: 3. Training data loss:  1941.2762451171875\n",
      "Epoch: 4. Training data loss:  2512.566162109375\n",
      "Epoch: 5. Training data loss:  2647.1533203125\n",
      "Epoch: 6. Training data loss:  1777.0723876953125\n",
      "Epoch: 7. Training data loss:  1943.662109375\n",
      "Epoch: 8. Training data loss:  2379.824462890625\n",
      "Epoch: 9. Training data loss:  2280.22314453125\n",
      "Epoch: 10. Training data loss:  1951.407958984375\n",
      "Epoch: 11. Training data loss:  2219.759765625\n",
      "Epoch: 12. Training data loss:  2766.495361328125\n",
      "Epoch: 13. Training data loss:  1994.916015625\n",
      "Epoch: 14. Training data loss:  1856.6695556640625\n",
      "Epoch: 15. Training data loss:  3968.71875\n",
      "Epoch: 16. Training data loss:  2291.507080078125\n",
      "Epoch: 17. Training data loss:  1683.5418701171875\n",
      "Epoch: 18. Training data loss:  1787.2850341796875\n",
      "Epoch: 19. Training data loss:  1955.0252685546875\n",
      "Epoch: 20. Training data loss:  2930.99658203125\n",
      "Epoch: 21. Training data loss:  2690.0\n",
      "Epoch: 22. Training data loss:  1744.7786865234375\n",
      "Epoch: 23. Training data loss:  1819.4825439453125\n",
      "Epoch: 24. Training data loss:  2186.39599609375\n",
      "Epoch: 25. Training data loss:  4220.3935546875\n",
      "Epoch: 26. Training data loss:  1781.6436767578125\n",
      "Epoch: 27. Training data loss:  1719.838134765625\n",
      "Epoch: 28. Training data loss:  1838.484619140625\n",
      "Epoch: 29. Training data loss:  2282.685791015625\n",
      "Epoch: 30. Training data loss:  3362.0859375\n",
      "Epoch: 31. Training data loss:  1762.8048095703125\n",
      "Epoch: 32. Training data loss:  1787.384765625\n",
      "Epoch: 33. Training data loss:  2057.5830078125\n",
      "Epoch: 34. Training data loss:  2374.787109375\n",
      "Epoch: 35. Training data loss:  2015.3341064453125\n",
      "Epoch: 36. Training data loss:  2250.2490234375\n",
      "Epoch: 37. Training data loss:  2164.1689453125\n",
      "Epoch: 38. Training data loss:  2042.2464599609375\n",
      "Epoch: 39. Training data loss:  2321.71240234375\n",
      "Accuracy found: 4.49\n",
      "40 0.01 0.01 CrossEntropyLoss() 128 1\n",
      "Starting iteration, 28\n",
      "Epoch: 0. Training data loss:  1764.6728515625\n",
      "Epoch: 1. Training data loss:  1725.904052734375\n",
      "Epoch: 2. Training data loss:  1712.7705078125\n",
      "Epoch: 3. Training data loss:  1708.25927734375\n",
      "Epoch: 4. Training data loss:  1707.249755859375\n",
      "Epoch: 5. Training data loss:  1707.3341064453125\n",
      "Epoch: 6. Training data loss:  1707.9306640625\n",
      "Epoch: 7. Training data loss:  1708.1612548828125\n",
      "Epoch: 8. Training data loss:  1707.7926025390625\n",
      "Epoch: 9. Training data loss:  1707.9622802734375\n",
      "Epoch: 10. Training data loss:  1707.6788330078125\n",
      "Epoch: 11. Training data loss:  1707.543701171875\n",
      "Epoch: 12. Training data loss:  1707.0572509765625\n",
      "Epoch: 13. Training data loss:  1707.262939453125\n",
      "Epoch: 14. Training data loss:  1706.7869873046875\n",
      "Epoch: 15. Training data loss:  1706.5771484375\n",
      "Epoch: 16. Training data loss:  1706.4857177734375\n",
      "Epoch: 17. Training data loss:  1706.508544921875\n",
      "Epoch: 18. Training data loss:  1706.0382080078125\n",
      "Epoch: 19. Training data loss:  1706.3590087890625\n",
      "Epoch: 20. Training data loss:  1706.010986328125\n",
      "Epoch: 21. Training data loss:  1705.7005615234375\n",
      "Epoch: 22. Training data loss:  1706.093505859375\n",
      "Epoch: 23. Training data loss:  1705.681640625\n",
      "Epoch: 24. Training data loss:  1706.135009765625\n",
      "Epoch: 25. Training data loss:  1705.44482421875\n",
      "Epoch: 26. Training data loss:  1705.98876953125\n",
      "Epoch: 27. Training data loss:  1706.0736083984375\n",
      "Epoch: 28. Training data loss:  1705.6942138671875\n",
      "Epoch: 29. Training data loss:  1705.7735595703125\n",
      "Epoch: 30. Training data loss:  1705.8609619140625\n",
      "Epoch: 31. Training data loss:  1705.5584716796875\n",
      "Epoch: 32. Training data loss:  1705.7999267578125\n",
      "Epoch: 33. Training data loss:  1705.7529296875\n",
      "Epoch: 34. Training data loss:  1705.6632080078125\n",
      "Epoch: 35. Training data loss:  1705.8564453125\n",
      "Epoch: 36. Training data loss:  1705.9774169921875\n",
      "Epoch: 37. Training data loss:  1705.41259765625\n",
      "Epoch: 38. Training data loss:  1705.894287109375\n",
      "Epoch: 39. Training data loss:  1706.0147705078125\n",
      "Epoch: 40. Training data loss:  1705.552978515625\n",
      "Epoch: 41. Training data loss:  1706.019287109375\n",
      "Epoch: 42. Training data loss:  1705.7479248046875\n",
      "Epoch: 43. Training data loss:  1705.8424072265625\n",
      "Epoch: 44. Training data loss:  1705.833740234375\n",
      "Epoch: 45. Training data loss:  1706.080078125\n",
      "Epoch: 46. Training data loss:  1705.99755859375\n",
      "Epoch: 47. Training data loss:  1705.706787109375\n",
      "Epoch: 48. Training data loss:  1705.9246826171875\n",
      "Epoch: 49. Training data loss:  1706.1531982421875\n",
      "Epoch: 50. Training data loss:  1705.8992919921875\n",
      "Epoch: 51. Training data loss:  1706.049560546875\n",
      "Epoch: 52. Training data loss:  1705.9031982421875\n",
      "Epoch: 53. Training data loss:  1705.8814697265625\n",
      "Epoch: 54. Training data loss:  1705.9639892578125\n",
      "Epoch: 55. Training data loss:  1705.9425048828125\n",
      "Epoch: 56. Training data loss:  1705.837890625\n",
      "Epoch: 57. Training data loss:  1706.1185302734375\n",
      "Epoch: 58. Training data loss:  1706.0289306640625\n",
      "Epoch: 59. Training data loss:  1705.93212890625\n",
      "Accuracy found: 4.54\n",
      "60 0.01 0.3 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 29\n",
      "Epoch: 0. Training data loss:  930.2770385742188\n",
      "Epoch: 1. Training data loss:  778.4797973632812\n",
      "Epoch: 2. Training data loss:  779.3816528320312\n",
      "Epoch: 3. Training data loss:  795.5673217773438\n",
      "Epoch: 4. Training data loss:  800.4420166015625\n",
      "Epoch: 5. Training data loss:  777.77978515625\n",
      "Epoch: 6. Training data loss:  820.150146484375\n",
      "Epoch: 7. Training data loss:  816.8468017578125\n",
      "Epoch: 8. Training data loss:  821.3991088867188\n",
      "Epoch: 9. Training data loss:  778.871826171875\n",
      "Epoch: 10. Training data loss:  783.6560668945312\n",
      "Epoch: 11. Training data loss:  831.0774536132812\n",
      "Epoch: 12. Training data loss:  814.5985107421875\n",
      "Epoch: 13. Training data loss:  756.3956909179688\n",
      "Epoch: 14. Training data loss:  774.3429565429688\n",
      "Epoch: 15. Training data loss:  768.7218627929688\n",
      "Epoch: 16. Training data loss:  799.5972900390625\n",
      "Epoch: 17. Training data loss:  791.93798828125\n",
      "Epoch: 18. Training data loss:  792.0076293945312\n",
      "Epoch: 19. Training data loss:  828.9240112304688\n",
      "Epoch: 20. Training data loss:  841.5576171875\n",
      "Epoch: 21. Training data loss:  793.1492309570312\n",
      "Epoch: 22. Training data loss:  794.3666381835938\n",
      "Epoch: 23. Training data loss:  787.8660278320312\n",
      "Epoch: 24. Training data loss:  799.9580078125\n",
      "Epoch: 25. Training data loss:  811.0822143554688\n",
      "Epoch: 26. Training data loss:  833.6275634765625\n",
      "Epoch: 27. Training data loss:  780.709716796875\n",
      "Epoch: 28. Training data loss:  826.3404541015625\n",
      "Epoch: 29. Training data loss:  765.7025146484375\n",
      "Epoch: 30. Training data loss:  784.6743774414062\n",
      "Epoch: 31. Training data loss:  821.4835205078125\n",
      "Epoch: 32. Training data loss:  774.6157836914062\n",
      "Epoch: 33. Training data loss:  797.4918212890625\n",
      "Epoch: 34. Training data loss:  738.3767700195312\n",
      "Epoch: 35. Training data loss:  791.0487060546875\n",
      "Epoch: 36. Training data loss:  778.6694946289062\n",
      "Epoch: 37. Training data loss:  766.085205078125\n",
      "Epoch: 38. Training data loss:  811.5057373046875\n",
      "Epoch: 39. Training data loss:  804.841064453125\n",
      "Accuracy found: 11.38\n",
      "40 0.1 0.001 MultiMarginLoss() 32 0\n",
      "(24.44, (40, 0.0001, 0, CrossEntropyLoss(), 10, 1))\n"
     ]
    }
   ],
   "source": [
    "print(rand_search('tln',param_dist, classes, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration, 0\n",
      "Epoch: 0. Training data loss:  37156656.0\n",
      "Epoch: 1. Training data loss:  31100416.0\n",
      "Epoch: 2. Training data loss:  71011656.0\n",
      "Epoch: 3. Training data loss:  88432304.0\n",
      "Epoch: 4. Training data loss:  134198672.0\n",
      "Epoch: 5. Training data loss:  179986416.0\n",
      "Epoch: 6. Training data loss:  166815296.0\n",
      "Epoch: 7. Training data loss:  154176688.0\n",
      "Epoch: 8. Training data loss:  257930224.0\n",
      "Epoch: 9. Training data loss:  135425328.0\n",
      "Epoch: 10. Training data loss:  146105072.0\n",
      "Epoch: 11. Training data loss:  189600800.0\n",
      "Epoch: 12. Training data loss:  184277376.0\n",
      "Epoch: 13. Training data loss:  568846464.0\n",
      "Epoch: 14. Training data loss:  1003298240.0\n",
      "Epoch: 15. Training data loss:  1431583488.0\n",
      "Epoch: 16. Training data loss:  442635904.0\n",
      "Epoch: 17. Training data loss:  810659392.0\n",
      "Epoch: 18. Training data loss:  299015616.0\n",
      "Epoch: 19. Training data loss:  257196208.0\n",
      "Epoch: 20. Training data loss:  198378656.0\n",
      "Epoch: 21. Training data loss:  257136752.0\n",
      "Epoch: 22. Training data loss:  352809696.0\n",
      "Epoch: 23. Training data loss:  362757440.0\n",
      "Epoch: 24. Training data loss:  302209952.0\n",
      "Epoch: 25. Training data loss:  414045536.0\n",
      "Epoch: 26. Training data loss:  602559744.0\n",
      "Epoch: 27. Training data loss:  462093152.0\n",
      "Epoch: 28. Training data loss:  494478592.0\n",
      "Epoch: 29. Training data loss:  562238976.0\n",
      "Epoch: 30. Training data loss:  616834880.0\n",
      "Epoch: 31. Training data loss:  977862208.0\n",
      "Epoch: 32. Training data loss:  2091148672.0\n",
      "Epoch: 33. Training data loss:  6860202496.0\n",
      "Epoch: 34. Training data loss:  35915423744.0\n",
      "Epoch: 35. Training data loss:  125145464832.0\n",
      "Epoch: 36. Training data loss:  309862072320.0\n",
      "Epoch: 37. Training data loss:  644008968192.0\n",
      "Epoch: 38. Training data loss:  1200641540096.0\n",
      "Epoch: 39. Training data loss:  1940396703744.0\n",
      "Accuracy found: 1.0\n",
      "40 0.1 0 MultiMarginLoss() 10 1\n",
      "Starting iteration, 1\n",
      "Epoch: 0. Training data loss:  22963.779296875\n",
      "Epoch: 1. Training data loss:  22143.044921875\n",
      "Epoch: 2. Training data loss:  21625.09375\n",
      "Epoch: 3. Training data loss:  21282.72265625\n",
      "Epoch: 4. Training data loss:  21029.14453125\n",
      "Epoch: 5. Training data loss:  20842.72265625\n",
      "Epoch: 6. Training data loss:  20721.677734375\n",
      "Epoch: 7. Training data loss:  20647.302734375\n",
      "Epoch: 8. Training data loss:  20581.802734375\n",
      "Epoch: 9. Training data loss:  20543.384765625\n",
      "Epoch: 10. Training data loss:  20494.9765625\n",
      "Epoch: 11. Training data loss:  20469.6796875\n",
      "Epoch: 12. Training data loss:  20439.265625\n",
      "Epoch: 13. Training data loss:  20418.37109375\n",
      "Epoch: 14. Training data loss:  20407.322265625\n",
      "Epoch: 15. Training data loss:  20410.53125\n",
      "Epoch: 16. Training data loss:  20412.4140625\n",
      "Epoch: 17. Training data loss:  20407.27734375\n",
      "Epoch: 18. Training data loss:  20406.384765625\n",
      "Epoch: 19. Training data loss:  20403.107421875\n",
      "Accuracy found: 10.04\n",
      "20 0.01 0.01 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 2\n",
      "Epoch: 0. Training data loss:  21447.740234375\n",
      "Epoch: 1. Training data loss:  20232.24609375\n",
      "Epoch: 2. Training data loss:  19859.36328125\n",
      "Epoch: 3. Training data loss:  19620.0625\n",
      "Epoch: 4. Training data loss:  19497.892578125\n",
      "Epoch: 5. Training data loss:  19426.166015625\n",
      "Epoch: 6. Training data loss:  19365.220703125\n",
      "Epoch: 7. Training data loss:  19325.099609375\n",
      "Epoch: 8. Training data loss:  19309.197265625\n",
      "Epoch: 9. Training data loss:  19276.119140625\n",
      "Epoch: 10. Training data loss:  19273.15234375\n",
      "Epoch: 11. Training data loss:  19266.912109375\n",
      "Epoch: 12. Training data loss:  19242.154296875\n",
      "Epoch: 13. Training data loss:  19262.435546875\n",
      "Epoch: 14. Training data loss:  19247.310546875\n",
      "Epoch: 15. Training data loss:  19262.830078125\n",
      "Epoch: 16. Training data loss:  19274.544921875\n",
      "Epoch: 17. Training data loss:  19258.35546875\n",
      "Epoch: 18. Training data loss:  19235.505859375\n",
      "Epoch: 19. Training data loss:  19263.73828125\n",
      "Accuracy found: 15.07\n",
      "20 0.1 0.001 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 3\n",
      "Epoch: 0. Training data loss:  1613.8199462890625\n",
      "Epoch: 1. Training data loss:  1558.11083984375\n",
      "Epoch: 2. Training data loss:  1521.790771484375\n",
      "Epoch: 3. Training data loss:  1490.0040283203125\n",
      "Epoch: 4. Training data loss:  1467.5958251953125\n",
      "Epoch: 5. Training data loss:  1443.199462890625\n",
      "Epoch: 6. Training data loss:  1426.9556884765625\n",
      "Epoch: 7. Training data loss:  1411.9827880859375\n",
      "Epoch: 8. Training data loss:  1398.051025390625\n",
      "Epoch: 9. Training data loss:  1388.37548828125\n",
      "Epoch: 10. Training data loss:  1370.47509765625\n",
      "Epoch: 11. Training data loss:  1351.7333984375\n",
      "Epoch: 12. Training data loss:  1345.563720703125\n",
      "Epoch: 13. Training data loss:  1337.087890625\n",
      "Epoch: 14. Training data loss:  1328.4725341796875\n",
      "Epoch: 15. Training data loss:  1317.6722412109375\n",
      "Epoch: 16. Training data loss:  1309.96337890625\n",
      "Epoch: 17. Training data loss:  1298.6766357421875\n",
      "Epoch: 18. Training data loss:  1290.572998046875\n",
      "Epoch: 19. Training data loss:  1279.974853515625\n",
      "Epoch: 20. Training data loss:  1270.8597412109375\n",
      "Epoch: 21. Training data loss:  1265.0919189453125\n",
      "Epoch: 22. Training data loss:  1255.586181640625\n",
      "Epoch: 23. Training data loss:  1248.2789306640625\n",
      "Epoch: 24. Training data loss:  1239.5384521484375\n",
      "Epoch: 25. Training data loss:  1232.867919921875\n",
      "Epoch: 26. Training data loss:  1228.177490234375\n",
      "Epoch: 27. Training data loss:  1216.8843994140625\n",
      "Epoch: 28. Training data loss:  1211.8704833984375\n",
      "Epoch: 29. Training data loss:  1205.70703125\n",
      "Epoch: 30. Training data loss:  1196.496337890625\n",
      "Epoch: 31. Training data loss:  1194.2789306640625\n",
      "Epoch: 32. Training data loss:  1183.09130859375\n",
      "Epoch: 33. Training data loss:  1179.3529052734375\n",
      "Epoch: 34. Training data loss:  1173.5328369140625\n",
      "Epoch: 35. Training data loss:  1165.80810546875\n",
      "Epoch: 36. Training data loss:  1157.4427490234375\n",
      "Epoch: 37. Training data loss:  1156.6876220703125\n",
      "Epoch: 38. Training data loss:  1149.4266357421875\n",
      "Epoch: 39. Training data loss:  1141.7408447265625\n",
      "Accuracy found: 5.98\n",
      "40 0.001 0.01 MultiMarginLoss() 32 0\n",
      "Starting iteration, 4\n",
      "Epoch: 0. Training data loss:  750.1583251953125\n",
      "Epoch: 1. Training data loss:  710.7488403320312\n",
      "Epoch: 2. Training data loss:  740.350830078125\n",
      "Epoch: 3. Training data loss:  763.394287109375\n",
      "Epoch: 4. Training data loss:  771.1940307617188\n",
      "Epoch: 5. Training data loss:  773.520263671875\n",
      "Epoch: 6. Training data loss:  774.1192016601562\n",
      "Epoch: 7. Training data loss:  774.1886596679688\n",
      "Epoch: 8. Training data loss:  774.1897583007812\n",
      "Epoch: 9. Training data loss:  774.1908569335938\n",
      "Epoch: 10. Training data loss:  774.1907348632812\n",
      "Epoch: 11. Training data loss:  774.190185546875\n",
      "Epoch: 12. Training data loss:  774.1897583007812\n",
      "Epoch: 13. Training data loss:  774.1910400390625\n",
      "Epoch: 14. Training data loss:  774.1905517578125\n",
      "Epoch: 15. Training data loss:  774.190673828125\n",
      "Epoch: 16. Training data loss:  774.1911010742188\n",
      "Epoch: 17. Training data loss:  774.1911010742188\n",
      "Epoch: 18. Training data loss:  774.1908569335938\n",
      "Epoch: 19. Training data loss:  774.1913452148438\n",
      "Epoch: 20. Training data loss:  774.1897583007812\n",
      "Epoch: 21. Training data loss:  774.1900024414062\n",
      "Epoch: 22. Training data loss:  774.1907348632812\n",
      "Epoch: 23. Training data loss:  774.1901245117188\n",
      "Epoch: 24. Training data loss:  774.191162109375\n",
      "Epoch: 25. Training data loss:  774.191162109375\n",
      "Epoch: 26. Training data loss:  774.1906127929688\n",
      "Epoch: 27. Training data loss:  774.1904296875\n",
      "Epoch: 28. Training data loss:  774.1904907226562\n",
      "Epoch: 29. Training data loss:  774.1907958984375\n",
      "Epoch: 30. Training data loss:  774.191162109375\n",
      "Epoch: 31. Training data loss:  774.1912231445312\n",
      "Epoch: 32. Training data loss:  774.19091796875\n",
      "Epoch: 33. Training data loss:  774.1904907226562\n",
      "Epoch: 34. Training data loss:  774.1906127929688\n",
      "Epoch: 35. Training data loss:  774.1913452148438\n",
      "Epoch: 36. Training data loss:  774.19091796875\n",
      "Epoch: 37. Training data loss:  774.1912231445312\n",
      "Epoch: 38. Training data loss:  774.1902465820312\n",
      "Epoch: 39. Training data loss:  774.1907348632812\n",
      "Epoch: 40. Training data loss:  774.1905517578125\n",
      "Epoch: 41. Training data loss:  774.1906127929688\n",
      "Epoch: 42. Training data loss:  774.1900634765625\n",
      "Epoch: 43. Training data loss:  774.1911010742188\n",
      "Epoch: 44. Training data loss:  774.1897583007812\n",
      "Epoch: 45. Training data loss:  774.1907958984375\n",
      "Epoch: 46. Training data loss:  774.1908569335938\n",
      "Epoch: 47. Training data loss:  774.1902465820312\n",
      "Epoch: 48. Training data loss:  774.1903076171875\n",
      "Epoch: 49. Training data loss:  774.1908569335938\n",
      "Epoch: 50. Training data loss:  774.1907348632812\n",
      "Epoch: 51. Training data loss:  774.1903686523438\n",
      "Epoch: 52. Training data loss:  774.1909790039062\n",
      "Epoch: 53. Training data loss:  774.1912231445312\n",
      "Epoch: 54. Training data loss:  774.1897583007812\n",
      "Epoch: 55. Training data loss:  774.189697265625\n",
      "Epoch: 56. Training data loss:  774.1903686523438\n",
      "Epoch: 57. Training data loss:  774.1895141601562\n",
      "Epoch: 58. Training data loss:  774.1912841796875\n",
      "Epoch: 59. Training data loss:  774.1906127929688\n",
      "Accuracy found: 1.0\n",
      "60 0.01 0.3 MultiMarginLoss() 64 0\n",
      "Starting iteration, 5\n",
      "Epoch: 0. Training data loss:  1581.0758056640625\n",
      "Epoch: 1. Training data loss:  1571.1533203125\n",
      "Epoch: 2. Training data loss:  1632.0687255859375\n",
      "Epoch: 3. Training data loss:  1572.2799072265625\n",
      "Epoch: 4. Training data loss:  2122451.5\n",
      "Epoch: 5. Training data loss:  67415072.0\n",
      "Epoch: 6. Training data loss:  161140432.0\n",
      "Epoch: 7. Training data loss:  112470792.0\n",
      "Epoch: 8. Training data loss:  146228496.0\n",
      "Epoch: 9. Training data loss:  454922112.0\n",
      "Epoch: 10. Training data loss:  514428192.0\n",
      "Epoch: 11. Training data loss:  407698624.0\n",
      "Epoch: 12. Training data loss:  237913040.0\n",
      "Epoch: 13. Training data loss:  181572432.0\n",
      "Epoch: 14. Training data loss:  191361280.0\n",
      "Epoch: 15. Training data loss:  139151824.0\n",
      "Epoch: 16. Training data loss:  141890640.0\n",
      "Epoch: 17. Training data loss:  282316864.0\n",
      "Epoch: 18. Training data loss:  223567792.0\n",
      "Epoch: 19. Training data loss:  218774304.0\n",
      "Epoch: 20. Training data loss:  150275248.0\n",
      "Epoch: 21. Training data loss:  267661792.0\n",
      "Epoch: 22. Training data loss:  261129984.0\n",
      "Epoch: 23. Training data loss:  233198928.0\n",
      "Epoch: 24. Training data loss:  238466576.0\n",
      "Epoch: 25. Training data loss:  219419024.0\n",
      "Epoch: 26. Training data loss:  137585840.0\n",
      "Epoch: 27. Training data loss:  113669944.0\n",
      "Epoch: 28. Training data loss:  188403504.0\n",
      "Epoch: 29. Training data loss:  228822112.0\n",
      "Epoch: 30. Training data loss:  203500912.0\n",
      "Epoch: 31. Training data loss:  216395664.0\n",
      "Epoch: 32. Training data loss:  394776224.0\n",
      "Epoch: 33. Training data loss:  569147584.0\n",
      "Epoch: 34. Training data loss:  356341120.0\n",
      "Epoch: 35. Training data loss:  395218528.0\n",
      "Epoch: 36. Training data loss:  364674144.0\n",
      "Epoch: 37. Training data loss:  236099216.0\n",
      "Epoch: 38. Training data loss:  144943792.0\n",
      "Epoch: 39. Training data loss:  141292080.0\n",
      "Epoch: 40. Training data loss:  129397968.0\n",
      "Epoch: 41. Training data loss:  127973000.0\n",
      "Epoch: 42. Training data loss:  113902816.0\n",
      "Epoch: 43. Training data loss:  201445408.0\n",
      "Epoch: 44. Training data loss:  108172376.0\n",
      "Epoch: 45. Training data loss:  86790640.0\n",
      "Epoch: 46. Training data loss:  67979160.0\n",
      "Epoch: 47. Training data loss:  93964864.0\n",
      "Epoch: 48. Training data loss:  115791808.0\n",
      "Epoch: 49. Training data loss:  207536000.0\n",
      "Epoch: 50. Training data loss:  264179616.0\n",
      "Epoch: 51. Training data loss:  226949472.0\n",
      "Epoch: 52. Training data loss:  133726792.0\n",
      "Epoch: 53. Training data loss:  120131776.0\n",
      "Epoch: 54. Training data loss:  149855968.0\n",
      "Epoch: 55. Training data loss:  151609760.0\n",
      "Epoch: 56. Training data loss:  184091680.0\n",
      "Epoch: 57. Training data loss:  235528000.0\n",
      "Epoch: 58. Training data loss:  225123152.0\n",
      "Epoch: 59. Training data loss:  290856160.0\n",
      "Accuracy found: 1.26\n",
      "60 0.1 0.001 MultiMarginLoss() 32 1\n",
      "Starting iteration, 6\n",
      "Epoch: 0. Training data loss:  22169.591796875\n",
      "Epoch: 1. Training data loss:  20754.60546875\n",
      "Epoch: 2. Training data loss:  20216.998046875\n",
      "Epoch: 3. Training data loss:  19855.087890625\n",
      "Epoch: 4. Training data loss:  19640.8671875\n",
      "Epoch: 5. Training data loss:  19470.041015625\n",
      "Epoch: 6. Training data loss:  19350.79296875\n",
      "Epoch: 7. Training data loss:  19212.38671875\n",
      "Epoch: 8. Training data loss:  19110.669921875\n",
      "Epoch: 9. Training data loss:  19021.775390625\n",
      "Epoch: 10. Training data loss:  18921.923828125\n",
      "Epoch: 11. Training data loss:  18817.408203125\n",
      "Epoch: 12. Training data loss:  18778.310546875\n",
      "Epoch: 13. Training data loss:  18704.125\n",
      "Epoch: 14. Training data loss:  18666.9375\n",
      "Epoch: 15. Training data loss:  18614.759765625\n",
      "Epoch: 16. Training data loss:  18503.173828125\n",
      "Epoch: 17. Training data loss:  18488.830078125\n",
      "Epoch: 18. Training data loss:  18431.3828125\n",
      "Epoch: 19. Training data loss:  18401.810546875\n",
      "Epoch: 20. Training data loss:  18364.2578125\n",
      "Epoch: 21. Training data loss:  18329.845703125\n",
      "Epoch: 22. Training data loss:  18307.37890625\n",
      "Epoch: 23. Training data loss:  18227.22265625\n",
      "Epoch: 24. Training data loss:  18213.427734375\n",
      "Epoch: 25. Training data loss:  18183.859375\n",
      "Epoch: 26. Training data loss:  18165.50390625\n",
      "Epoch: 27. Training data loss:  18111.908203125\n",
      "Epoch: 28. Training data loss:  18117.630859375\n",
      "Epoch: 29. Training data loss:  18068.033203125\n",
      "Epoch: 30. Training data loss:  18064.658203125\n",
      "Epoch: 31. Training data loss:  18039.779296875\n",
      "Epoch: 32. Training data loss:  17986.5390625\n",
      "Epoch: 33. Training data loss:  17957.759765625\n",
      "Epoch: 34. Training data loss:  17982.529296875\n",
      "Epoch: 35. Training data loss:  17961.763671875\n",
      "Epoch: 36. Training data loss:  17928.90625\n",
      "Epoch: 37. Training data loss:  17902.103515625\n",
      "Epoch: 38. Training data loss:  17895.296875\n",
      "Epoch: 39. Training data loss:  17857.337890625\n",
      "Epoch: 40. Training data loss:  17849.60546875\n",
      "Epoch: 41. Training data loss:  17865.41796875\n",
      "Epoch: 42. Training data loss:  17818.978515625\n",
      "Epoch: 43. Training data loss:  17817.771484375\n",
      "Epoch: 44. Training data loss:  17820.498046875\n",
      "Epoch: 45. Training data loss:  17770.2265625\n",
      "Epoch: 46. Training data loss:  17750.310546875\n",
      "Epoch: 47. Training data loss:  17756.826171875\n",
      "Epoch: 48. Training data loss:  17729.646484375\n",
      "Epoch: 49. Training data loss:  17693.646484375\n",
      "Epoch: 50. Training data loss:  17693.517578125\n",
      "Epoch: 51. Training data loss:  17701.279296875\n",
      "Epoch: 52. Training data loss:  17682.115234375\n",
      "Epoch: 53. Training data loss:  17680.361328125\n",
      "Epoch: 54. Training data loss:  17642.705078125\n",
      "Epoch: 55. Training data loss:  17644.1328125\n",
      "Epoch: 56. Training data loss:  17647.45703125\n",
      "Epoch: 57. Training data loss:  17651.0078125\n",
      "Epoch: 58. Training data loss:  17630.111328125\n",
      "Epoch: 59. Training data loss:  17599.830078125\n",
      "Accuracy found: 20.98\n",
      "60 0.0001 0 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 7\n",
      "Epoch: 0. Training data loss:  20862.591796875\n",
      "Epoch: 1. Training data loss:  19615.5703125\n",
      "Epoch: 2. Training data loss:  19151.376953125\n",
      "Epoch: 3. Training data loss:  18907.064453125\n",
      "Epoch: 4. Training data loss:  18737.638671875\n",
      "Epoch: 5. Training data loss:  18604.263671875\n",
      "Epoch: 6. Training data loss:  18486.609375\n",
      "Epoch: 7. Training data loss:  18422.40234375\n",
      "Epoch: 8. Training data loss:  18357.814453125\n",
      "Epoch: 9. Training data loss:  18248.033203125\n",
      "Epoch: 10. Training data loss:  18199.138671875\n",
      "Epoch: 11. Training data loss:  18189.130859375\n",
      "Epoch: 12. Training data loss:  18114.701171875\n",
      "Epoch: 13. Training data loss:  18108.4453125\n",
      "Epoch: 14. Training data loss:  18050.640625\n",
      "Epoch: 15. Training data loss:  18020.375\n",
      "Epoch: 16. Training data loss:  18026.625\n",
      "Epoch: 17. Training data loss:  18000.634765625\n",
      "Epoch: 18. Training data loss:  17957.43359375\n",
      "Epoch: 19. Training data loss:  17919.0859375\n",
      "Epoch: 20. Training data loss:  17893.42578125\n",
      "Epoch: 21. Training data loss:  17849.966796875\n",
      "Epoch: 22. Training data loss:  17876.43359375\n",
      "Epoch: 23. Training data loss:  17838.798828125\n",
      "Epoch: 24. Training data loss:  17796.4140625\n",
      "Epoch: 25. Training data loss:  17806.8125\n",
      "Epoch: 26. Training data loss:  17789.0078125\n",
      "Epoch: 27. Training data loss:  17763.12109375\n",
      "Epoch: 28. Training data loss:  17773.287109375\n",
      "Epoch: 29. Training data loss:  17718.701171875\n",
      "Epoch: 30. Training data loss:  17698.16796875\n",
      "Epoch: 31. Training data loss:  17732.1953125\n",
      "Epoch: 32. Training data loss:  17699.990234375\n",
      "Epoch: 33. Training data loss:  17663.66796875\n",
      "Epoch: 34. Training data loss:  17673.232421875\n",
      "Epoch: 35. Training data loss:  17663.494140625\n",
      "Epoch: 36. Training data loss:  17624.74609375\n",
      "Epoch: 37. Training data loss:  17651.900390625\n",
      "Epoch: 38. Training data loss:  17588.408203125\n",
      "Epoch: 39. Training data loss:  17601.873046875\n",
      "Accuracy found: 20.91\n",
      "40 0.001 0 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 8\n",
      "Epoch: 0. Training data loss:  1880.9300537109375\n",
      "Epoch: 1. Training data loss:  1863.1773681640625\n",
      "Epoch: 2. Training data loss:  1850.9375\n",
      "Epoch: 3. Training data loss:  1841.32177734375\n",
      "Epoch: 4. Training data loss:  1831.378173828125\n",
      "Epoch: 5. Training data loss:  1824.80419921875\n",
      "Epoch: 6. Training data loss:  1818.8074951171875\n",
      "Epoch: 7. Training data loss:  1812.97265625\n",
      "Epoch: 8. Training data loss:  1806.678466796875\n",
      "Epoch: 9. Training data loss:  1802.1861572265625\n",
      "Epoch: 10. Training data loss:  1796.89111328125\n",
      "Epoch: 11. Training data loss:  1791.53955078125\n",
      "Epoch: 12. Training data loss:  1790.644287109375\n",
      "Epoch: 13. Training data loss:  1784.4251708984375\n",
      "Epoch: 14. Training data loss:  1781.5133056640625\n",
      "Epoch: 15. Training data loss:  1778.2430419921875\n",
      "Epoch: 16. Training data loss:  1775.486572265625\n",
      "Epoch: 17. Training data loss:  1774.098388671875\n",
      "Epoch: 18. Training data loss:  1770.051025390625\n",
      "Epoch: 19. Training data loss:  1766.7568359375\n",
      "Accuracy found: 5.04\n",
      "20 0.001 0.001 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 9\n",
      "Epoch: 0. Training data loss:  6984.86474609375\n",
      "Epoch: 1. Training data loss:  6585.904296875\n",
      "Epoch: 2. Training data loss:  6401.68701171875\n",
      "Epoch: 3. Training data loss:  6307.5947265625\n",
      "Epoch: 4. Training data loss:  6254.734375\n",
      "Epoch: 5. Training data loss:  6220.76123046875\n",
      "Epoch: 6. Training data loss:  6190.22216796875\n",
      "Epoch: 7. Training data loss:  6178.21044921875\n",
      "Epoch: 8. Training data loss:  6164.41357421875\n",
      "Epoch: 9. Training data loss:  6159.95068359375\n",
      "Epoch: 10. Training data loss:  6148.49462890625\n",
      "Epoch: 11. Training data loss:  6139.876953125\n",
      "Epoch: 12. Training data loss:  6142.79052734375\n",
      "Epoch: 13. Training data loss:  6138.8583984375\n",
      "Epoch: 14. Training data loss:  6133.22802734375\n",
      "Epoch: 15. Training data loss:  6132.1083984375\n",
      "Epoch: 16. Training data loss:  6127.86279296875\n",
      "Epoch: 17. Training data loss:  6128.77783203125\n",
      "Epoch: 18. Training data loss:  6125.23046875\n",
      "Epoch: 19. Training data loss:  6122.9169921875\n",
      "Accuracy found: 12.09\n",
      "20 0.0001 0.01 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 10\n",
      "Epoch: 0. Training data loss:  3616.31689453125\n",
      "Epoch: 1. Training data loss:  3614.8359375\n",
      "Epoch: 2. Training data loss:  3617.35498046875\n",
      "Epoch: 3. Training data loss:  3613.523193359375\n",
      "Epoch: 4. Training data loss:  3614.27685546875\n",
      "Epoch: 5. Training data loss:  3618.34423828125\n",
      "Epoch: 6. Training data loss:  3616.530029296875\n",
      "Epoch: 7. Training data loss:  3613.6015625\n",
      "Epoch: 8. Training data loss:  3616.134765625\n",
      "Epoch: 9. Training data loss:  3615.03857421875\n",
      "Epoch: 10. Training data loss:  3617.44287109375\n",
      "Epoch: 11. Training data loss:  3613.572998046875\n",
      "Epoch: 12. Training data loss:  3617.2998046875\n",
      "Epoch: 13. Training data loss:  3613.99609375\n",
      "Epoch: 14. Training data loss:  3614.64013671875\n",
      "Epoch: 15. Training data loss:  3618.180908203125\n",
      "Epoch: 16. Training data loss:  3613.557373046875\n",
      "Epoch: 17. Training data loss:  3616.95654296875\n",
      "Epoch: 18. Training data loss:  3613.1953125\n",
      "Epoch: 19. Training data loss:  3616.03955078125\n",
      "Accuracy found: 0.89\n",
      "20 0.1 0.01 CrossEntropyLoss() 64 1\n",
      "Starting iteration, 11\n",
      "Epoch: 0. Training data loss:  612.1022338867188\n",
      "Epoch: 1. Training data loss:  456.8345642089844\n",
      "Epoch: 2. Training data loss:  405.0765686035156\n",
      "Epoch: 3. Training data loss:  380.6255798339844\n",
      "Epoch: 4. Training data loss:  364.54248046875\n",
      "Epoch: 5. Training data loss:  353.7648620605469\n",
      "Epoch: 6. Training data loss:  345.1736755371094\n",
      "Epoch: 7. Training data loss:  338.9083251953125\n",
      "Epoch: 8. Training data loss:  333.39947509765625\n",
      "Epoch: 9. Training data loss:  329.1624755859375\n",
      "Epoch: 10. Training data loss:  324.3040466308594\n",
      "Epoch: 11. Training data loss:  321.09759521484375\n",
      "Epoch: 12. Training data loss:  317.50921630859375\n",
      "Epoch: 13. Training data loss:  314.520263671875\n",
      "Epoch: 14. Training data loss:  311.99102783203125\n",
      "Epoch: 15. Training data loss:  310.9391784667969\n",
      "Epoch: 16. Training data loss:  308.1958923339844\n",
      "Epoch: 17. Training data loss:  306.3751525878906\n",
      "Epoch: 18. Training data loss:  304.956298828125\n",
      "Epoch: 19. Training data loss:  304.6888427734375\n",
      "Accuracy found: 16.67\n",
      "20 0.1 0.001 MultiMarginLoss() 64 0\n",
      "Starting iteration, 12\n",
      "Epoch: 0. Training data loss:  412.1690979003906\n",
      "Epoch: 1. Training data loss:  401.5908508300781\n",
      "Epoch: 2. Training data loss:  392.5423278808594\n",
      "Epoch: 3. Training data loss:  383.7918701171875\n",
      "Epoch: 4. Training data loss:  376.5631408691406\n",
      "Epoch: 5. Training data loss:  369.65899658203125\n",
      "Epoch: 6. Training data loss:  364.6391296386719\n",
      "Epoch: 7. Training data loss:  361.9412536621094\n",
      "Epoch: 8. Training data loss:  356.4241943359375\n",
      "Epoch: 9. Training data loss:  354.60101318359375\n",
      "Epoch: 10. Training data loss:  352.28448486328125\n",
      "Epoch: 11. Training data loss:  350.9859313964844\n",
      "Epoch: 12. Training data loss:  350.45831298828125\n",
      "Epoch: 13. Training data loss:  349.2186279296875\n",
      "Epoch: 14. Training data loss:  349.259033203125\n",
      "Epoch: 15. Training data loss:  350.1483154296875\n",
      "Epoch: 16. Training data loss:  350.24920654296875\n",
      "Epoch: 17. Training data loss:  350.5755310058594\n",
      "Epoch: 18. Training data loss:  351.5816345214844\n",
      "Epoch: 19. Training data loss:  353.24981689453125\n",
      "Accuracy found: 3.7\n",
      "20 0.001 0.3 MultiMarginLoss() 128 0\n",
      "Starting iteration, 13\n",
      "Epoch: 0. Training data loss:  6463.18408203125\n",
      "Epoch: 1. Training data loss:  6023.83447265625\n",
      "Epoch: 2. Training data loss:  5866.9853515625\n",
      "Epoch: 3. Training data loss:  5769.6171875\n",
      "Epoch: 4. Training data loss:  5701.400390625\n",
      "Epoch: 5. Training data loss:  5648.4853515625\n",
      "Epoch: 6. Training data loss:  5593.6884765625\n",
      "Epoch: 7. Training data loss:  5557.2666015625\n",
      "Epoch: 8. Training data loss:  5523.37353515625\n",
      "Epoch: 9. Training data loss:  5495.08837890625\n",
      "Epoch: 10. Training data loss:  5474.462890625\n",
      "Epoch: 11. Training data loss:  5456.16943359375\n",
      "Epoch: 12. Training data loss:  5434.02587890625\n",
      "Epoch: 13. Training data loss:  5416.3662109375\n",
      "Epoch: 14. Training data loss:  5403.8515625\n",
      "Epoch: 15. Training data loss:  5385.1884765625\n",
      "Epoch: 16. Training data loss:  5372.9326171875\n",
      "Epoch: 17. Training data loss:  5353.96923828125\n",
      "Epoch: 18. Training data loss:  5344.39990234375\n",
      "Epoch: 19. Training data loss:  5338.310546875\n",
      "Epoch: 20. Training data loss:  5329.98291015625\n",
      "Epoch: 21. Training data loss:  5323.42041015625\n",
      "Epoch: 22. Training data loss:  5313.71875\n",
      "Epoch: 23. Training data loss:  5313.94189453125\n",
      "Epoch: 24. Training data loss:  5304.69140625\n",
      "Epoch: 25. Training data loss:  5294.01513671875\n",
      "Epoch: 26. Training data loss:  5280.37890625\n",
      "Epoch: 27. Training data loss:  5273.61279296875\n",
      "Epoch: 28. Training data loss:  5277.59912109375\n",
      "Epoch: 29. Training data loss:  5258.236328125\n",
      "Epoch: 30. Training data loss:  5257.61962890625\n",
      "Epoch: 31. Training data loss:  5255.37451171875\n",
      "Epoch: 32. Training data loss:  5256.265625\n",
      "Epoch: 33. Training data loss:  5241.60693359375\n",
      "Epoch: 34. Training data loss:  5237.02490234375\n",
      "Epoch: 35. Training data loss:  5244.9111328125\n",
      "Epoch: 36. Training data loss:  5234.98388671875\n",
      "Epoch: 37. Training data loss:  5225.49560546875\n",
      "Epoch: 38. Training data loss:  5224.3701171875\n",
      "Epoch: 39. Training data loss:  5215.0703125\n",
      "Epoch: 40. Training data loss:  5212.72021484375\n",
      "Epoch: 41. Training data loss:  5206.73828125\n",
      "Epoch: 42. Training data loss:  5200.86572265625\n",
      "Epoch: 43. Training data loss:  5197.44970703125\n",
      "Epoch: 44. Training data loss:  5201.48876953125\n",
      "Epoch: 45. Training data loss:  5193.22021484375\n",
      "Epoch: 46. Training data loss:  5192.47705078125\n",
      "Epoch: 47. Training data loss:  5187.0966796875\n",
      "Epoch: 48. Training data loss:  5180.29052734375\n",
      "Epoch: 49. Training data loss:  5191.94873046875\n",
      "Epoch: 50. Training data loss:  5178.6005859375\n",
      "Epoch: 51. Training data loss:  5179.34130859375\n",
      "Epoch: 52. Training data loss:  5178.64013671875\n",
      "Epoch: 53. Training data loss:  5172.80908203125\n",
      "Epoch: 54. Training data loss:  5164.22265625\n",
      "Epoch: 55. Training data loss:  5169.16796875\n",
      "Epoch: 56. Training data loss:  5169.34228515625\n",
      "Epoch: 57. Training data loss:  5161.755859375\n",
      "Epoch: 58. Training data loss:  5165.14208984375\n",
      "Epoch: 59. Training data loss:  5157.9375\n",
      "Accuracy found: 23.16\n",
      "60 0.001 0 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 14\n",
      "Epoch: 0. Training data loss:  731.6951293945312\n",
      "Epoch: 1. Training data loss:  658.0731201171875\n",
      "Epoch: 2. Training data loss:  618.6109619140625\n",
      "Epoch: 3. Training data loss:  588.6348266601562\n",
      "Epoch: 4. Training data loss:  563.8969116210938\n",
      "Epoch: 5. Training data loss:  540.2279052734375\n",
      "Epoch: 6. Training data loss:  521.7271118164062\n",
      "Epoch: 7. Training data loss:  505.4590759277344\n",
      "Epoch: 8. Training data loss:  491.2767333984375\n",
      "Epoch: 9. Training data loss:  479.3301696777344\n",
      "Epoch: 10. Training data loss:  468.2955627441406\n",
      "Epoch: 11. Training data loss:  461.2780456542969\n",
      "Epoch: 12. Training data loss:  452.4615173339844\n",
      "Epoch: 13. Training data loss:  447.0230407714844\n",
      "Epoch: 14. Training data loss:  441.4005432128906\n",
      "Epoch: 15. Training data loss:  438.02435302734375\n",
      "Epoch: 16. Training data loss:  434.3593444824219\n",
      "Epoch: 17. Training data loss:  431.2106018066406\n",
      "Epoch: 18. Training data loss:  428.3551940917969\n",
      "Epoch: 19. Training data loss:  427.6518859863281\n",
      "Epoch: 20. Training data loss:  425.8787841796875\n",
      "Epoch: 21. Training data loss:  424.8419494628906\n",
      "Epoch: 22. Training data loss:  423.5008239746094\n",
      "Epoch: 23. Training data loss:  422.9397277832031\n",
      "Epoch: 24. Training data loss:  421.9168395996094\n",
      "Epoch: 25. Training data loss:  421.418701171875\n",
      "Epoch: 26. Training data loss:  421.25244140625\n",
      "Epoch: 27. Training data loss:  421.9437561035156\n",
      "Epoch: 28. Training data loss:  421.4317932128906\n",
      "Epoch: 29. Training data loss:  421.3905334472656\n",
      "Epoch: 30. Training data loss:  421.0384216308594\n",
      "Epoch: 31. Training data loss:  421.6831359863281\n",
      "Epoch: 32. Training data loss:  421.4637756347656\n",
      "Epoch: 33. Training data loss:  421.3243408203125\n",
      "Epoch: 34. Training data loss:  420.9754943847656\n",
      "Epoch: 35. Training data loss:  421.3951721191406\n",
      "Epoch: 36. Training data loss:  421.5770263671875\n",
      "Epoch: 37. Training data loss:  420.73748779296875\n",
      "Epoch: 38. Training data loss:  421.3590393066406\n",
      "Epoch: 39. Training data loss:  421.1227722167969\n",
      "Epoch: 40. Training data loss:  421.438232421875\n",
      "Epoch: 41. Training data loss:  421.3090515136719\n",
      "Epoch: 42. Training data loss:  421.9940490722656\n",
      "Epoch: 43. Training data loss:  421.57196044921875\n",
      "Epoch: 44. Training data loss:  420.9600524902344\n",
      "Epoch: 45. Training data loss:  422.30694580078125\n",
      "Epoch: 46. Training data loss:  422.20892333984375\n",
      "Epoch: 47. Training data loss:  422.23126220703125\n",
      "Epoch: 48. Training data loss:  421.9722595214844\n",
      "Epoch: 49. Training data loss:  422.7591857910156\n",
      "Epoch: 50. Training data loss:  422.1745910644531\n",
      "Epoch: 51. Training data loss:  421.660888671875\n",
      "Epoch: 52. Training data loss:  422.3851623535156\n",
      "Epoch: 53. Training data loss:  422.14532470703125\n",
      "Epoch: 54. Training data loss:  421.8633117675781\n",
      "Epoch: 55. Training data loss:  422.7291259765625\n",
      "Epoch: 56. Training data loss:  421.56243896484375\n",
      "Epoch: 57. Training data loss:  422.2546081542969\n",
      "Epoch: 58. Training data loss:  421.50811767578125\n",
      "Epoch: 59. Training data loss:  422.4557189941406\n",
      "Accuracy found: 9.43\n",
      "60 0.01 0.01 MultiMarginLoss() 64 0\n",
      "Starting iteration, 15\n",
      "Epoch: 0. Training data loss:  7214.34912109375\n",
      "Epoch: 1. Training data loss:  6990.80615234375\n",
      "Epoch: 2. Training data loss:  6869.072265625\n",
      "Epoch: 3. Training data loss:  6753.6591796875\n",
      "Epoch: 4. Training data loss:  6658.99267578125\n",
      "Epoch: 5. Training data loss:  6583.89453125\n",
      "Epoch: 6. Training data loss:  6508.60205078125\n",
      "Epoch: 7. Training data loss:  6444.52587890625\n",
      "Epoch: 8. Training data loss:  6385.5048828125\n",
      "Epoch: 9. Training data loss:  6333.3701171875\n",
      "Epoch: 10. Training data loss:  6298.84130859375\n",
      "Epoch: 11. Training data loss:  6261.13916015625\n",
      "Epoch: 12. Training data loss:  6222.33837890625\n",
      "Epoch: 13. Training data loss:  6185.28369140625\n",
      "Epoch: 14. Training data loss:  6160.21240234375\n",
      "Epoch: 15. Training data loss:  6135.29736328125\n",
      "Epoch: 16. Training data loss:  6116.2265625\n",
      "Epoch: 17. Training data loss:  6086.8994140625\n",
      "Epoch: 18. Training data loss:  6070.1767578125\n",
      "Epoch: 19. Training data loss:  6042.4189453125\n",
      "Epoch: 20. Training data loss:  6030.052734375\n",
      "Epoch: 21. Training data loss:  6018.17578125\n",
      "Epoch: 22. Training data loss:  5992.8076171875\n",
      "Epoch: 23. Training data loss:  5984.2763671875\n",
      "Epoch: 24. Training data loss:  5978.46630859375\n",
      "Epoch: 25. Training data loss:  5961.1083984375\n",
      "Epoch: 26. Training data loss:  5941.9921875\n",
      "Epoch: 27. Training data loss:  5932.0703125\n",
      "Epoch: 28. Training data loss:  5927.1357421875\n",
      "Epoch: 29. Training data loss:  5906.71435546875\n",
      "Epoch: 30. Training data loss:  5903.97119140625\n",
      "Epoch: 31. Training data loss:  5882.2822265625\n",
      "Epoch: 32. Training data loss:  5876.5673828125\n",
      "Epoch: 33. Training data loss:  5867.44482421875\n",
      "Epoch: 34. Training data loss:  5856.1728515625\n",
      "Epoch: 35. Training data loss:  5849.86279296875\n",
      "Epoch: 36. Training data loss:  5844.42236328125\n",
      "Epoch: 37. Training data loss:  5834.43115234375\n",
      "Epoch: 38. Training data loss:  5827.4619140625\n",
      "Epoch: 39. Training data loss:  5820.15673828125\n",
      "Epoch: 40. Training data loss:  5801.4150390625\n",
      "Epoch: 41. Training data loss:  5809.4765625\n",
      "Epoch: 42. Training data loss:  5794.24462890625\n",
      "Epoch: 43. Training data loss:  5795.27783203125\n",
      "Epoch: 44. Training data loss:  5785.4951171875\n",
      "Epoch: 45. Training data loss:  5776.2080078125\n",
      "Epoch: 46. Training data loss:  5775.41552734375\n",
      "Epoch: 47. Training data loss:  5766.92919921875\n",
      "Epoch: 48. Training data loss:  5761.3447265625\n",
      "Epoch: 49. Training data loss:  5748.03515625\n",
      "Epoch: 50. Training data loss:  5740.572265625\n",
      "Epoch: 51. Training data loss:  5736.46240234375\n",
      "Epoch: 52. Training data loss:  5727.064453125\n",
      "Epoch: 53. Training data loss:  5721.05908203125\n",
      "Epoch: 54. Training data loss:  5714.45361328125\n",
      "Epoch: 55. Training data loss:  5721.123046875\n",
      "Epoch: 56. Training data loss:  5711.2919921875\n",
      "Epoch: 57. Training data loss:  5708.8603515625\n",
      "Epoch: 58. Training data loss:  5689.6982421875\n",
      "Epoch: 59. Training data loss:  5700.23486328125\n",
      "Accuracy found: 18.13\n",
      "60 0.01 0.001 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 16\n",
      "Epoch: 0. Training data loss:  25554.859375\n",
      "Epoch: 1. Training data loss:  26164.7265625\n",
      "Epoch: 2. Training data loss:  26373.712890625\n",
      "Epoch: 3. Training data loss:  26320.966796875\n",
      "Epoch: 4. Training data loss:  26700.111328125\n",
      "Epoch: 5. Training data loss:  26208.3671875\n",
      "Epoch: 6. Training data loss:  26430.001953125\n",
      "Epoch: 7. Training data loss:  26666.173828125\n",
      "Epoch: 8. Training data loss:  26326.421875\n",
      "Epoch: 9. Training data loss:  26416.103515625\n",
      "Epoch: 10. Training data loss:  26557.494140625\n",
      "Epoch: 11. Training data loss:  26398.455078125\n",
      "Epoch: 12. Training data loss:  26456.4921875\n",
      "Epoch: 13. Training data loss:  26273.193359375\n",
      "Epoch: 14. Training data loss:  26483.31640625\n",
      "Epoch: 15. Training data loss:  26334.841796875\n",
      "Epoch: 16. Training data loss:  26674.197265625\n",
      "Epoch: 17. Training data loss:  26120.61328125\n",
      "Epoch: 18. Training data loss:  26389.326171875\n",
      "Epoch: 19. Training data loss:  26734.298828125\n",
      "Accuracy found: 1.47\n",
      "20 0.1 0 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 17\n",
      "Epoch: 0. Training data loss:  23027.00390625\n",
      "Epoch: 1. Training data loss:  23028.658203125\n",
      "Epoch: 2. Training data loss:  23027.603515625\n",
      "Epoch: 3. Training data loss:  23027.9453125\n",
      "Epoch: 4. Training data loss:  23027.642578125\n",
      "Epoch: 5. Training data loss:  23027.47265625\n",
      "Epoch: 6. Training data loss:  23027.33203125\n",
      "Epoch: 7. Training data loss:  23027.90625\n",
      "Epoch: 8. Training data loss:  23027.6875\n",
      "Epoch: 9. Training data loss:  23027.54296875\n",
      "Epoch: 10. Training data loss:  23027.10546875\n",
      "Epoch: 11. Training data loss:  23026.421875\n",
      "Epoch: 12. Training data loss:  23027.24609375\n",
      "Epoch: 13. Training data loss:  23027.962890625\n",
      "Epoch: 14. Training data loss:  23027.740234375\n",
      "Epoch: 15. Training data loss:  23027.21875\n",
      "Epoch: 16. Training data loss:  23026.947265625\n",
      "Epoch: 17. Training data loss:  23027.203125\n",
      "Epoch: 18. Training data loss:  23027.1015625\n",
      "Epoch: 19. Training data loss:  23027.64453125\n",
      "Epoch: 20. Training data loss:  23027.701171875\n",
      "Epoch: 21. Training data loss:  23027.845703125\n",
      "Epoch: 22. Training data loss:  23027.412109375\n",
      "Epoch: 23. Training data loss:  23027.171875\n",
      "Epoch: 24. Training data loss:  23027.068359375\n",
      "Epoch: 25. Training data loss:  23027.078125\n",
      "Epoch: 26. Training data loss:  23027.1875\n",
      "Epoch: 27. Training data loss:  23027.09765625\n",
      "Epoch: 28. Training data loss:  23027.685546875\n",
      "Epoch: 29. Training data loss:  23027.203125\n",
      "Epoch: 30. Training data loss:  23026.927734375\n",
      "Epoch: 31. Training data loss:  23027.234375\n",
      "Epoch: 32. Training data loss:  23027.828125\n",
      "Epoch: 33. Training data loss:  23027.98828125\n",
      "Epoch: 34. Training data loss:  23027.109375\n",
      "Epoch: 35. Training data loss:  23028.119140625\n",
      "Epoch: 36. Training data loss:  23027.546875\n",
      "Epoch: 37. Training data loss:  23027.01953125\n",
      "Epoch: 38. Training data loss:  23028.322265625\n",
      "Epoch: 39. Training data loss:  23027.091796875\n",
      "Epoch: 40. Training data loss:  23027.2890625\n",
      "Epoch: 41. Training data loss:  23027.19140625\n",
      "Epoch: 42. Training data loss:  23027.240234375\n",
      "Epoch: 43. Training data loss:  23026.619140625\n",
      "Epoch: 44. Training data loss:  23027.294921875\n",
      "Epoch: 45. Training data loss:  23027.1171875\n",
      "Epoch: 46. Training data loss:  23026.662109375\n",
      "Epoch: 47. Training data loss:  23027.265625\n",
      "Epoch: 48. Training data loss:  23027.388671875\n",
      "Epoch: 49. Training data loss:  23027.267578125\n",
      "Epoch: 50. Training data loss:  23027.283203125\n",
      "Epoch: 51. Training data loss:  23027.89453125\n",
      "Epoch: 52. Training data loss:  23027.83203125\n",
      "Epoch: 53. Training data loss:  23026.55859375\n",
      "Epoch: 54. Training data loss:  23027.02734375\n",
      "Epoch: 55. Training data loss:  23027.466796875\n",
      "Epoch: 56. Training data loss:  23026.666015625\n",
      "Epoch: 57. Training data loss:  23027.470703125\n",
      "Epoch: 58. Training data loss:  23027.509765625\n",
      "Epoch: 59. Training data loss:  23026.72265625\n",
      "Accuracy found: 1.0\n",
      "60 0.01 0.3 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 18\n",
      "Epoch: 0. Training data loss:  5122.29736328125\n",
      "Epoch: 1. Training data loss:  4925.6025390625\n",
      "Epoch: 2. Training data loss:  4770.47705078125\n",
      "Epoch: 3. Training data loss:  4653.154296875\n",
      "Epoch: 4. Training data loss:  4560.24755859375\n",
      "Epoch: 5. Training data loss:  4465.28271484375\n",
      "Epoch: 6. Training data loss:  4395.55859375\n",
      "Epoch: 7. Training data loss:  4314.384765625\n",
      "Epoch: 8. Training data loss:  4242.486328125\n",
      "Epoch: 9. Training data loss:  4196.68505859375\n",
      "Epoch: 10. Training data loss:  4136.10693359375\n",
      "Epoch: 11. Training data loss:  4091.349609375\n",
      "Epoch: 12. Training data loss:  4037.775390625\n",
      "Epoch: 13. Training data loss:  3992.50244140625\n",
      "Epoch: 14. Training data loss:  3945.3203125\n",
      "Epoch: 15. Training data loss:  3898.525634765625\n",
      "Epoch: 16. Training data loss:  3865.39794921875\n",
      "Epoch: 17. Training data loss:  3823.48486328125\n",
      "Epoch: 18. Training data loss:  3783.928955078125\n",
      "Epoch: 19. Training data loss:  3742.7568359375\n",
      "Epoch: 20. Training data loss:  3705.885986328125\n",
      "Epoch: 21. Training data loss:  3675.344482421875\n",
      "Epoch: 22. Training data loss:  3641.59619140625\n",
      "Epoch: 23. Training data loss:  3600.038818359375\n",
      "Epoch: 24. Training data loss:  3567.325927734375\n",
      "Epoch: 25. Training data loss:  3544.225830078125\n",
      "Epoch: 26. Training data loss:  3512.04443359375\n",
      "Epoch: 27. Training data loss:  3471.58154296875\n",
      "Epoch: 28. Training data loss:  3457.914306640625\n",
      "Epoch: 29. Training data loss:  3431.285400390625\n",
      "Epoch: 30. Training data loss:  3393.18994140625\n",
      "Epoch: 31. Training data loss:  3367.744140625\n",
      "Epoch: 32. Training data loss:  3346.310302734375\n",
      "Epoch: 33. Training data loss:  3332.054931640625\n",
      "Epoch: 34. Training data loss:  3304.736328125\n",
      "Epoch: 35. Training data loss:  3282.7880859375\n",
      "Epoch: 36. Training data loss:  3260.186279296875\n",
      "Epoch: 37. Training data loss:  3244.68017578125\n",
      "Epoch: 38. Training data loss:  3230.03125\n",
      "Epoch: 39. Training data loss:  3205.433349609375\n",
      "Accuracy found: 7.86\n",
      "40 0.001 0 MultiMarginLoss() 10 0\n",
      "Starting iteration, 19\n",
      "Epoch: 0. Training data loss:  4951.20556640625\n",
      "Epoch: 1. Training data loss:  4954.57568359375\n",
      "Epoch: 2. Training data loss:  4949.91943359375\n",
      "Epoch: 3. Training data loss:  4951.1748046875\n",
      "Epoch: 4. Training data loss:  4950.14404296875\n",
      "Epoch: 5. Training data loss:  4948.876953125\n",
      "Epoch: 6. Training data loss:  4948.71484375\n",
      "Epoch: 7. Training data loss:  4950.439453125\n",
      "Epoch: 8. Training data loss:  4948.6767578125\n",
      "Epoch: 9. Training data loss:  4951.93115234375\n",
      "Epoch: 10. Training data loss:  4951.8115234375\n",
      "Epoch: 11. Training data loss:  4952.5947265625\n",
      "Epoch: 12. Training data loss:  4948.6142578125\n",
      "Epoch: 13. Training data loss:  4951.15966796875\n",
      "Epoch: 14. Training data loss:  4950.3857421875\n",
      "Epoch: 15. Training data loss:  4948.48583984375\n",
      "Epoch: 16. Training data loss:  4951.3740234375\n",
      "Epoch: 17. Training data loss:  4950.47265625\n",
      "Epoch: 18. Training data loss:  4951.39990234375\n",
      "Epoch: 19. Training data loss:  4950.26025390625\n",
      "Epoch: 20. Training data loss:  4952.5908203125\n",
      "Epoch: 21. Training data loss:  4949.78515625\n",
      "Epoch: 22. Training data loss:  4947.87109375\n",
      "Epoch: 23. Training data loss:  4951.1474609375\n",
      "Epoch: 24. Training data loss:  4949.47509765625\n",
      "Epoch: 25. Training data loss:  4948.77099609375\n",
      "Epoch: 26. Training data loss:  4951.3046875\n",
      "Epoch: 27. Training data loss:  4949.76806640625\n",
      "Epoch: 28. Training data loss:  4952.58984375\n",
      "Epoch: 29. Training data loss:  4948.7041015625\n",
      "Epoch: 30. Training data loss:  4952.216796875\n",
      "Epoch: 31. Training data loss:  4951.1533203125\n",
      "Epoch: 32. Training data loss:  4946.3583984375\n",
      "Epoch: 33. Training data loss:  4951.65478515625\n",
      "Epoch: 34. Training data loss:  4951.8515625\n",
      "Epoch: 35. Training data loss:  4950.46435546875\n",
      "Epoch: 36. Training data loss:  4953.54345703125\n",
      "Epoch: 37. Training data loss:  4951.53173828125\n",
      "Epoch: 38. Training data loss:  4950.552734375\n",
      "Epoch: 39. Training data loss:  4950.06787109375\n",
      "Accuracy found: 1.0\n",
      "40 0.1 0.3 MultiMarginLoss() 10 1\n",
      "Starting iteration, 20\n",
      "Epoch: 0. Training data loss:  7150.755859375\n",
      "Epoch: 1. Training data loss:  7178.58837890625\n",
      "Epoch: 2. Training data loss:  7196.84521484375\n",
      "Epoch: 3. Training data loss:  7197.9443359375\n",
      "Epoch: 4. Training data loss:  7197.93505859375\n",
      "Epoch: 5. Training data loss:  7197.94140625\n",
      "Epoch: 6. Training data loss:  7197.94482421875\n",
      "Epoch: 7. Training data loss:  7197.94921875\n",
      "Epoch: 8. Training data loss:  7197.955078125\n",
      "Epoch: 9. Training data loss:  7197.95068359375\n",
      "Epoch: 10. Training data loss:  7197.9501953125\n",
      "Epoch: 11. Training data loss:  7197.95458984375\n",
      "Epoch: 12. Training data loss:  7197.94140625\n",
      "Epoch: 13. Training data loss:  7197.9404296875\n",
      "Epoch: 14. Training data loss:  7197.93994140625\n",
      "Epoch: 15. Training data loss:  7197.9443359375\n",
      "Epoch: 16. Training data loss:  7197.94775390625\n",
      "Epoch: 17. Training data loss:  7197.95654296875\n",
      "Epoch: 18. Training data loss:  7197.9501953125\n",
      "Epoch: 19. Training data loss:  7197.955078125\n",
      "Epoch: 20. Training data loss:  7197.9580078125\n",
      "Epoch: 21. Training data loss:  7197.94287109375\n",
      "Epoch: 22. Training data loss:  7197.95947265625\n",
      "Epoch: 23. Training data loss:  7197.94677734375\n",
      "Epoch: 24. Training data loss:  7197.9404296875\n",
      "Epoch: 25. Training data loss:  7197.939453125\n",
      "Epoch: 26. Training data loss:  7197.94873046875\n",
      "Epoch: 27. Training data loss:  7197.94873046875\n",
      "Epoch: 28. Training data loss:  7197.943359375\n",
      "Epoch: 29. Training data loss:  7197.955078125\n",
      "Epoch: 30. Training data loss:  7197.9443359375\n",
      "Epoch: 31. Training data loss:  7197.94970703125\n",
      "Epoch: 32. Training data loss:  7197.95263671875\n",
      "Epoch: 33. Training data loss:  7197.951171875\n",
      "Epoch: 34. Training data loss:  7197.9501953125\n",
      "Epoch: 35. Training data loss:  7197.94775390625\n",
      "Epoch: 36. Training data loss:  7197.9521484375\n",
      "Epoch: 37. Training data loss:  7197.9482421875\n",
      "Epoch: 38. Training data loss:  7197.9384765625\n",
      "Epoch: 39. Training data loss:  7197.94677734375\n",
      "Epoch: 40. Training data loss:  7197.9443359375\n",
      "Epoch: 41. Training data loss:  7197.947265625\n",
      "Epoch: 42. Training data loss:  7197.95947265625\n",
      "Epoch: 43. Training data loss:  7197.95068359375\n",
      "Epoch: 44. Training data loss:  7197.94921875\n",
      "Epoch: 45. Training data loss:  7197.955078125\n",
      "Epoch: 46. Training data loss:  7197.953125\n",
      "Epoch: 47. Training data loss:  7197.93994140625\n",
      "Epoch: 48. Training data loss:  7197.94873046875\n",
      "Epoch: 49. Training data loss:  7197.94580078125\n",
      "Epoch: 50. Training data loss:  7197.9453125\n",
      "Epoch: 51. Training data loss:  7197.94921875\n",
      "Epoch: 52. Training data loss:  7197.93798828125\n",
      "Epoch: 53. Training data loss:  7197.9501953125\n",
      "Epoch: 54. Training data loss:  7197.95947265625\n",
      "Epoch: 55. Training data loss:  7197.943359375\n",
      "Epoch: 56. Training data loss:  7197.9443359375\n",
      "Epoch: 57. Training data loss:  7197.943359375\n",
      "Epoch: 58. Training data loss:  7197.94873046875\n",
      "Epoch: 59. Training data loss:  7197.951171875\n",
      "Accuracy found: 1.0\n",
      "60 0.0001 0.3 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 21\n",
      "Epoch: 0. Training data loss:  7133.05517578125\n",
      "Epoch: 1. Training data loss:  7174.51171875\n",
      "Epoch: 2. Training data loss:  7196.462890625\n",
      "Epoch: 3. Training data loss:  7197.9453125\n",
      "Epoch: 4. Training data loss:  7197.947265625\n",
      "Epoch: 5. Training data loss:  7197.9580078125\n",
      "Epoch: 6. Training data loss:  7197.9560546875\n",
      "Epoch: 7. Training data loss:  7197.95263671875\n",
      "Epoch: 8. Training data loss:  7197.94140625\n",
      "Epoch: 9. Training data loss:  7197.9599609375\n",
      "Epoch: 10. Training data loss:  7197.94677734375\n",
      "Epoch: 11. Training data loss:  7197.95703125\n",
      "Epoch: 12. Training data loss:  7197.9482421875\n",
      "Epoch: 13. Training data loss:  7197.9560546875\n",
      "Epoch: 14. Training data loss:  7197.9326171875\n",
      "Epoch: 15. Training data loss:  7197.9521484375\n",
      "Epoch: 16. Training data loss:  7197.9423828125\n",
      "Epoch: 17. Training data loss:  7197.94482421875\n",
      "Epoch: 18. Training data loss:  7197.94580078125\n",
      "Epoch: 19. Training data loss:  7197.955078125\n",
      "Epoch: 20. Training data loss:  7197.95849609375\n",
      "Epoch: 21. Training data loss:  7197.9375\n",
      "Epoch: 22. Training data loss:  7197.951171875\n",
      "Epoch: 23. Training data loss:  7197.94580078125\n",
      "Epoch: 24. Training data loss:  7197.95458984375\n",
      "Epoch: 25. Training data loss:  7197.95947265625\n",
      "Epoch: 26. Training data loss:  7197.9443359375\n",
      "Epoch: 27. Training data loss:  7197.94775390625\n",
      "Epoch: 28. Training data loss:  7197.958984375\n",
      "Epoch: 29. Training data loss:  7197.94677734375\n",
      "Epoch: 30. Training data loss:  7197.953125\n",
      "Epoch: 31. Training data loss:  7197.93994140625\n",
      "Epoch: 32. Training data loss:  7197.9462890625\n",
      "Epoch: 33. Training data loss:  7197.95458984375\n",
      "Epoch: 34. Training data loss:  7197.9462890625\n",
      "Epoch: 35. Training data loss:  7197.943359375\n",
      "Epoch: 36. Training data loss:  7197.95166015625\n",
      "Epoch: 37. Training data loss:  7197.943359375\n",
      "Epoch: 38. Training data loss:  7197.95849609375\n",
      "Epoch: 39. Training data loss:  7197.93896484375\n",
      "Epoch: 40. Training data loss:  7197.95751953125\n",
      "Epoch: 41. Training data loss:  7197.94189453125\n",
      "Epoch: 42. Training data loss:  7197.94970703125\n",
      "Epoch: 43. Training data loss:  7197.9453125\n",
      "Epoch: 44. Training data loss:  7197.947265625\n",
      "Epoch: 45. Training data loss:  7197.94091796875\n",
      "Epoch: 46. Training data loss:  7197.9423828125\n",
      "Epoch: 47. Training data loss:  7197.9384765625\n",
      "Epoch: 48. Training data loss:  7197.95263671875\n",
      "Epoch: 49. Training data loss:  7197.93798828125\n",
      "Epoch: 50. Training data loss:  7197.9404296875\n",
      "Epoch: 51. Training data loss:  7197.94384765625\n",
      "Epoch: 52. Training data loss:  7197.93994140625\n",
      "Epoch: 53. Training data loss:  7197.94873046875\n",
      "Epoch: 54. Training data loss:  7197.95556640625\n",
      "Epoch: 55. Training data loss:  7197.9501953125\n",
      "Epoch: 56. Training data loss:  7197.9443359375\n",
      "Epoch: 57. Training data loss:  7197.95263671875\n",
      "Epoch: 58. Training data loss:  7197.93701171875\n",
      "Epoch: 59. Training data loss:  7197.9521484375\n",
      "Accuracy found: 1.0\n",
      "60 0.0001 0.3 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 22\n",
      "Epoch: 0. Training data loss:  5261.86083984375\n",
      "Epoch: 1. Training data loss:  5232.93408203125\n",
      "Epoch: 2. Training data loss:  5191.2880859375\n",
      "Epoch: 3. Training data loss:  5180.3896484375\n",
      "Epoch: 4. Training data loss:  5147.68310546875\n",
      "Epoch: 5. Training data loss:  5123.13623046875\n",
      "Epoch: 6. Training data loss:  5089.1416015625\n",
      "Epoch: 7. Training data loss:  5081.41943359375\n",
      "Epoch: 8. Training data loss:  5038.2880859375\n",
      "Epoch: 9. Training data loss:  5015.0419921875\n",
      "Epoch: 10. Training data loss:  4982.8466796875\n",
      "Epoch: 11. Training data loss:  4969.79736328125\n",
      "Epoch: 12. Training data loss:  4941.63818359375\n",
      "Epoch: 13. Training data loss:  4917.70947265625\n",
      "Epoch: 14. Training data loss:  4904.36962890625\n",
      "Epoch: 15. Training data loss:  4876.02734375\n",
      "Epoch: 16. Training data loss:  4856.96240234375\n",
      "Epoch: 17. Training data loss:  4834.80859375\n",
      "Epoch: 18. Training data loss:  4822.01025390625\n",
      "Epoch: 19. Training data loss:  4805.7744140625\n",
      "Epoch: 20. Training data loss:  4783.04296875\n",
      "Epoch: 21. Training data loss:  4769.42138671875\n",
      "Epoch: 22. Training data loss:  4737.67626953125\n",
      "Epoch: 23. Training data loss:  4755.62890625\n",
      "Epoch: 24. Training data loss:  4726.5029296875\n",
      "Epoch: 25. Training data loss:  4710.59033203125\n",
      "Epoch: 26. Training data loss:  4681.02978515625\n",
      "Epoch: 27. Training data loss:  4672.837890625\n",
      "Epoch: 28. Training data loss:  4643.689453125\n",
      "Epoch: 29. Training data loss:  4635.95458984375\n",
      "Epoch: 30. Training data loss:  4621.8642578125\n",
      "Epoch: 31. Training data loss:  4626.8759765625\n",
      "Epoch: 32. Training data loss:  4610.1181640625\n",
      "Epoch: 33. Training data loss:  4596.4375\n",
      "Epoch: 34. Training data loss:  4578.623046875\n",
      "Epoch: 35. Training data loss:  4568.82177734375\n",
      "Epoch: 36. Training data loss:  4569.3583984375\n",
      "Epoch: 37. Training data loss:  4531.462890625\n",
      "Epoch: 38. Training data loss:  4530.82470703125\n",
      "Epoch: 39. Training data loss:  4527.4658203125\n",
      "Accuracy found: 2.07\n",
      "40 0.0001 0.01 MultiMarginLoss() 10 0\n",
      "Starting iteration, 23\n",
      "Epoch: 0. Training data loss:  7483.8984375\n",
      "Epoch: 1. Training data loss:  7474.60595703125\n",
      "Epoch: 2. Training data loss:  7451.576171875\n",
      "Epoch: 3. Training data loss:  7446.2197265625\n",
      "Epoch: 4. Training data loss:  7429.25927734375\n",
      "Epoch: 5. Training data loss:  7423.07958984375\n",
      "Epoch: 6. Training data loss:  7412.3486328125\n",
      "Epoch: 7. Training data loss:  7399.31005859375\n",
      "Epoch: 8. Training data loss:  7392.9296875\n",
      "Epoch: 9. Training data loss:  7385.07177734375\n",
      "Epoch: 10. Training data loss:  7376.29443359375\n",
      "Epoch: 11. Training data loss:  7367.345703125\n",
      "Epoch: 12. Training data loss:  7362.44775390625\n",
      "Epoch: 13. Training data loss:  7355.6728515625\n",
      "Epoch: 14. Training data loss:  7349.994140625\n",
      "Epoch: 15. Training data loss:  7348.974609375\n",
      "Epoch: 16. Training data loss:  7332.69189453125\n",
      "Epoch: 17. Training data loss:  7327.009765625\n",
      "Epoch: 18. Training data loss:  7319.0595703125\n",
      "Epoch: 19. Training data loss:  7318.150390625\n",
      "Accuracy found: 1.15\n",
      "20 0.0001 0.001 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 24\n",
      "Epoch: 0. Training data loss:  6470.78515625\n",
      "Epoch: 1. Training data loss:  6015.51318359375\n",
      "Epoch: 2. Training data loss:  5858.41552734375\n",
      "Epoch: 3. Training data loss:  5765.7060546875\n",
      "Epoch: 4. Training data loss:  5691.41845703125\n",
      "Epoch: 5. Training data loss:  5632.35595703125\n",
      "Epoch: 6. Training data loss:  5583.4228515625\n",
      "Epoch: 7. Training data loss:  5551.5126953125\n",
      "Epoch: 8. Training data loss:  5513.7060546875\n",
      "Epoch: 9. Training data loss:  5495.75390625\n",
      "Epoch: 10. Training data loss:  5462.51171875\n",
      "Epoch: 11. Training data loss:  5445.06005859375\n",
      "Epoch: 12. Training data loss:  5428.5458984375\n",
      "Epoch: 13. Training data loss:  5400.49755859375\n",
      "Epoch: 14. Training data loss:  5388.4755859375\n",
      "Epoch: 15. Training data loss:  5379.78955078125\n",
      "Epoch: 16. Training data loss:  5367.73388671875\n",
      "Epoch: 17. Training data loss:  5362.0830078125\n",
      "Epoch: 18. Training data loss:  5340.7763671875\n",
      "Epoch: 19. Training data loss:  5328.0380859375\n",
      "Epoch: 20. Training data loss:  5315.0830078125\n",
      "Epoch: 21. Training data loss:  5312.0341796875\n",
      "Epoch: 22. Training data loss:  5308.9453125\n",
      "Epoch: 23. Training data loss:  5300.1728515625\n",
      "Epoch: 24. Training data loss:  5295.50634765625\n",
      "Epoch: 25. Training data loss:  5282.74072265625\n",
      "Epoch: 26. Training data loss:  5279.6064453125\n",
      "Epoch: 27. Training data loss:  5270.9775390625\n",
      "Epoch: 28. Training data loss:  5269.86767578125\n",
      "Epoch: 29. Training data loss:  5259.57568359375\n",
      "Epoch: 30. Training data loss:  5250.5224609375\n",
      "Epoch: 31. Training data loss:  5249.82861328125\n",
      "Epoch: 32. Training data loss:  5244.4091796875\n",
      "Epoch: 33. Training data loss:  5232.34130859375\n",
      "Epoch: 34. Training data loss:  5229.861328125\n",
      "Epoch: 35. Training data loss:  5227.93701171875\n",
      "Epoch: 36. Training data loss:  5229.095703125\n",
      "Epoch: 37. Training data loss:  5213.88525390625\n",
      "Epoch: 38. Training data loss:  5224.732421875\n",
      "Epoch: 39. Training data loss:  5206.9482421875\n",
      "Accuracy found: 22.73\n",
      "40 0.001 0 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 25\n",
      "Epoch: 0. Training data loss:  410.661865234375\n",
      "Epoch: 1. Training data loss:  409.7693176269531\n",
      "Epoch: 2. Training data loss:  409.16363525390625\n",
      "Epoch: 3. Training data loss:  406.4427795410156\n",
      "Epoch: 4. Training data loss:  406.8039855957031\n",
      "Epoch: 5. Training data loss:  405.41925048828125\n",
      "Epoch: 6. Training data loss:  405.33514404296875\n",
      "Epoch: 7. Training data loss:  403.8382568359375\n",
      "Epoch: 8. Training data loss:  403.1406555175781\n",
      "Epoch: 9. Training data loss:  400.83001708984375\n",
      "Epoch: 10. Training data loss:  400.6335754394531\n",
      "Epoch: 11. Training data loss:  398.2332763671875\n",
      "Epoch: 12. Training data loss:  399.52099609375\n",
      "Epoch: 13. Training data loss:  397.0543518066406\n",
      "Epoch: 14. Training data loss:  397.0870666503906\n",
      "Epoch: 15. Training data loss:  396.8534240722656\n",
      "Epoch: 16. Training data loss:  394.6783142089844\n",
      "Epoch: 17. Training data loss:  394.6820983886719\n",
      "Epoch: 18. Training data loss:  393.6363830566406\n",
      "Epoch: 19. Training data loss:  393.4376220703125\n",
      "Epoch: 20. Training data loss:  392.48760986328125\n",
      "Epoch: 21. Training data loss:  391.1174621582031\n",
      "Epoch: 22. Training data loss:  390.3193054199219\n",
      "Epoch: 23. Training data loss:  389.6128845214844\n",
      "Epoch: 24. Training data loss:  388.4677734375\n",
      "Epoch: 25. Training data loss:  388.6192932128906\n",
      "Epoch: 26. Training data loss:  387.41961669921875\n",
      "Epoch: 27. Training data loss:  386.27984619140625\n",
      "Epoch: 28. Training data loss:  384.9360046386719\n",
      "Epoch: 29. Training data loss:  385.3760986328125\n",
      "Epoch: 30. Training data loss:  384.6262512207031\n",
      "Epoch: 31. Training data loss:  382.1357421875\n",
      "Epoch: 32. Training data loss:  384.3575134277344\n",
      "Epoch: 33. Training data loss:  384.0122985839844\n",
      "Epoch: 34. Training data loss:  381.4841613769531\n",
      "Epoch: 35. Training data loss:  382.2928466796875\n",
      "Epoch: 36. Training data loss:  380.1465759277344\n",
      "Epoch: 37. Training data loss:  381.1915283203125\n",
      "Epoch: 38. Training data loss:  379.1547546386719\n",
      "Epoch: 39. Training data loss:  380.2796630859375\n",
      "Accuracy found: 1.87\n",
      "40 0.0001 0.01 MultiMarginLoss() 128 0\n",
      "Starting iteration, 26\n",
      "Epoch: 0. Training data loss:  235.91432189941406\n",
      "Epoch: 1. Training data loss:  217.5055694580078\n",
      "Epoch: 2. Training data loss:  209.21226501464844\n",
      "Epoch: 3. Training data loss:  206.85348510742188\n",
      "Epoch: 4. Training data loss:  209.4033660888672\n",
      "Epoch: 5. Training data loss:  207.03565979003906\n",
      "Epoch: 6. Training data loss:  208.22340393066406\n",
      "Epoch: 7. Training data loss:  208.7658233642578\n",
      "Epoch: 8. Training data loss:  210.57821655273438\n",
      "Epoch: 9. Training data loss:  209.7997589111328\n",
      "Epoch: 10. Training data loss:  210.14137268066406\n",
      "Epoch: 11. Training data loss:  210.2251739501953\n",
      "Epoch: 12. Training data loss:  210.26939392089844\n",
      "Epoch: 13. Training data loss:  211.7761688232422\n",
      "Epoch: 14. Training data loss:  210.2510986328125\n",
      "Epoch: 15. Training data loss:  211.62705993652344\n",
      "Epoch: 16. Training data loss:  211.21556091308594\n",
      "Epoch: 17. Training data loss:  212.150146484375\n",
      "Epoch: 18. Training data loss:  211.79898071289062\n",
      "Epoch: 19. Training data loss:  209.7711944580078\n",
      "Epoch: 20. Training data loss:  210.78378295898438\n",
      "Epoch: 21. Training data loss:  210.56240844726562\n",
      "Epoch: 22. Training data loss:  210.6175079345703\n",
      "Epoch: 23. Training data loss:  211.2102508544922\n",
      "Epoch: 24. Training data loss:  210.44415283203125\n",
      "Epoch: 25. Training data loss:  212.200439453125\n",
      "Epoch: 26. Training data loss:  212.2982635498047\n",
      "Epoch: 27. Training data loss:  212.29238891601562\n",
      "Epoch: 28. Training data loss:  210.78997802734375\n",
      "Epoch: 29. Training data loss:  211.4608917236328\n",
      "Epoch: 30. Training data loss:  210.36997985839844\n",
      "Epoch: 31. Training data loss:  210.09019470214844\n",
      "Epoch: 32. Training data loss:  210.93734741210938\n",
      "Epoch: 33. Training data loss:  210.52197265625\n",
      "Epoch: 34. Training data loss:  210.398681640625\n",
      "Epoch: 35. Training data loss:  210.63902282714844\n",
      "Epoch: 36. Training data loss:  210.32350158691406\n",
      "Epoch: 37. Training data loss:  210.52938842773438\n",
      "Epoch: 38. Training data loss:  210.5756378173828\n",
      "Epoch: 39. Training data loss:  209.87448120117188\n",
      "Accuracy found: 8.13\n",
      "40 0.01 0.001 MultiMarginLoss() 128 1\n",
      "Starting iteration, 27\n",
      "Epoch: 0. Training data loss:  23028.693359375\n",
      "Epoch: 1. Training data loss:  23027.22265625\n",
      "Epoch: 2. Training data loss:  23027.77734375\n",
      "Epoch: 3. Training data loss:  23028.951171875\n",
      "Epoch: 4. Training data loss:  23027.220703125\n",
      "Epoch: 5. Training data loss:  23026.728515625\n",
      "Epoch: 6. Training data loss:  23027.05859375\n",
      "Epoch: 7. Training data loss:  23027.265625\n",
      "Epoch: 8. Training data loss:  23027.439453125\n",
      "Epoch: 9. Training data loss:  23027.806640625\n",
      "Epoch: 10. Training data loss:  23027.064453125\n",
      "Epoch: 11. Training data loss:  23027.361328125\n",
      "Epoch: 12. Training data loss:  23026.580078125\n",
      "Epoch: 13. Training data loss:  23027.666015625\n",
      "Epoch: 14. Training data loss:  23027.953125\n",
      "Epoch: 15. Training data loss:  23028.220703125\n",
      "Epoch: 16. Training data loss:  23027.4453125\n",
      "Epoch: 17. Training data loss:  23027.341796875\n",
      "Epoch: 18. Training data loss:  23027.1484375\n",
      "Epoch: 19. Training data loss:  23026.89453125\n",
      "Epoch: 20. Training data loss:  23027.82421875\n",
      "Epoch: 21. Training data loss:  23027.75390625\n",
      "Epoch: 22. Training data loss:  23026.169921875\n",
      "Epoch: 23. Training data loss:  23027.34375\n",
      "Epoch: 24. Training data loss:  23027.6328125\n",
      "Epoch: 25. Training data loss:  23028.029296875\n",
      "Epoch: 26. Training data loss:  23026.8515625\n",
      "Epoch: 27. Training data loss:  23027.05078125\n",
      "Epoch: 28. Training data loss:  23027.513671875\n",
      "Epoch: 29. Training data loss:  23027.25\n",
      "Epoch: 30. Training data loss:  23027.708984375\n",
      "Epoch: 31. Training data loss:  23026.82421875\n",
      "Epoch: 32. Training data loss:  23027.5234375\n",
      "Epoch: 33. Training data loss:  23027.916015625\n",
      "Epoch: 34. Training data loss:  23027.9140625\n",
      "Epoch: 35. Training data loss:  23027.396484375\n",
      "Epoch: 36. Training data loss:  23026.60546875\n",
      "Epoch: 37. Training data loss:  23026.888671875\n",
      "Epoch: 38. Training data loss:  23027.271484375\n",
      "Epoch: 39. Training data loss:  23028.02734375\n",
      "Epoch: 40. Training data loss:  23027.685546875\n",
      "Epoch: 41. Training data loss:  23026.662109375\n",
      "Epoch: 42. Training data loss:  23027.548828125\n",
      "Epoch: 43. Training data loss:  23026.873046875\n",
      "Epoch: 44. Training data loss:  23027.634765625\n",
      "Epoch: 45. Training data loss:  23027.8046875\n",
      "Epoch: 46. Training data loss:  23027.935546875\n",
      "Epoch: 47. Training data loss:  23027.900390625\n",
      "Epoch: 48. Training data loss:  23028.455078125\n",
      "Epoch: 49. Training data loss:  23027.775390625\n",
      "Epoch: 50. Training data loss:  23026.91796875\n",
      "Epoch: 51. Training data loss:  23026.931640625\n",
      "Epoch: 52. Training data loss:  23027.7421875\n",
      "Epoch: 53. Training data loss:  23027.703125\n",
      "Epoch: 54. Training data loss:  23027.01171875\n",
      "Epoch: 55. Training data loss:  23027.3125\n",
      "Epoch: 56. Training data loss:  23027.791015625\n",
      "Epoch: 57. Training data loss:  23027.681640625\n",
      "Epoch: 58. Training data loss:  23027.212890625\n",
      "Epoch: 59. Training data loss:  23028.3046875\n",
      "Accuracy found: 1.0\n",
      "60 0.01 0.3 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 28\n",
      "Epoch: 0. Training data loss:  3177.65234375\n",
      "Epoch: 1. Training data loss:  3003.37451171875\n",
      "Epoch: 2. Training data loss:  2953.89111328125\n",
      "Epoch: 3. Training data loss:  2922.671630859375\n",
      "Epoch: 4. Training data loss:  2902.0302734375\n",
      "Epoch: 5. Training data loss:  2882.625732421875\n",
      "Epoch: 6. Training data loss:  2869.39892578125\n",
      "Epoch: 7. Training data loss:  2854.868408203125\n",
      "Epoch: 8. Training data loss:  2845.7587890625\n",
      "Epoch: 9. Training data loss:  2837.5908203125\n",
      "Epoch: 10. Training data loss:  2831.205810546875\n",
      "Epoch: 11. Training data loss:  2823.362060546875\n",
      "Epoch: 12. Training data loss:  2824.30615234375\n",
      "Epoch: 13. Training data loss:  2810.653076171875\n",
      "Epoch: 14. Training data loss:  2808.350341796875\n",
      "Epoch: 15. Training data loss:  2802.211181640625\n",
      "Epoch: 16. Training data loss:  2802.161376953125\n",
      "Epoch: 17. Training data loss:  2796.59228515625\n",
      "Epoch: 18. Training data loss:  2790.124755859375\n",
      "Epoch: 19. Training data loss:  2787.59033203125\n",
      "Accuracy found: 17.79\n",
      "20 0.01 0 CrossEntropyLoss() 64 1\n",
      "Starting iteration, 29\n",
      "Epoch: 0. Training data loss:  7340.62451171875\n",
      "Epoch: 1. Training data loss:  7561.41162109375\n",
      "Epoch: 2. Training data loss:  7535.1650390625\n",
      "Epoch: 3. Training data loss:  7430.50341796875\n",
      "Epoch: 4. Training data loss:  7510.435546875\n",
      "Epoch: 5. Training data loss:  7486.5458984375\n",
      "Epoch: 6. Training data loss:  7569.0966796875\n",
      "Epoch: 7. Training data loss:  7483.38623046875\n",
      "Epoch: 8. Training data loss:  7558.19970703125\n",
      "Epoch: 9. Training data loss:  7527.70947265625\n",
      "Epoch: 10. Training data loss:  7520.787109375\n",
      "Epoch: 11. Training data loss:  7596.52197265625\n",
      "Epoch: 12. Training data loss:  7522.72802734375\n",
      "Epoch: 13. Training data loss:  7602.59423828125\n",
      "Epoch: 14. Training data loss:  7577.86865234375\n",
      "Epoch: 15. Training data loss:  7592.30859375\n",
      "Epoch: 16. Training data loss:  7552.3046875\n",
      "Epoch: 17. Training data loss:  7688.02978515625\n",
      "Epoch: 18. Training data loss:  7636.2548828125\n",
      "Epoch: 19. Training data loss:  7616.80078125\n",
      "Epoch: 20. Training data loss:  7701.96337890625\n",
      "Epoch: 21. Training data loss:  7594.41162109375\n",
      "Epoch: 22. Training data loss:  7672.05224609375\n",
      "Epoch: 23. Training data loss:  7664.96923828125\n",
      "Epoch: 24. Training data loss:  7670.5283203125\n",
      "Epoch: 25. Training data loss:  7612.8701171875\n",
      "Epoch: 26. Training data loss:  7618.49658203125\n",
      "Epoch: 27. Training data loss:  7640.05322265625\n",
      "Epoch: 28. Training data loss:  7552.20458984375\n",
      "Epoch: 29. Training data loss:  7619.1201171875\n",
      "Epoch: 30. Training data loss:  7646.43212890625\n",
      "Epoch: 31. Training data loss:  7608.10986328125\n",
      "Epoch: 32. Training data loss:  7654.26806640625\n",
      "Epoch: 33. Training data loss:  7589.36962890625\n",
      "Epoch: 34. Training data loss:  7681.9951171875\n",
      "Epoch: 35. Training data loss:  7535.498046875\n",
      "Epoch: 36. Training data loss:  7623.74853515625\n",
      "Epoch: 37. Training data loss:  7532.3994140625\n",
      "Epoch: 38. Training data loss:  7687.14990234375\n",
      "Epoch: 39. Training data loss:  7606.67822265625\n",
      "Accuracy found: 3.04\n",
      "40 0.1 0 CrossEntropyLoss() 32 1\n",
      "(23.16, (60, 0.001, 0, CrossEntropyLoss(), 32, 1))\n"
     ]
    }
   ],
   "source": [
    "print(rand_search('mln',param_dist, classes, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [128, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  2720794.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [80, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Training data loss:  3486765809664.0\n",
      "Epoch: 2. Training data loss:  100631008247808.0\n",
      "Epoch: 3. Training data loss:  278058665771008.0\n",
      "Epoch: 4. Training data loss:  304886709223424.0\n",
      "Epoch: 5. Training data loss:  548464102473728.0\n",
      "Epoch: 6. Training data loss:  752293519556608.0\n",
      "Epoch: 7. Training data loss:  1686764385206272.0\n",
      "Epoch: 8. Training data loss:  1808727363551232.0\n",
      "Epoch: 9. Training data loss:  1779023235514368.0\n",
      "Epoch: 10. Training data loss:  1432573456678912.0\n",
      "Epoch: 11. Training data loss:  1379921989468160.0\n",
      "Epoch: 12. Training data loss:  1336105672638464.0\n",
      "Epoch: 13. Training data loss:  1130120249081856.0\n",
      "Epoch: 14. Training data loss:  964702469357568.0\n",
      "Epoch: 15. Training data loss:  760038285115392.0\n",
      "Epoch: 16. Training data loss:  523457024491520.0\n",
      "Epoch: 17. Training data loss:  502805144010752.0\n",
      "Epoch: 18. Training data loss:  427652712235008.0\n",
      "Epoch: 19. Training data loss:  384280555421696.0\n",
      "Accuracy found: 1.0\n",
      "20 0.1 0 MultiMarginLoss() 128 1\n",
      "Starting iteration, 1\n",
      "Epoch: 0. Training data loss:  3504.120361328125\n",
      "Epoch: 1. Training data loss:  3329.7314453125\n",
      "Epoch: 2. Training data loss:  3238.089111328125\n",
      "Epoch: 3. Training data loss:  3172.611572265625\n",
      "Epoch: 4. Training data loss:  3121.318603515625\n",
      "Epoch: 5. Training data loss:  3080.91455078125\n",
      "Epoch: 6. Training data loss:  3043.144287109375\n",
      "Epoch: 7. Training data loss:  3011.29345703125\n",
      "Epoch: 8. Training data loss:  2985.88037109375\n",
      "Epoch: 9. Training data loss:  2956.849365234375\n",
      "Epoch: 10. Training data loss:  2935.28955078125\n",
      "Epoch: 11. Training data loss:  2914.284423828125\n",
      "Epoch: 12. Training data loss:  2894.2900390625\n",
      "Epoch: 13. Training data loss:  2874.8095703125\n",
      "Epoch: 14. Training data loss:  2861.35693359375\n",
      "Epoch: 15. Training data loss:  2840.254150390625\n",
      "Epoch: 16. Training data loss:  2832.021728515625\n",
      "Epoch: 17. Training data loss:  2817.2939453125\n",
      "Epoch: 18. Training data loss:  2802.5703125\n",
      "Epoch: 19. Training data loss:  2794.570556640625\n",
      "Epoch: 20. Training data loss:  2777.4931640625\n",
      "Epoch: 21. Training data loss:  2768.734375\n",
      "Epoch: 22. Training data loss:  2760.698974609375\n",
      "Epoch: 23. Training data loss:  2750.0712890625\n",
      "Epoch: 24. Training data loss:  2739.13671875\n",
      "Epoch: 25. Training data loss:  2726.388671875\n",
      "Epoch: 26. Training data loss:  2716.02392578125\n",
      "Epoch: 27. Training data loss:  2712.102783203125\n",
      "Epoch: 28. Training data loss:  2703.7060546875\n",
      "Epoch: 29. Training data loss:  2693.0341796875\n",
      "Epoch: 30. Training data loss:  2684.619873046875\n",
      "Epoch: 31. Training data loss:  2677.172607421875\n",
      "Epoch: 32. Training data loss:  2666.900390625\n",
      "Epoch: 33. Training data loss:  2662.315673828125\n",
      "Epoch: 34. Training data loss:  2652.35205078125\n",
      "Epoch: 35. Training data loss:  2646.429443359375\n",
      "Epoch: 36. Training data loss:  2634.67724609375\n",
      "Epoch: 37. Training data loss:  2628.425048828125\n",
      "Epoch: 38. Training data loss:  2621.658935546875\n",
      "Epoch: 39. Training data loss:  2614.425048828125\n",
      "Accuracy found: 22.46\n",
      "40 0.0001 0.001 CrossEntropyLoss() 64 1\n",
      "Starting iteration, 2\n",
      "Epoch: 0. Training data loss:  1802.1697998046875\n",
      "Epoch: 1. Training data loss:  1802.0770263671875\n",
      "Epoch: 2. Training data loss:  1802.165283203125\n",
      "Epoch: 3. Training data loss:  1802.0706787109375\n",
      "Epoch: 4. Training data loss:  1802.0010986328125\n",
      "Epoch: 5. Training data loss:  1802.05322265625\n",
      "Epoch: 6. Training data loss:  1801.83203125\n",
      "Epoch: 7. Training data loss:  1801.9193115234375\n",
      "Epoch: 8. Training data loss:  1801.91748046875\n",
      "Epoch: 9. Training data loss:  1801.800048828125\n",
      "Epoch: 10. Training data loss:  1801.83154296875\n",
      "Epoch: 11. Training data loss:  1801.783203125\n",
      "Epoch: 12. Training data loss:  1801.8143310546875\n",
      "Epoch: 13. Training data loss:  1801.833984375\n",
      "Epoch: 14. Training data loss:  1801.72119140625\n",
      "Epoch: 15. Training data loss:  1801.6728515625\n",
      "Epoch: 16. Training data loss:  1801.681640625\n",
      "Epoch: 17. Training data loss:  1801.85009765625\n",
      "Epoch: 18. Training data loss:  1801.6802978515625\n",
      "Epoch: 19. Training data loss:  1801.75390625\n",
      "Epoch: 20. Training data loss:  1801.5347900390625\n",
      "Epoch: 21. Training data loss:  1801.54541015625\n",
      "Epoch: 22. Training data loss:  1801.61474609375\n",
      "Epoch: 23. Training data loss:  1801.56884765625\n",
      "Epoch: 24. Training data loss:  1801.4937744140625\n",
      "Epoch: 25. Training data loss:  1801.5697021484375\n",
      "Epoch: 26. Training data loss:  1801.6126708984375\n",
      "Epoch: 27. Training data loss:  1801.4947509765625\n",
      "Epoch: 28. Training data loss:  1801.48291015625\n",
      "Epoch: 29. Training data loss:  1801.4508056640625\n",
      "Epoch: 30. Training data loss:  1801.42431640625\n",
      "Epoch: 31. Training data loss:  1801.445556640625\n",
      "Epoch: 32. Training data loss:  1801.347900390625\n",
      "Epoch: 33. Training data loss:  1801.2974853515625\n",
      "Epoch: 34. Training data loss:  1801.2552490234375\n",
      "Epoch: 35. Training data loss:  1801.3311767578125\n",
      "Epoch: 36. Training data loss:  1801.3165283203125\n",
      "Epoch: 37. Training data loss:  1801.3404541015625\n",
      "Epoch: 38. Training data loss:  1801.289306640625\n",
      "Epoch: 39. Training data loss:  1801.1385498046875\n",
      "Accuracy found: 0.84\n",
      "40 0.0001 0.01 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 3\n",
      "Epoch: 0. Training data loss:  387.1922302246094\n",
      "Epoch: 1. Training data loss:  387.09375\n",
      "Epoch: 2. Training data loss:  387.09954833984375\n",
      "Epoch: 3. Training data loss:  387.09344482421875\n",
      "Epoch: 4. Training data loss:  387.09075927734375\n",
      "Epoch: 5. Training data loss:  387.09307861328125\n",
      "Epoch: 6. Training data loss:  387.0924987792969\n",
      "Epoch: 7. Training data loss:  387.0919494628906\n",
      "Epoch: 8. Training data loss:  387.0939025878906\n",
      "Epoch: 9. Training data loss:  387.0939025878906\n",
      "Epoch: 10. Training data loss:  387.09356689453125\n",
      "Epoch: 11. Training data loss:  387.093994140625\n",
      "Epoch: 12. Training data loss:  387.09429931640625\n",
      "Epoch: 13. Training data loss:  387.0937805175781\n",
      "Epoch: 14. Training data loss:  387.0939025878906\n",
      "Epoch: 15. Training data loss:  387.09356689453125\n",
      "Epoch: 16. Training data loss:  387.0936279296875\n",
      "Epoch: 17. Training data loss:  387.0939636230469\n",
      "Epoch: 18. Training data loss:  387.0936279296875\n",
      "Epoch: 19. Training data loss:  387.0939636230469\n",
      "Epoch: 20. Training data loss:  387.0937805175781\n",
      "Epoch: 21. Training data loss:  387.093994140625\n",
      "Epoch: 22. Training data loss:  387.0937194824219\n",
      "Epoch: 23. Training data loss:  387.09381103515625\n",
      "Epoch: 24. Training data loss:  387.0938720703125\n",
      "Epoch: 25. Training data loss:  387.093994140625\n",
      "Epoch: 26. Training data loss:  387.0937805175781\n",
      "Epoch: 27. Training data loss:  387.0938415527344\n",
      "Epoch: 28. Training data loss:  387.0938415527344\n",
      "Epoch: 29. Training data loss:  387.09381103515625\n",
      "Epoch: 30. Training data loss:  387.0938415527344\n",
      "Epoch: 31. Training data loss:  387.0937805175781\n",
      "Epoch: 32. Training data loss:  387.09356689453125\n",
      "Epoch: 33. Training data loss:  387.09375\n",
      "Epoch: 34. Training data loss:  387.09375\n",
      "Epoch: 35. Training data loss:  387.09368896484375\n",
      "Epoch: 36. Training data loss:  387.0936584472656\n",
      "Epoch: 37. Training data loss:  387.0938415527344\n",
      "Epoch: 38. Training data loss:  387.0938415527344\n",
      "Epoch: 39. Training data loss:  387.09368896484375\n",
      "Accuracy found: 1.0\n",
      "40 0.01 0.3 MultiMarginLoss() 128 0\n",
      "Starting iteration, 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [10, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  4948.95849609375\n",
      "Epoch: 1. Training data loss:  4946.5546875\n",
      "Epoch: 2. Training data loss:  4942.55908203125\n",
      "Epoch: 3. Training data loss:  4939.14697265625\n",
      "Epoch: 4. Training data loss:  4933.81298828125\n",
      "Epoch: 5. Training data loss:  4922.79931640625\n",
      "Epoch: 6. Training data loss:  4905.4560546875\n",
      "Epoch: 7. Training data loss:  4865.3974609375\n",
      "Epoch: 8. Training data loss:  4792.05126953125\n",
      "Epoch: 9. Training data loss:  4705.30859375\n",
      "Epoch: 10. Training data loss:  4632.21240234375\n",
      "Epoch: 11. Training data loss:  4548.27587890625\n",
      "Epoch: 12. Training data loss:  4450.84912109375\n",
      "Epoch: 13. Training data loss:  4340.814453125\n",
      "Epoch: 14. Training data loss:  4263.99560546875\n",
      "Epoch: 15. Training data loss:  4215.439453125\n",
      "Epoch: 16. Training data loss:  4175.9755859375\n",
      "Epoch: 17. Training data loss:  4141.4375\n",
      "Epoch: 18. Training data loss:  4113.98388671875\n",
      "Epoch: 19. Training data loss:  4090.2998046875\n",
      "Epoch: 20. Training data loss:  4061.123046875\n",
      "Epoch: 21. Training data loss:  4040.887939453125\n",
      "Epoch: 22. Training data loss:  4025.326904296875\n",
      "Epoch: 23. Training data loss:  4009.470458984375\n",
      "Epoch: 24. Training data loss:  4008.99365234375\n",
      "Epoch: 25. Training data loss:  3982.029052734375\n",
      "Epoch: 26. Training data loss:  3967.6474609375\n",
      "Epoch: 27. Training data loss:  3969.2578125\n",
      "Epoch: 28. Training data loss:  3943.7822265625\n",
      "Epoch: 29. Training data loss:  3938.19580078125\n",
      "Epoch: 30. Training data loss:  3935.0400390625\n",
      "Epoch: 31. Training data loss:  3929.177978515625\n",
      "Epoch: 32. Training data loss:  3913.344482421875\n",
      "Epoch: 33. Training data loss:  3910.031982421875\n",
      "Epoch: 34. Training data loss:  3894.546630859375\n",
      "Epoch: 35. Training data loss:  3884.419189453125\n",
      "Epoch: 36. Training data loss:  3901.021484375\n",
      "Epoch: 37. Training data loss:  3894.56298828125\n",
      "Epoch: 38. Training data loss:  3896.9345703125\n",
      "Epoch: 39. Training data loss:  3881.54443359375\n",
      "Accuracy found: 3.02\n",
      "40 0.001 0.01 MultiMarginLoss() 10 0\n",
      "Starting iteration, 5\n",
      "Epoch: 0. Training data loss:  7200.92822265625\n",
      "Epoch: 1. Training data loss:  7199.83984375\n",
      "Epoch: 2. Training data loss:  7198.41357421875\n",
      "Epoch: 3. Training data loss:  7196.45703125\n",
      "Epoch: 4. Training data loss:  7194.16455078125\n",
      "Epoch: 5. Training data loss:  7191.51904296875\n",
      "Epoch: 6. Training data loss:  7188.08056640625\n",
      "Epoch: 7. Training data loss:  7184.23974609375\n",
      "Epoch: 8. Training data loss:  7178.46337890625\n",
      "Epoch: 9. Training data loss:  7169.833984375\n",
      "Epoch: 10. Training data loss:  7159.3505859375\n",
      "Epoch: 11. Training data loss:  7147.779296875\n",
      "Epoch: 12. Training data loss:  7134.9853515625\n",
      "Epoch: 13. Training data loss:  7117.5439453125\n",
      "Epoch: 14. Training data loss:  7106.08984375\n",
      "Epoch: 15. Training data loss:  7090.6923828125\n",
      "Epoch: 16. Training data loss:  7076.61767578125\n",
      "Epoch: 17. Training data loss:  7062.89501953125\n",
      "Epoch: 18. Training data loss:  7050.853515625\n",
      "Epoch: 19. Training data loss:  7038.47119140625\n",
      "Epoch: 20. Training data loss:  7025.7275390625\n",
      "Epoch: 21. Training data loss:  7014.6474609375\n",
      "Epoch: 22. Training data loss:  7005.51611328125\n",
      "Epoch: 23. Training data loss:  6996.4345703125\n",
      "Epoch: 24. Training data loss:  6984.7958984375\n",
      "Epoch: 25. Training data loss:  6973.17236328125\n",
      "Epoch: 26. Training data loss:  6962.83935546875\n",
      "Epoch: 27. Training data loss:  6956.03564453125\n",
      "Epoch: 28. Training data loss:  6942.98681640625\n",
      "Epoch: 29. Training data loss:  6932.7763671875\n",
      "Epoch: 30. Training data loss:  6922.48095703125\n",
      "Epoch: 31. Training data loss:  6910.55419921875\n",
      "Epoch: 32. Training data loss:  6903.01953125\n",
      "Epoch: 33. Training data loss:  6898.69775390625\n",
      "Epoch: 34. Training data loss:  6886.48095703125\n",
      "Epoch: 35. Training data loss:  6876.96728515625\n",
      "Epoch: 36. Training data loss:  6864.19189453125\n",
      "Epoch: 37. Training data loss:  6861.8896484375\n",
      "Epoch: 38. Training data loss:  6850.14404296875\n",
      "Epoch: 39. Training data loss:  6840.57275390625\n",
      "Accuracy found: 5.51\n",
      "40 0.001 0.001 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 6\n",
      "Epoch: 0. Training data loss:  386.95501708984375\n",
      "Epoch: 1. Training data loss:  387.0464172363281\n",
      "Epoch: 2. Training data loss:  387.0478210449219\n",
      "Epoch: 3. Training data loss:  387.03863525390625\n",
      "Epoch: 4. Training data loss:  387.0376281738281\n",
      "Epoch: 5. Training data loss:  387.0531311035156\n",
      "Epoch: 6. Training data loss:  387.021484375\n",
      "Epoch: 7. Training data loss:  386.9833984375\n",
      "Epoch: 8. Training data loss:  387.0191650390625\n",
      "Epoch: 9. Training data loss:  387.0638732910156\n",
      "Epoch: 10. Training data loss:  387.0415954589844\n",
      "Epoch: 11. Training data loss:  386.9969482421875\n",
      "Epoch: 12. Training data loss:  387.01202392578125\n",
      "Epoch: 13. Training data loss:  386.9732360839844\n",
      "Epoch: 14. Training data loss:  386.9972229003906\n",
      "Epoch: 15. Training data loss:  387.0395812988281\n",
      "Epoch: 16. Training data loss:  387.0289306640625\n",
      "Epoch: 17. Training data loss:  386.9765930175781\n",
      "Epoch: 18. Training data loss:  387.0635070800781\n",
      "Epoch: 19. Training data loss:  387.0498352050781\n",
      "Accuracy found: 1.03\n",
      "20 0.0001 0.3 MultiMarginLoss() 128 0\n",
      "Starting iteration, 7\n",
      "Epoch: 0. Training data loss:  1801.6217041015625\n",
      "Epoch: 1. Training data loss:  1801.5037841796875\n",
      "Epoch: 2. Training data loss:  1801.382080078125\n",
      "Epoch: 3. Training data loss:  1801.292236328125\n",
      "Epoch: 4. Training data loss:  1801.1654052734375\n",
      "Epoch: 5. Training data loss:  1801.070556640625\n",
      "Epoch: 6. Training data loss:  1800.910400390625\n",
      "Epoch: 7. Training data loss:  1800.8348388671875\n",
      "Epoch: 8. Training data loss:  1800.818603515625\n",
      "Epoch: 9. Training data loss:  1800.64453125\n",
      "Epoch: 10. Training data loss:  1800.556884765625\n",
      "Epoch: 11. Training data loss:  1800.361328125\n",
      "Epoch: 12. Training data loss:  1800.189697265625\n",
      "Epoch: 13. Training data loss:  1800.1021728515625\n",
      "Epoch: 14. Training data loss:  1800.01318359375\n",
      "Epoch: 15. Training data loss:  1799.7857666015625\n",
      "Epoch: 16. Training data loss:  1799.6160888671875\n",
      "Epoch: 17. Training data loss:  1799.4827880859375\n",
      "Epoch: 18. Training data loss:  1799.3773193359375\n",
      "Epoch: 19. Training data loss:  1799.08642578125\n",
      "Accuracy found: 1.01\n",
      "20 0.001 0.01 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 8\n",
      "Epoch: 0. Training data loss:  23036.3984375\n",
      "Epoch: 1. Training data loss:  23029.236328125\n",
      "Epoch: 2. Training data loss:  23020.341796875\n",
      "Epoch: 3. Training data loss:  23007.970703125\n",
      "Epoch: 4. Training data loss:  22991.892578125\n",
      "Epoch: 5. Training data loss:  22966.654296875\n",
      "Epoch: 6. Training data loss:  22936.44921875\n",
      "Epoch: 7. Training data loss:  22898.703125\n",
      "Epoch: 8. Training data loss:  22865.693359375\n",
      "Epoch: 9. Training data loss:  22821.806640625\n",
      "Epoch: 10. Training data loss:  22777.947265625\n",
      "Epoch: 11. Training data loss:  22737.80859375\n",
      "Epoch: 12. Training data loss:  22700.099609375\n",
      "Epoch: 13. Training data loss:  22642.95703125\n",
      "Epoch: 14. Training data loss:  22600.36328125\n",
      "Epoch: 15. Training data loss:  22538.146484375\n",
      "Epoch: 16. Training data loss:  22481.017578125\n",
      "Epoch: 17. Training data loss:  22416.158203125\n",
      "Epoch: 18. Training data loss:  22360.517578125\n",
      "Epoch: 19. Training data loss:  22298.87109375\n",
      "Epoch: 20. Training data loss:  22244.626953125\n",
      "Epoch: 21. Training data loss:  22190.365234375\n",
      "Epoch: 22. Training data loss:  22131.404296875\n",
      "Epoch: 23. Training data loss:  22088.609375\n",
      "Epoch: 24. Training data loss:  22034.458984375\n",
      "Epoch: 25. Training data loss:  21983.52734375\n",
      "Epoch: 26. Training data loss:  21948.57421875\n",
      "Epoch: 27. Training data loss:  21910.419921875\n",
      "Epoch: 28. Training data loss:  21870.826171875\n",
      "Epoch: 29. Training data loss:  21830.556640625\n",
      "Epoch: 30. Training data loss:  21792.22265625\n",
      "Epoch: 31. Training data loss:  21746.41796875\n",
      "Epoch: 32. Training data loss:  21700.822265625\n",
      "Epoch: 33. Training data loss:  21671.0390625\n",
      "Epoch: 34. Training data loss:  21636.96484375\n",
      "Epoch: 35. Training data loss:  21592.5234375\n",
      "Epoch: 36. Training data loss:  21572.568359375\n",
      "Epoch: 37. Training data loss:  21522.19140625\n",
      "Epoch: 38. Training data loss:  21489.984375\n",
      "Epoch: 39. Training data loss:  21463.65625\n",
      "Accuracy found: 6.35\n",
      "40 0.001 0.001 CrossEntropyLoss() 10 0\n",
      "Starting iteration, 9\n",
      "Epoch: 0. Training data loss:  1801.4051513671875\n",
      "Epoch: 1. Training data loss:  1801.40869140625\n",
      "Epoch: 2. Training data loss:  1801.5152587890625\n",
      "Epoch: 3. Training data loss:  1801.4146728515625\n",
      "Epoch: 4. Training data loss:  1801.388671875\n",
      "Epoch: 5. Training data loss:  1801.38525390625\n",
      "Epoch: 6. Training data loss:  1801.43017578125\n",
      "Epoch: 7. Training data loss:  1801.4051513671875\n",
      "Epoch: 8. Training data loss:  1801.37255859375\n",
      "Epoch: 9. Training data loss:  1801.34814453125\n",
      "Epoch: 10. Training data loss:  1801.356201171875\n",
      "Epoch: 11. Training data loss:  1801.3372802734375\n",
      "Epoch: 12. Training data loss:  1801.342041015625\n",
      "Epoch: 13. Training data loss:  1801.2071533203125\n",
      "Epoch: 14. Training data loss:  1801.296142578125\n",
      "Epoch: 15. Training data loss:  1801.378662109375\n",
      "Epoch: 16. Training data loss:  1801.2620849609375\n",
      "Epoch: 17. Training data loss:  1801.2994384765625\n",
      "Epoch: 18. Training data loss:  1801.2706298828125\n",
      "Epoch: 19. Training data loss:  1801.283447265625\n",
      "Epoch: 20. Training data loss:  1801.1927490234375\n",
      "Epoch: 21. Training data loss:  1801.2552490234375\n",
      "Epoch: 22. Training data loss:  1801.23779296875\n",
      "Epoch: 23. Training data loss:  1801.31103515625\n",
      "Epoch: 24. Training data loss:  1801.1051025390625\n",
      "Epoch: 25. Training data loss:  1801.1346435546875\n",
      "Epoch: 26. Training data loss:  1801.1746826171875\n",
      "Epoch: 27. Training data loss:  1801.2333984375\n",
      "Epoch: 28. Training data loss:  1801.148681640625\n",
      "Epoch: 29. Training data loss:  1801.193603515625\n",
      "Epoch: 30. Training data loss:  1801.1434326171875\n",
      "Epoch: 31. Training data loss:  1801.168212890625\n",
      "Epoch: 32. Training data loss:  1801.0859375\n",
      "Epoch: 33. Training data loss:  1801.1085205078125\n",
      "Epoch: 34. Training data loss:  1801.17041015625\n",
      "Epoch: 35. Training data loss:  1801.1273193359375\n",
      "Epoch: 36. Training data loss:  1801.097412109375\n",
      "Epoch: 37. Training data loss:  1801.135986328125\n",
      "Epoch: 38. Training data loss:  1801.134033203125\n",
      "Epoch: 39. Training data loss:  1801.092529296875\n",
      "Epoch: 40. Training data loss:  1801.085693359375\n",
      "Epoch: 41. Training data loss:  1801.13671875\n",
      "Epoch: 42. Training data loss:  1801.096435546875\n",
      "Epoch: 43. Training data loss:  1801.0992431640625\n",
      "Epoch: 44. Training data loss:  1801.0740966796875\n",
      "Epoch: 45. Training data loss:  1801.01953125\n",
      "Epoch: 46. Training data loss:  1801.0396728515625\n",
      "Epoch: 47. Training data loss:  1800.97998046875\n",
      "Epoch: 48. Training data loss:  1801.0743408203125\n",
      "Epoch: 49. Training data loss:  1800.9908447265625\n",
      "Epoch: 50. Training data loss:  1801.0125732421875\n",
      "Epoch: 51. Training data loss:  1801.0006103515625\n",
      "Epoch: 52. Training data loss:  1800.989990234375\n",
      "Epoch: 53. Training data loss:  1800.95263671875\n",
      "Epoch: 54. Training data loss:  1800.9664306640625\n",
      "Epoch: 55. Training data loss:  1800.8724365234375\n",
      "Epoch: 56. Training data loss:  1800.882568359375\n",
      "Epoch: 57. Training data loss:  1800.910400390625\n",
      "Epoch: 58. Training data loss:  1800.8834228515625\n",
      "Epoch: 59. Training data loss:  1800.9105224609375\n",
      "Accuracy found: 1.03\n",
      "60 0.0001 0.01 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [32, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  1216.5650634765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [16, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Training data loss:  2176.980224609375\n",
      "Epoch: 2. Training data loss:  2245.508544921875\n",
      "Epoch: 3. Training data loss:  1705.1134033203125\n",
      "Epoch: 4. Training data loss:  4370.15625\n",
      "Epoch: 5. Training data loss:  7313.20751953125\n",
      "Epoch: 6. Training data loss:  5851.21240234375\n",
      "Epoch: 7. Training data loss:  3356.233642578125\n",
      "Epoch: 8. Training data loss:  2566.076904296875\n",
      "Epoch: 9. Training data loss:  2473.469970703125\n",
      "Epoch: 10. Training data loss:  2736.810791015625\n",
      "Epoch: 11. Training data loss:  1786.5740966796875\n",
      "Epoch: 12. Training data loss:  1377.926025390625\n",
      "Epoch: 13. Training data loss:  1296.2530517578125\n",
      "Epoch: 14. Training data loss:  1325.74560546875\n",
      "Epoch: 15. Training data loss:  1308.225341796875\n",
      "Epoch: 16. Training data loss:  1219.5855712890625\n",
      "Epoch: 17. Training data loss:  1332.6802978515625\n",
      "Epoch: 18. Training data loss:  1266.4031982421875\n",
      "Epoch: 19. Training data loss:  1230.568603515625\n",
      "Epoch: 20. Training data loss:  1360.2947998046875\n",
      "Epoch: 21. Training data loss:  1387.7076416015625\n",
      "Epoch: 22. Training data loss:  1296.18408203125\n",
      "Epoch: 23. Training data loss:  1217.315673828125\n",
      "Epoch: 24. Training data loss:  1257.04150390625\n",
      "Epoch: 25. Training data loss:  1466.3489990234375\n",
      "Epoch: 26. Training data loss:  1638.214599609375\n",
      "Epoch: 27. Training data loss:  1357.08984375\n",
      "Epoch: 28. Training data loss:  1299.5205078125\n",
      "Epoch: 29. Training data loss:  1321.3131103515625\n",
      "Epoch: 30. Training data loss:  1222.734130859375\n",
      "Epoch: 31. Training data loss:  1257.540771484375\n",
      "Epoch: 32. Training data loss:  1217.869384765625\n",
      "Epoch: 33. Training data loss:  1486.9534912109375\n",
      "Epoch: 34. Training data loss:  1319.4453125\n",
      "Epoch: 35. Training data loss:  1224.4827880859375\n",
      "Epoch: 36. Training data loss:  1161.2906494140625\n",
      "Epoch: 37. Training data loss:  1117.7664794921875\n",
      "Epoch: 38. Training data loss:  1150.3538818359375\n",
      "Epoch: 39. Training data loss:  1142.1728515625\n",
      "Accuracy found: 3.92\n",
      "40 0.001 0 MultiMarginLoss() 32 1\n",
      "Starting iteration, 11\n",
      "Epoch: 0. Training data loss:  387.2216796875\n",
      "Epoch: 1. Training data loss:  387.0906066894531\n",
      "Epoch: 2. Training data loss:  387.09503173828125\n",
      "Epoch: 3. Training data loss:  387.0918273925781\n",
      "Epoch: 4. Training data loss:  387.09527587890625\n",
      "Epoch: 5. Training data loss:  387.0924072265625\n",
      "Epoch: 6. Training data loss:  387.091552734375\n",
      "Epoch: 7. Training data loss:  387.0952453613281\n",
      "Epoch: 8. Training data loss:  387.09381103515625\n",
      "Epoch: 9. Training data loss:  387.09423828125\n",
      "Epoch: 10. Training data loss:  387.0936279296875\n",
      "Epoch: 11. Training data loss:  387.0938720703125\n",
      "Epoch: 12. Training data loss:  387.093994140625\n",
      "Epoch: 13. Training data loss:  387.0938415527344\n",
      "Epoch: 14. Training data loss:  387.0939025878906\n",
      "Epoch: 15. Training data loss:  387.09368896484375\n",
      "Epoch: 16. Training data loss:  387.09381103515625\n",
      "Epoch: 17. Training data loss:  387.093994140625\n",
      "Epoch: 18. Training data loss:  387.0938720703125\n",
      "Epoch: 19. Training data loss:  387.09393310546875\n",
      "Accuracy found: 1.0\n",
      "20 0.01 0.3 MultiMarginLoss() 128 0\n",
      "Starting iteration, 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carol/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [64, 100]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/Resize.cpp:17.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training data loss:  773.3575439453125\n",
      "Epoch: 1. Training data loss:  772.5415649414062\n",
      "Epoch: 2. Training data loss:  772.033447265625\n",
      "Epoch: 3. Training data loss:  771.1635131835938\n",
      "Epoch: 4. Training data loss:  770.1806640625\n",
      "Epoch: 5. Training data loss:  769.0689086914062\n",
      "Epoch: 6. Training data loss:  767.5250854492188\n",
      "Epoch: 7. Training data loss:  766.023193359375\n",
      "Epoch: 8. Training data loss:  764.1390991210938\n",
      "Epoch: 9. Training data loss:  761.3364868164062\n",
      "Epoch: 10. Training data loss:  757.8060913085938\n",
      "Epoch: 11. Training data loss:  754.3247680664062\n",
      "Epoch: 12. Training data loss:  749.649658203125\n",
      "Epoch: 13. Training data loss:  745.4552612304688\n",
      "Epoch: 14. Training data loss:  741.2228393554688\n",
      "Epoch: 15. Training data loss:  735.2282104492188\n",
      "Epoch: 16. Training data loss:  729.5370483398438\n",
      "Epoch: 17. Training data loss:  725.0995483398438\n",
      "Epoch: 18. Training data loss:  717.8963623046875\n",
      "Epoch: 19. Training data loss:  712.3663940429688\n",
      "Epoch: 20. Training data loss:  707.6715698242188\n",
      "Epoch: 21. Training data loss:  700.8663940429688\n",
      "Epoch: 22. Training data loss:  697.5062255859375\n",
      "Epoch: 23. Training data loss:  689.3914794921875\n",
      "Epoch: 24. Training data loss:  685.87548828125\n",
      "Epoch: 25. Training data loss:  681.88720703125\n",
      "Epoch: 26. Training data loss:  677.1434936523438\n",
      "Epoch: 27. Training data loss:  673.4796142578125\n",
      "Epoch: 28. Training data loss:  668.5806274414062\n",
      "Epoch: 29. Training data loss:  663.4537353515625\n",
      "Epoch: 30. Training data loss:  660.9256591796875\n",
      "Epoch: 31. Training data loss:  658.8565063476562\n",
      "Epoch: 32. Training data loss:  655.8513793945312\n",
      "Epoch: 33. Training data loss:  651.916748046875\n",
      "Epoch: 34. Training data loss:  649.225830078125\n",
      "Epoch: 35. Training data loss:  645.2265625\n",
      "Epoch: 36. Training data loss:  641.8668823242188\n",
      "Epoch: 37. Training data loss:  638.9515991210938\n",
      "Epoch: 38. Training data loss:  636.8890991210938\n",
      "Epoch: 39. Training data loss:  634.6952514648438\n",
      "Epoch: 40. Training data loss:  631.3350219726562\n",
      "Epoch: 41. Training data loss:  630.4177856445312\n",
      "Epoch: 42. Training data loss:  627.9899291992188\n",
      "Epoch: 43. Training data loss:  624.1727294921875\n",
      "Epoch: 44. Training data loss:  622.3228149414062\n",
      "Epoch: 45. Training data loss:  619.5157470703125\n",
      "Epoch: 46. Training data loss:  617.1348266601562\n",
      "Epoch: 47. Training data loss:  615.4462280273438\n",
      "Epoch: 48. Training data loss:  611.66943359375\n",
      "Epoch: 49. Training data loss:  611.9473266601562\n",
      "Epoch: 50. Training data loss:  608.114013671875\n",
      "Epoch: 51. Training data loss:  606.0294799804688\n",
      "Epoch: 52. Training data loss:  604.9328002929688\n",
      "Epoch: 53. Training data loss:  602.96484375\n",
      "Epoch: 54. Training data loss:  598.1091918945312\n",
      "Epoch: 55. Training data loss:  598.4783325195312\n",
      "Epoch: 56. Training data loss:  595.5147705078125\n",
      "Epoch: 57. Training data loss:  593.431884765625\n",
      "Epoch: 58. Training data loss:  591.6759033203125\n",
      "Epoch: 59. Training data loss:  589.8736572265625\n",
      "Accuracy found: 3.77\n",
      "60 0.001 0 MultiMarginLoss() 64 0\n",
      "Starting iteration, 13\n",
      "Epoch: 0. Training data loss:  387.0824890136719\n",
      "Epoch: 1. Training data loss:  387.0910339355469\n",
      "Epoch: 2. Training data loss:  387.1050720214844\n",
      "Epoch: 3. Training data loss:  387.0932312011719\n",
      "Epoch: 4. Training data loss:  387.1015930175781\n",
      "Epoch: 5. Training data loss:  387.1021423339844\n",
      "Epoch: 6. Training data loss:  387.1004943847656\n",
      "Epoch: 7. Training data loss:  387.10076904296875\n",
      "Epoch: 8. Training data loss:  387.10015869140625\n",
      "Epoch: 9. Training data loss:  387.10028076171875\n",
      "Epoch: 10. Training data loss:  387.10137939453125\n",
      "Epoch: 11. Training data loss:  387.1014099121094\n",
      "Epoch: 12. Training data loss:  387.1009826660156\n",
      "Epoch: 13. Training data loss:  387.1014709472656\n",
      "Epoch: 14. Training data loss:  387.10052490234375\n",
      "Epoch: 15. Training data loss:  387.1019592285156\n",
      "Epoch: 16. Training data loss:  387.1016540527344\n",
      "Epoch: 17. Training data loss:  387.10162353515625\n",
      "Epoch: 18. Training data loss:  387.1021728515625\n",
      "Epoch: 19. Training data loss:  387.1018371582031\n",
      "Epoch: 20. Training data loss:  387.10125732421875\n",
      "Epoch: 21. Training data loss:  387.1005554199219\n",
      "Epoch: 22. Training data loss:  387.1007995605469\n",
      "Epoch: 23. Training data loss:  387.10150146484375\n",
      "Epoch: 24. Training data loss:  387.1008605957031\n",
      "Epoch: 25. Training data loss:  387.10186767578125\n",
      "Epoch: 26. Training data loss:  387.1011962890625\n",
      "Epoch: 27. Training data loss:  387.1014404296875\n",
      "Epoch: 28. Training data loss:  387.100830078125\n",
      "Epoch: 29. Training data loss:  387.09979248046875\n",
      "Epoch: 30. Training data loss:  387.1008605957031\n",
      "Epoch: 31. Training data loss:  387.1018371582031\n",
      "Epoch: 32. Training data loss:  387.1007080078125\n",
      "Epoch: 33. Training data loss:  387.1017150878906\n",
      "Epoch: 34. Training data loss:  387.1011047363281\n",
      "Epoch: 35. Training data loss:  387.1014099121094\n",
      "Epoch: 36. Training data loss:  387.1018981933594\n",
      "Epoch: 37. Training data loss:  387.1021423339844\n",
      "Epoch: 38. Training data loss:  387.10101318359375\n",
      "Epoch: 39. Training data loss:  387.1004333496094\n",
      "Accuracy found: 1.0\n",
      "40 0.0001 0.3 MultiMarginLoss() 128 1\n",
      "Starting iteration, 14\n",
      "Epoch: 0. Training data loss:  1800.875732421875\n",
      "Epoch: 1. Training data loss:  1800.6405029296875\n",
      "Epoch: 2. Training data loss:  1800.638916015625\n",
      "Epoch: 3. Training data loss:  1800.6407470703125\n",
      "Epoch: 4. Training data loss:  1800.6412353515625\n",
      "Epoch: 5. Training data loss:  1800.6412353515625\n",
      "Epoch: 6. Training data loss:  1800.6378173828125\n",
      "Epoch: 7. Training data loss:  1800.6422119140625\n",
      "Epoch: 8. Training data loss:  1800.6417236328125\n",
      "Epoch: 9. Training data loss:  1800.6414794921875\n",
      "Epoch: 10. Training data loss:  1800.6405029296875\n",
      "Epoch: 11. Training data loss:  1800.6383056640625\n",
      "Epoch: 12. Training data loss:  1800.6396484375\n",
      "Epoch: 13. Training data loss:  1800.6387939453125\n",
      "Epoch: 14. Training data loss:  1800.6396484375\n",
      "Epoch: 15. Training data loss:  1800.638671875\n",
      "Epoch: 16. Training data loss:  1800.638916015625\n",
      "Epoch: 17. Training data loss:  1800.640869140625\n",
      "Epoch: 18. Training data loss:  1800.641845703125\n",
      "Epoch: 19. Training data loss:  1800.640625\n",
      "Accuracy found: 1.0\n",
      "20 0.1 0.3 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 15\n",
      "Epoch: 0. Training data loss:  7199.8984375\n",
      "Epoch: 1. Training data loss:  7199.70751953125\n",
      "Epoch: 2. Training data loss:  7200.03662109375\n",
      "Epoch: 3. Training data loss:  7200.189453125\n",
      "Epoch: 4. Training data loss:  7199.05615234375\n",
      "Epoch: 5. Training data loss:  7199.85498046875\n",
      "Epoch: 6. Training data loss:  7199.89111328125\n",
      "Epoch: 7. Training data loss:  7200.01123046875\n",
      "Epoch: 8. Training data loss:  7200.0595703125\n",
      "Epoch: 9. Training data loss:  7199.68017578125\n",
      "Epoch: 10. Training data loss:  7199.61865234375\n",
      "Epoch: 11. Training data loss:  7200.24560546875\n",
      "Epoch: 12. Training data loss:  7199.73095703125\n",
      "Epoch: 13. Training data loss:  7200.1083984375\n",
      "Epoch: 14. Training data loss:  7199.61572265625\n",
      "Epoch: 15. Training data loss:  7199.71435546875\n",
      "Epoch: 16. Training data loss:  7199.53466796875\n",
      "Epoch: 17. Training data loss:  7199.38134765625\n",
      "Epoch: 18. Training data loss:  7199.91943359375\n",
      "Epoch: 19. Training data loss:  7199.912109375\n",
      "Epoch: 20. Training data loss:  7199.9541015625\n",
      "Epoch: 21. Training data loss:  7199.7080078125\n",
      "Epoch: 22. Training data loss:  7199.1611328125\n",
      "Epoch: 23. Training data loss:  7199.96337890625\n",
      "Epoch: 24. Training data loss:  7200.16064453125\n",
      "Epoch: 25. Training data loss:  7199.72314453125\n",
      "Epoch: 26. Training data loss:  7199.8203125\n",
      "Epoch: 27. Training data loss:  7199.45654296875\n",
      "Epoch: 28. Training data loss:  7199.552734375\n",
      "Epoch: 29. Training data loss:  7199.986328125\n",
      "Epoch: 30. Training data loss:  7199.24658203125\n",
      "Epoch: 31. Training data loss:  7199.51416015625\n",
      "Epoch: 32. Training data loss:  7199.953125\n",
      "Epoch: 33. Training data loss:  7199.79541015625\n",
      "Epoch: 34. Training data loss:  7199.1806640625\n",
      "Epoch: 35. Training data loss:  7199.91455078125\n",
      "Epoch: 36. Training data loss:  7199.6630859375\n",
      "Epoch: 37. Training data loss:  7199.97216796875\n",
      "Epoch: 38. Training data loss:  7199.7353515625\n",
      "Epoch: 39. Training data loss:  7200.03662109375\n",
      "Accuracy found: 1.0\n",
      "40 0.1 0.3 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 16\n",
      "Epoch: 0. Training data loss:  695.4888916015625\n",
      "Epoch: 1. Training data loss:  586.7229614257812\n",
      "Epoch: 2. Training data loss:  568.0254516601562\n",
      "Epoch: 3. Training data loss:  534.0936889648438\n",
      "Epoch: 4. Training data loss:  512.7725830078125\n",
      "Epoch: 5. Training data loss:  511.06048583984375\n",
      "Epoch: 6. Training data loss:  492.1654052734375\n",
      "Epoch: 7. Training data loss:  497.8512268066406\n",
      "Epoch: 8. Training data loss:  491.4531555175781\n",
      "Epoch: 9. Training data loss:  490.14947509765625\n",
      "Epoch: 10. Training data loss:  485.90203857421875\n",
      "Epoch: 11. Training data loss:  483.612548828125\n",
      "Epoch: 12. Training data loss:  474.22735595703125\n",
      "Epoch: 13. Training data loss:  473.75830078125\n",
      "Epoch: 14. Training data loss:  477.91064453125\n",
      "Epoch: 15. Training data loss:  468.9144592285156\n",
      "Epoch: 16. Training data loss:  466.4181213378906\n",
      "Epoch: 17. Training data loss:  461.1710205078125\n",
      "Epoch: 18. Training data loss:  463.8743591308594\n",
      "Epoch: 19. Training data loss:  458.74285888671875\n",
      "Epoch: 20. Training data loss:  463.4395446777344\n",
      "Epoch: 21. Training data loss:  460.26361083984375\n",
      "Epoch: 22. Training data loss:  474.37255859375\n",
      "Epoch: 23. Training data loss:  463.2861022949219\n",
      "Epoch: 24. Training data loss:  454.8238220214844\n",
      "Epoch: 25. Training data loss:  464.4168395996094\n",
      "Epoch: 26. Training data loss:  459.7882995605469\n",
      "Epoch: 27. Training data loss:  457.6517028808594\n",
      "Epoch: 28. Training data loss:  458.36114501953125\n",
      "Epoch: 29. Training data loss:  451.9870300292969\n",
      "Epoch: 30. Training data loss:  464.8908996582031\n",
      "Epoch: 31. Training data loss:  457.6579895019531\n",
      "Epoch: 32. Training data loss:  455.5658264160156\n",
      "Epoch: 33. Training data loss:  449.6991271972656\n",
      "Epoch: 34. Training data loss:  450.8521423339844\n",
      "Epoch: 35. Training data loss:  455.45574951171875\n",
      "Epoch: 36. Training data loss:  445.7454528808594\n",
      "Epoch: 37. Training data loss:  443.4850769042969\n",
      "Epoch: 38. Training data loss:  446.2153625488281\n",
      "Epoch: 39. Training data loss:  456.5216979980469\n",
      "Accuracy found: 6.17\n",
      "40 0.0001 0.01 MultiMarginLoss() 64 1\n",
      "Starting iteration, 17\n",
      "Epoch: 0. Training data loss:  6941.98974609375\n",
      "Epoch: 1. Training data loss:  6575.515625\n",
      "Epoch: 2. Training data loss:  6389.95849609375\n",
      "Epoch: 3. Training data loss:  6251.8876953125\n",
      "Epoch: 4. Training data loss:  6126.55419921875\n",
      "Epoch: 5. Training data loss:  6034.03076171875\n",
      "Epoch: 6. Training data loss:  5956.38720703125\n",
      "Epoch: 7. Training data loss:  5883.28564453125\n",
      "Epoch: 8. Training data loss:  5811.42919921875\n",
      "Epoch: 9. Training data loss:  5760.8916015625\n",
      "Epoch: 10. Training data loss:  5700.29833984375\n",
      "Epoch: 11. Training data loss:  5660.0986328125\n",
      "Epoch: 12. Training data loss:  5615.8447265625\n",
      "Epoch: 13. Training data loss:  5578.56201171875\n",
      "Epoch: 14. Training data loss:  5532.9580078125\n",
      "Epoch: 15. Training data loss:  5498.3154296875\n",
      "Epoch: 16. Training data loss:  5456.12109375\n",
      "Epoch: 17. Training data loss:  5441.25830078125\n",
      "Epoch: 18. Training data loss:  5402.11083984375\n",
      "Epoch: 19. Training data loss:  5370.15625\n",
      "Accuracy found: 21.74\n",
      "20 0.0001 0 CrossEntropyLoss() 32 1\n",
      "Starting iteration, 18\n",
      "Epoch: 0. Training data loss:  5325.47314453125\n",
      "Epoch: 1. Training data loss:  59489304576.0\n",
      "Epoch: 2. Training data loss:  0.0\n",
      "Epoch: 3. Training data loss:  0.0\n",
      "Epoch: 4. Training data loss:  0.0\n",
      "Epoch: 5. Training data loss:  0.0\n",
      "Epoch: 6. Training data loss:  0.0\n",
      "Epoch: 7. Training data loss:  0.0\n",
      "Epoch: 8. Training data loss:  0.0\n",
      "Epoch: 9. Training data loss:  0.0\n",
      "Epoch: 10. Training data loss:  0.0\n",
      "Epoch: 11. Training data loss:  0.0\n",
      "Epoch: 12. Training data loss:  0.0\n",
      "Epoch: 13. Training data loss:  0.0\n",
      "Epoch: 14. Training data loss:  0.0\n",
      "Epoch: 15. Training data loss:  0.0\n",
      "Epoch: 16. Training data loss:  0.0\n",
      "Epoch: 17. Training data loss:  0.0\n",
      "Epoch: 18. Training data loss:  0.0\n",
      "Epoch: 19. Training data loss:  0.0\n",
      "Epoch: 20. Training data loss:  0.0\n",
      "Epoch: 21. Training data loss:  0.0\n",
      "Epoch: 22. Training data loss:  0.0\n",
      "Epoch: 23. Training data loss:  0.0\n",
      "Epoch: 24. Training data loss:  0.0\n",
      "Epoch: 25. Training data loss:  0.0\n",
      "Epoch: 26. Training data loss:  0.0\n",
      "Epoch: 27. Training data loss:  0.0\n",
      "Epoch: 28. Training data loss:  0.0\n",
      "Epoch: 29. Training data loss:  0.0\n",
      "Epoch: 30. Training data loss:  0.0\n",
      "Epoch: 31. Training data loss:  0.0\n",
      "Epoch: 32. Training data loss:  0.0\n",
      "Epoch: 33. Training data loss:  0.0\n",
      "Epoch: 34. Training data loss:  0.0\n",
      "Epoch: 35. Training data loss:  0.0\n",
      "Epoch: 36. Training data loss:  0.0\n",
      "Epoch: 37. Training data loss:  0.0\n",
      "Epoch: 38. Training data loss:  0.0\n",
      "Epoch: 39. Training data loss:  0.0\n",
      "Epoch: 40. Training data loss:  0.0\n",
      "Epoch: 41. Training data loss:  0.0\n",
      "Epoch: 42. Training data loss:  0.0\n",
      "Epoch: 43. Training data loss:  0.0\n",
      "Epoch: 44. Training data loss:  0.0\n",
      "Epoch: 45. Training data loss:  0.0\n",
      "Epoch: 46. Training data loss:  0.0\n",
      "Epoch: 47. Training data loss:  0.0\n",
      "Epoch: 48. Training data loss:  0.0\n",
      "Epoch: 49. Training data loss:  0.0\n",
      "Epoch: 50. Training data loss:  0.0\n",
      "Epoch: 51. Training data loss:  0.0\n",
      "Epoch: 52. Training data loss:  0.0\n",
      "Epoch: 53. Training data loss:  0.0\n",
      "Epoch: 54. Training data loss:  0.0\n",
      "Epoch: 55. Training data loss:  0.0\n",
      "Epoch: 56. Training data loss:  0.0\n",
      "Epoch: 57. Training data loss:  0.0\n",
      "Epoch: 58. Training data loss:  0.0\n",
      "Epoch: 59. Training data loss:  0.0\n",
      "Accuracy found: 1.0\n",
      "60 0.1 0.01 MultiMarginLoss() 10 1\n",
      "Starting iteration, 19\n",
      "Epoch: 0. Training data loss:  773.96337890625\n",
      "Epoch: 1. Training data loss:  774.1995239257812\n",
      "Epoch: 2. Training data loss:  774.1948852539062\n",
      "Epoch: 3. Training data loss:  774.1975708007812\n",
      "Epoch: 4. Training data loss:  774.1903076171875\n",
      "Epoch: 5. Training data loss:  774.193115234375\n",
      "Epoch: 6. Training data loss:  774.1953125\n",
      "Epoch: 7. Training data loss:  774.1893310546875\n",
      "Epoch: 8. Training data loss:  774.1902465820312\n",
      "Epoch: 9. Training data loss:  774.1898193359375\n",
      "Epoch: 10. Training data loss:  774.1901245117188\n",
      "Epoch: 11. Training data loss:  774.1904296875\n",
      "Epoch: 12. Training data loss:  774.1904296875\n",
      "Epoch: 13. Training data loss:  774.1898193359375\n",
      "Epoch: 14. Training data loss:  774.1911010742188\n",
      "Epoch: 15. Training data loss:  774.1900634765625\n",
      "Epoch: 16. Training data loss:  774.191650390625\n",
      "Epoch: 17. Training data loss:  774.1905517578125\n",
      "Epoch: 18. Training data loss:  774.1902465820312\n",
      "Epoch: 19. Training data loss:  774.190673828125\n",
      "Epoch: 20. Training data loss:  774.1903686523438\n",
      "Epoch: 21. Training data loss:  774.1909790039062\n",
      "Epoch: 22. Training data loss:  774.1907348632812\n",
      "Epoch: 23. Training data loss:  774.1902465820312\n",
      "Epoch: 24. Training data loss:  774.19091796875\n",
      "Epoch: 25. Training data loss:  774.1909790039062\n",
      "Epoch: 26. Training data loss:  774.1907348632812\n",
      "Epoch: 27. Training data loss:  774.1907348632812\n",
      "Epoch: 28. Training data loss:  774.191162109375\n",
      "Epoch: 29. Training data loss:  774.1903076171875\n",
      "Epoch: 30. Training data loss:  774.1904296875\n",
      "Epoch: 31. Training data loss:  774.1898193359375\n",
      "Epoch: 32. Training data loss:  774.1907348632812\n",
      "Epoch: 33. Training data loss:  774.18994140625\n",
      "Epoch: 34. Training data loss:  774.1895751953125\n",
      "Epoch: 35. Training data loss:  774.1906127929688\n",
      "Epoch: 36. Training data loss:  774.1903686523438\n",
      "Epoch: 37. Training data loss:  774.1903076171875\n",
      "Epoch: 38. Training data loss:  774.1905517578125\n",
      "Epoch: 39. Training data loss:  774.1907958984375\n",
      "Epoch: 40. Training data loss:  774.1903686523438\n",
      "Epoch: 41. Training data loss:  774.190673828125\n",
      "Epoch: 42. Training data loss:  774.1907348632812\n",
      "Epoch: 43. Training data loss:  774.189697265625\n",
      "Epoch: 44. Training data loss:  774.1912841796875\n",
      "Epoch: 45. Training data loss:  774.1903686523438\n",
      "Epoch: 46. Training data loss:  774.190673828125\n",
      "Epoch: 47. Training data loss:  774.1908569335938\n",
      "Epoch: 48. Training data loss:  774.1907958984375\n",
      "Epoch: 49. Training data loss:  774.1906127929688\n",
      "Epoch: 50. Training data loss:  774.190673828125\n",
      "Epoch: 51. Training data loss:  774.1905517578125\n",
      "Epoch: 52. Training data loss:  774.19140625\n",
      "Epoch: 53. Training data loss:  774.1915893554688\n",
      "Epoch: 54. Training data loss:  774.1907348632812\n",
      "Epoch: 55. Training data loss:  774.1901245117188\n",
      "Epoch: 56. Training data loss:  774.1908569335938\n",
      "Epoch: 57. Training data loss:  774.1904296875\n",
      "Epoch: 58. Training data loss:  774.1903686523438\n",
      "Epoch: 59. Training data loss:  774.1911010742188\n",
      "Accuracy found: 1.0\n",
      "60 0.01 0.3 MultiMarginLoss() 64 0\n",
      "Starting iteration, 20\n",
      "Epoch: 0. Training data loss:  7191.34619140625\n",
      "Epoch: 1. Training data loss:  7114.337890625\n",
      "Epoch: 2. Training data loss:  7000.107421875\n",
      "Epoch: 3. Training data loss:  6923.0849609375\n",
      "Epoch: 4. Training data loss:  6856.08984375\n",
      "Epoch: 5. Training data loss:  6810.60009765625\n",
      "Epoch: 6. Training data loss:  6755.412109375\n",
      "Epoch: 7. Training data loss:  6712.43212890625\n",
      "Epoch: 8. Training data loss:  6663.919921875\n",
      "Epoch: 9. Training data loss:  6624.48876953125\n",
      "Epoch: 10. Training data loss:  6574.17626953125\n",
      "Epoch: 11. Training data loss:  6529.63916015625\n",
      "Epoch: 12. Training data loss:  6493.2900390625\n",
      "Epoch: 13. Training data loss:  6450.84130859375\n",
      "Epoch: 14. Training data loss:  6412.46826171875\n",
      "Epoch: 15. Training data loss:  6377.16015625\n",
      "Epoch: 16. Training data loss:  6340.79833984375\n",
      "Epoch: 17. Training data loss:  6311.8515625\n",
      "Epoch: 18. Training data loss:  6278.91845703125\n",
      "Epoch: 19. Training data loss:  6253.29052734375\n",
      "Epoch: 20. Training data loss:  6223.0986328125\n",
      "Epoch: 21. Training data loss:  6190.6552734375\n",
      "Epoch: 22. Training data loss:  6165.1708984375\n",
      "Epoch: 23. Training data loss:  6145.66650390625\n",
      "Epoch: 24. Training data loss:  6128.42333984375\n",
      "Epoch: 25. Training data loss:  6100.5\n",
      "Epoch: 26. Training data loss:  6078.228515625\n",
      "Epoch: 27. Training data loss:  6050.60546875\n",
      "Epoch: 28. Training data loss:  6033.185546875\n",
      "Epoch: 29. Training data loss:  6017.62548828125\n",
      "Epoch: 30. Training data loss:  5996.97216796875\n",
      "Epoch: 31. Training data loss:  5973.8544921875\n",
      "Epoch: 32. Training data loss:  5955.2802734375\n",
      "Epoch: 33. Training data loss:  5948.369140625\n",
      "Epoch: 34. Training data loss:  5920.4501953125\n",
      "Epoch: 35. Training data loss:  5902.82666015625\n",
      "Epoch: 36. Training data loss:  5893.2978515625\n",
      "Epoch: 37. Training data loss:  5874.4560546875\n",
      "Epoch: 38. Training data loss:  5866.33642578125\n",
      "Epoch: 39. Training data loss:  5850.6943359375\n",
      "Epoch: 40. Training data loss:  5833.48193359375\n",
      "Epoch: 41. Training data loss:  5814.71923828125\n",
      "Epoch: 42. Training data loss:  5803.9736328125\n",
      "Epoch: 43. Training data loss:  5801.62060546875\n",
      "Epoch: 44. Training data loss:  5784.62255859375\n",
      "Epoch: 45. Training data loss:  5770.0263671875\n",
      "Epoch: 46. Training data loss:  5755.35009765625\n",
      "Epoch: 47. Training data loss:  5751.36865234375\n",
      "Epoch: 48. Training data loss:  5735.79833984375\n",
      "Epoch: 49. Training data loss:  5726.42724609375\n",
      "Epoch: 50. Training data loss:  5710.67333984375\n",
      "Epoch: 51. Training data loss:  5695.2509765625\n",
      "Epoch: 52. Training data loss:  5687.99658203125\n",
      "Epoch: 53. Training data loss:  5680.23876953125\n",
      "Epoch: 54. Training data loss:  5666.60888671875\n",
      "Epoch: 55. Training data loss:  5662.99853515625\n",
      "Epoch: 56. Training data loss:  5656.44921875\n",
      "Epoch: 57. Training data loss:  5643.28125\n",
      "Epoch: 58. Training data loss:  5633.0849609375\n",
      "Epoch: 59. Training data loss:  5621.24267578125\n",
      "Accuracy found: 18.98\n",
      "60 0.01 0 CrossEntropyLoss() 32 0\n",
      "Starting iteration, 21\n",
      "Epoch: 0. Training data loss:  386.8503112792969\n",
      "Epoch: 1. Training data loss:  387.0370788574219\n",
      "Epoch: 2. Training data loss:  387.0890808105469\n",
      "Epoch: 3. Training data loss:  387.0898742675781\n",
      "Epoch: 4. Training data loss:  387.0855407714844\n",
      "Epoch: 5. Training data loss:  387.0966796875\n",
      "Epoch: 6. Training data loss:  387.09405517578125\n",
      "Epoch: 7. Training data loss:  387.0933837890625\n",
      "Epoch: 8. Training data loss:  387.0927429199219\n",
      "Epoch: 9. Training data loss:  387.0943603515625\n",
      "Epoch: 10. Training data loss:  387.09405517578125\n",
      "Epoch: 11. Training data loss:  387.0936279296875\n",
      "Epoch: 12. Training data loss:  387.09417724609375\n",
      "Epoch: 13. Training data loss:  387.0938720703125\n",
      "Epoch: 14. Training data loss:  387.0934143066406\n",
      "Epoch: 15. Training data loss:  387.0938720703125\n",
      "Epoch: 16. Training data loss:  387.0938720703125\n",
      "Epoch: 17. Training data loss:  387.0937805175781\n",
      "Epoch: 18. Training data loss:  387.0936279296875\n",
      "Epoch: 19. Training data loss:  387.0939636230469\n",
      "Accuracy found: 1.0\n",
      "20 0.01 0.3 MultiMarginLoss() 128 0\n",
      "Starting iteration, 22\n",
      "Epoch: 0. Training data loss:  24862.150390625\n",
      "Epoch: 1. Training data loss:  23419.576171875\n",
      "Epoch: 2. Training data loss:  121447528.0\n",
      "Epoch: 3. Training data loss:  3832986.75\n",
      "Epoch: 4. Training data loss:  1088041.75\n",
      "Epoch: 5. Training data loss:  45063972.0\n",
      "Epoch: 6. Training data loss:  1008102.375\n",
      "Epoch: 7. Training data loss:  83963.0\n",
      "Epoch: 8. Training data loss:  63414192.0\n",
      "Epoch: 9. Training data loss:  2171977.75\n",
      "Epoch: 10. Training data loss:  3759063.25\n",
      "Epoch: 11. Training data loss:  66095.78125\n",
      "Epoch: 12. Training data loss:  2981028.75\n",
      "Epoch: 13. Training data loss:  300525.15625\n",
      "Epoch: 14. Training data loss:  30610.544921875\n",
      "Epoch: 15. Training data loss:  23387.970703125\n",
      "Epoch: 16. Training data loss:  25999968.0\n",
      "Epoch: 17. Training data loss:  4095024.25\n",
      "Epoch: 18. Training data loss:  594050.5625\n",
      "Epoch: 19. Training data loss:  182312.890625\n",
      "Accuracy found: 1.0\n",
      "20 0.1 0.001 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 23\n",
      "Epoch: 0. Training data loss:  997.2555541992188\n",
      "Epoch: 1. Training data loss:  742.4188232421875\n",
      "Epoch: 2. Training data loss:  662.3900756835938\n",
      "Epoch: 3. Training data loss:  1199.777099609375\n",
      "Epoch: 4. Training data loss:  1204.94287109375\n",
      "Epoch: 5. Training data loss:  1317.7388916015625\n",
      "Epoch: 6. Training data loss:  1315.2967529296875\n",
      "Epoch: 7. Training data loss:  1317.7803955078125\n",
      "Epoch: 8. Training data loss:  1107.988037109375\n",
      "Epoch: 9. Training data loss:  1184.466796875\n",
      "Epoch: 10. Training data loss:  1296.667724609375\n",
      "Epoch: 11. Training data loss:  1634.3468017578125\n",
      "Epoch: 12. Training data loss:  1191.1036376953125\n",
      "Epoch: 13. Training data loss:  1108.8770751953125\n",
      "Epoch: 14. Training data loss:  1330.0501708984375\n",
      "Epoch: 15. Training data loss:  1794.6661376953125\n",
      "Epoch: 16. Training data loss:  1392.35107421875\n",
      "Epoch: 17. Training data loss:  1310.82861328125\n",
      "Epoch: 18. Training data loss:  1209.4390869140625\n",
      "Epoch: 19. Training data loss:  1338.5389404296875\n",
      "Accuracy found: 3.32\n",
      "20 0.001 0.001 MultiMarginLoss() 32 1\n",
      "Starting iteration, 24\n",
      "Epoch: 0. Training data loss:  3601.549072265625\n",
      "Epoch: 1. Training data loss:  3601.290283203125\n",
      "Epoch: 2. Training data loss:  3601.285400390625\n",
      "Epoch: 3. Training data loss:  3601.287841796875\n",
      "Epoch: 4. Training data loss:  3601.2861328125\n",
      "Epoch: 5. Training data loss:  3601.285888671875\n",
      "Epoch: 6. Training data loss:  3601.27880859375\n",
      "Epoch: 7. Training data loss:  3601.28369140625\n",
      "Epoch: 8. Training data loss:  3601.284423828125\n",
      "Epoch: 9. Training data loss:  3601.289306640625\n",
      "Epoch: 10. Training data loss:  3601.28271484375\n",
      "Epoch: 11. Training data loss:  3601.281494140625\n",
      "Epoch: 12. Training data loss:  3601.273681640625\n",
      "Epoch: 13. Training data loss:  3601.285400390625\n",
      "Epoch: 14. Training data loss:  3601.2783203125\n",
      "Epoch: 15. Training data loss:  3601.288330078125\n",
      "Epoch: 16. Training data loss:  3601.282958984375\n",
      "Epoch: 17. Training data loss:  3601.2822265625\n",
      "Epoch: 18. Training data loss:  3601.282958984375\n",
      "Epoch: 19. Training data loss:  3601.26953125\n",
      "Epoch: 20. Training data loss:  3601.2880859375\n",
      "Epoch: 21. Training data loss:  3601.283935546875\n",
      "Epoch: 22. Training data loss:  3601.27001953125\n",
      "Epoch: 23. Training data loss:  3601.2783203125\n",
      "Epoch: 24. Training data loss:  3601.2890625\n",
      "Epoch: 25. Training data loss:  3601.286376953125\n",
      "Epoch: 26. Training data loss:  3601.29248046875\n",
      "Epoch: 27. Training data loss:  3601.289794921875\n",
      "Epoch: 28. Training data loss:  3601.2939453125\n",
      "Epoch: 29. Training data loss:  3601.288330078125\n",
      "Epoch: 30. Training data loss:  3601.2802734375\n",
      "Epoch: 31. Training data loss:  3601.282470703125\n",
      "Epoch: 32. Training data loss:  3601.294189453125\n",
      "Epoch: 33. Training data loss:  3601.284423828125\n",
      "Epoch: 34. Training data loss:  3601.2890625\n",
      "Epoch: 35. Training data loss:  3601.285400390625\n",
      "Epoch: 36. Training data loss:  3601.294677734375\n",
      "Epoch: 37. Training data loss:  3601.2890625\n",
      "Epoch: 38. Training data loss:  3601.2919921875\n",
      "Epoch: 39. Training data loss:  3601.279541015625\n",
      "Epoch: 40. Training data loss:  3601.2841796875\n",
      "Epoch: 41. Training data loss:  3601.28759765625\n",
      "Epoch: 42. Training data loss:  3601.27783203125\n",
      "Epoch: 43. Training data loss:  3601.284423828125\n",
      "Epoch: 44. Training data loss:  3601.28173828125\n",
      "Epoch: 45. Training data loss:  3601.286865234375\n",
      "Epoch: 46. Training data loss:  3601.28076171875\n",
      "Epoch: 47. Training data loss:  3601.29443359375\n",
      "Epoch: 48. Training data loss:  3601.27685546875\n",
      "Epoch: 49. Training data loss:  3601.282470703125\n",
      "Epoch: 50. Training data loss:  3601.285888671875\n",
      "Epoch: 51. Training data loss:  3601.278076171875\n",
      "Epoch: 52. Training data loss:  3601.28466796875\n",
      "Epoch: 53. Training data loss:  3601.286376953125\n",
      "Epoch: 54. Training data loss:  3601.27587890625\n",
      "Epoch: 55. Training data loss:  3601.287109375\n",
      "Epoch: 56. Training data loss:  3601.29443359375\n",
      "Epoch: 57. Training data loss:  3601.271728515625\n",
      "Epoch: 58. Training data loss:  3601.276123046875\n",
      "Epoch: 59. Training data loss:  3601.283447265625\n",
      "Accuracy found: 1.0\n",
      "60 0.1 0.3 CrossEntropyLoss() 64 0\n",
      "Starting iteration, 25\n",
      "Epoch: 0. Training data loss:  1801.40380859375\n",
      "Epoch: 1. Training data loss:  1800.6937255859375\n",
      "Epoch: 2. Training data loss:  1799.4644775390625\n",
      "Epoch: 3. Training data loss:  1796.650390625\n",
      "Epoch: 4. Training data loss:  1790.54443359375\n",
      "Epoch: 5. Training data loss:  1783.05517578125\n",
      "Epoch: 6. Training data loss:  1775.5706787109375\n",
      "Epoch: 7. Training data loss:  1766.7120361328125\n",
      "Epoch: 8. Training data loss:  1755.455078125\n",
      "Epoch: 9. Training data loss:  1743.1728515625\n",
      "Epoch: 10. Training data loss:  1733.1845703125\n",
      "Epoch: 11. Training data loss:  1723.767822265625\n",
      "Epoch: 12. Training data loss:  1715.209716796875\n",
      "Epoch: 13. Training data loss:  1707.829345703125\n",
      "Epoch: 14. Training data loss:  1700.94873046875\n",
      "Epoch: 15. Training data loss:  1694.0927734375\n",
      "Epoch: 16. Training data loss:  1687.890625\n",
      "Epoch: 17. Training data loss:  1681.2685546875\n",
      "Epoch: 18. Training data loss:  1674.896728515625\n",
      "Epoch: 19. Training data loss:  1670.063232421875\n",
      "Accuracy found: 6.44\n",
      "20 0.01 0 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 26\n",
      "Epoch: 0. Training data loss:  1801.013427734375\n",
      "Epoch: 1. Training data loss:  1800.1358642578125\n",
      "Epoch: 2. Training data loss:  1799.0518798828125\n",
      "Epoch: 3. Training data loss:  1797.4349365234375\n",
      "Epoch: 4. Training data loss:  1795.422119140625\n",
      "Epoch: 5. Training data loss:  1792.3226318359375\n",
      "Epoch: 6. Training data loss:  1788.9747314453125\n",
      "Epoch: 7. Training data loss:  1786.0213623046875\n",
      "Epoch: 8. Training data loss:  1782.95751953125\n",
      "Epoch: 9. Training data loss:  1779.1446533203125\n",
      "Epoch: 10. Training data loss:  1775.3131103515625\n",
      "Epoch: 11. Training data loss:  1770.322509765625\n",
      "Epoch: 12. Training data loss:  1766.023681640625\n",
      "Epoch: 13. Training data loss:  1760.4815673828125\n",
      "Epoch: 14. Training data loss:  1755.5780029296875\n",
      "Epoch: 15. Training data loss:  1749.7147216796875\n",
      "Epoch: 16. Training data loss:  1743.5941162109375\n",
      "Epoch: 17. Training data loss:  1739.287109375\n",
      "Epoch: 18. Training data loss:  1734.897216796875\n",
      "Epoch: 19. Training data loss:  1730.9752197265625\n",
      "Epoch: 20. Training data loss:  1726.3388671875\n",
      "Epoch: 21. Training data loss:  1723.7244873046875\n",
      "Epoch: 22. Training data loss:  1718.88037109375\n",
      "Epoch: 23. Training data loss:  1715.5484619140625\n",
      "Epoch: 24. Training data loss:  1712.3427734375\n",
      "Epoch: 25. Training data loss:  1709.77880859375\n",
      "Epoch: 26. Training data loss:  1705.83154296875\n",
      "Epoch: 27. Training data loss:  1702.83984375\n",
      "Epoch: 28. Training data loss:  1698.178466796875\n",
      "Epoch: 29. Training data loss:  1694.430419921875\n",
      "Epoch: 30. Training data loss:  1691.4342041015625\n",
      "Epoch: 31. Training data loss:  1687.4560546875\n",
      "Epoch: 32. Training data loss:  1683.8807373046875\n",
      "Epoch: 33. Training data loss:  1679.4381103515625\n",
      "Epoch: 34. Training data loss:  1674.974365234375\n",
      "Epoch: 35. Training data loss:  1671.8280029296875\n",
      "Epoch: 36. Training data loss:  1668.4649658203125\n",
      "Epoch: 37. Training data loss:  1664.058349609375\n",
      "Epoch: 38. Training data loss:  1659.306396484375\n",
      "Epoch: 39. Training data loss:  1657.0830078125\n",
      "Accuracy found: 6.49\n",
      "40 0.01 0.01 CrossEntropyLoss() 128 0\n",
      "Starting iteration, 27\n",
      "Epoch: 0. Training data loss:  23027.119140625\n",
      "Epoch: 1. Training data loss:  23026.220703125\n",
      "Epoch: 2. Training data loss:  23026.189453125\n",
      "Epoch: 3. Training data loss:  23026.2578125\n",
      "Epoch: 4. Training data loss:  23026.1953125\n",
      "Epoch: 5. Training data loss:  23026.2890625\n",
      "Epoch: 6. Training data loss:  23026.197265625\n",
      "Epoch: 7. Training data loss:  23026.296875\n",
      "Epoch: 8. Training data loss:  23026.248046875\n",
      "Epoch: 9. Training data loss:  23026.189453125\n",
      "Epoch: 10. Training data loss:  23026.22265625\n",
      "Epoch: 11. Training data loss:  23026.25390625\n",
      "Epoch: 12. Training data loss:  23026.224609375\n",
      "Epoch: 13. Training data loss:  23026.24609375\n",
      "Epoch: 14. Training data loss:  23026.240234375\n",
      "Epoch: 15. Training data loss:  23026.2578125\n",
      "Epoch: 16. Training data loss:  23026.2890625\n",
      "Epoch: 17. Training data loss:  23026.291015625\n",
      "Epoch: 18. Training data loss:  23026.130859375\n",
      "Epoch: 19. Training data loss:  23026.24609375\n",
      "Epoch: 20. Training data loss:  23026.2265625\n",
      "Epoch: 21. Training data loss:  23026.23828125\n",
      "Epoch: 22. Training data loss:  23026.20703125\n",
      "Epoch: 23. Training data loss:  23026.123046875\n",
      "Epoch: 24. Training data loss:  23026.2734375\n",
      "Epoch: 25. Training data loss:  23026.25390625\n",
      "Epoch: 26. Training data loss:  23026.259765625\n",
      "Epoch: 27. Training data loss:  23026.1484375\n",
      "Epoch: 28. Training data loss:  23026.212890625\n",
      "Epoch: 29. Training data loss:  23026.28515625\n",
      "Epoch: 30. Training data loss:  23026.26171875\n",
      "Epoch: 31. Training data loss:  23026.224609375\n",
      "Epoch: 32. Training data loss:  23026.232421875\n",
      "Epoch: 33. Training data loss:  23026.216796875\n",
      "Epoch: 34. Training data loss:  23026.26953125\n",
      "Epoch: 35. Training data loss:  23026.232421875\n",
      "Epoch: 36. Training data loss:  23026.279296875\n",
      "Epoch: 37. Training data loss:  23026.3046875\n",
      "Epoch: 38. Training data loss:  23026.30078125\n",
      "Epoch: 39. Training data loss:  23026.2578125\n",
      "Accuracy found: 1.0\n",
      "40 0.0001 0.3 CrossEntropyLoss() 10 1\n",
      "Starting iteration, 28\n",
      "Epoch: 0. Training data loss:  1812.4263916015625\n",
      "Epoch: 1. Training data loss:  1805.2193603515625\n",
      "Epoch: 2. Training data loss:  1805.222412109375\n",
      "Epoch: 3. Training data loss:  1805.4256591796875\n",
      "Epoch: 4. Training data loss:  1805.3004150390625\n",
      "Epoch: 5. Training data loss:  1805.3079833984375\n",
      "Epoch: 6. Training data loss:  1805.6927490234375\n",
      "Epoch: 7. Training data loss:  1805.1558837890625\n",
      "Epoch: 8. Training data loss:  1845.2811279296875\n",
      "Epoch: 9. Training data loss:  1804.7344970703125\n",
      "Epoch: 10. Training data loss:  1805.304443359375\n",
      "Epoch: 11. Training data loss:  1804.76953125\n",
      "Epoch: 12. Training data loss:  1805.1346435546875\n",
      "Epoch: 13. Training data loss:  1805.2486572265625\n",
      "Epoch: 14. Training data loss:  143855.109375\n",
      "Epoch: 15. Training data loss:  2056.8896484375\n",
      "Epoch: 16. Training data loss:  1927.8826904296875\n",
      "Epoch: 17. Training data loss:  1830.9637451171875\n",
      "Epoch: 18. Training data loss:  1842.3580322265625\n",
      "Epoch: 19. Training data loss:  1797.3067626953125\n",
      "Epoch: 20. Training data loss:  1794.7633056640625\n",
      "Epoch: 21. Training data loss:  1802.9600830078125\n",
      "Epoch: 22. Training data loss:  1806.065673828125\n",
      "Epoch: 23. Training data loss:  1805.4215087890625\n",
      "Epoch: 24. Training data loss:  1805.2164306640625\n",
      "Epoch: 25. Training data loss:  1805.36962890625\n",
      "Epoch: 26. Training data loss:  1805.62060546875\n",
      "Epoch: 27. Training data loss:  1805.2979736328125\n",
      "Epoch: 28. Training data loss:  1805.459228515625\n",
      "Epoch: 29. Training data loss:  1805.4169921875\n",
      "Epoch: 30. Training data loss:  1805.4722900390625\n",
      "Epoch: 31. Training data loss:  1805.127197265625\n",
      "Epoch: 32. Training data loss:  1805.435546875\n",
      "Epoch: 33. Training data loss:  1805.144775390625\n",
      "Epoch: 34. Training data loss:  1805.456298828125\n",
      "Epoch: 35. Training data loss:  1805.0625\n",
      "Epoch: 36. Training data loss:  1804.78564453125\n",
      "Epoch: 37. Training data loss:  1805.4085693359375\n",
      "Epoch: 38. Training data loss:  1804.9656982421875\n",
      "Epoch: 39. Training data loss:  1805.219970703125\n",
      "Accuracy found: 1.0\n",
      "40 0.1 0.01 CrossEntropyLoss() 128 1\n",
      "Starting iteration, 29\n",
      "Epoch: 0. Training data loss:  23067.609375\n",
      "Epoch: 1. Training data loss:  23070.947265625\n",
      "Epoch: 2. Training data loss:  23070.40234375\n",
      "Epoch: 3. Training data loss:  23070.72265625\n",
      "Epoch: 4. Training data loss:  23069.453125\n",
      "Epoch: 5. Training data loss:  23070.26171875\n",
      "Epoch: 6. Training data loss:  23068.3515625\n",
      "Epoch: 7. Training data loss:  23071.916015625\n",
      "Epoch: 8. Training data loss:  23070.02734375\n",
      "Epoch: 9. Training data loss:  23064.556640625\n",
      "Epoch: 10. Training data loss:  23071.59375\n",
      "Epoch: 11. Training data loss:  23068.083984375\n",
      "Epoch: 12. Training data loss:  23069.38671875\n",
      "Epoch: 13. Training data loss:  23072.42578125\n",
      "Epoch: 14. Training data loss:  23071.24609375\n",
      "Epoch: 15. Training data loss:  23068.521484375\n",
      "Epoch: 16. Training data loss:  23065.134765625\n",
      "Epoch: 17. Training data loss:  23069.416015625\n",
      "Epoch: 18. Training data loss:  23071.423828125\n",
      "Epoch: 19. Training data loss:  23075.8359375\n",
      "Epoch: 20. Training data loss:  23067.955078125\n",
      "Epoch: 21. Training data loss:  23072.3671875\n",
      "Epoch: 22. Training data loss:  23069.86328125\n",
      "Epoch: 23. Training data loss:  23069.837890625\n",
      "Epoch: 24. Training data loss:  23070.615234375\n",
      "Epoch: 25. Training data loss:  23069.208984375\n",
      "Epoch: 26. Training data loss:  23069.89453125\n",
      "Epoch: 27. Training data loss:  23064.26171875\n",
      "Epoch: 28. Training data loss:  23075.037109375\n",
      "Epoch: 29. Training data loss:  23071.833984375\n",
      "Epoch: 30. Training data loss:  23071.203125\n",
      "Epoch: 31. Training data loss:  23066.361328125\n",
      "Epoch: 32. Training data loss:  23091.66015625\n",
      "Epoch: 33. Training data loss:  23070.873046875\n",
      "Epoch: 34. Training data loss:  23068.54296875\n",
      "Epoch: 35. Training data loss:  23068.783203125\n",
      "Epoch: 36. Training data loss:  23068.95703125\n",
      "Epoch: 37. Training data loss:  23069.017578125\n",
      "Epoch: 38. Training data loss:  23070.533203125\n",
      "Epoch: 39. Training data loss:  23068.830078125\n",
      "Accuracy found: 1.0\n",
      "40 0.01 0.001 CrossEntropyLoss() 10 1\n",
      "(22.46, (40, 0.0001, 0.001, CrossEntropyLoss(), 64, 1))\n"
     ]
    }
   ],
   "source": [
    "print(rand_search('cnn2',param_dist, classes, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XX8c2fb99iA"
   },
   "source": [
    "## **Session 2:  Finetuning the ConvNet**\n",
    "### 2.1 STL-10 DATASET\n",
    "> The above networks are trained on CIFAR-100, which\n",
    "contains the images of 100 different object categories, each of which has $32\\times32 \\times3$ dimensions. \n",
    "The dataset we use throughout this session is a subset of [STL-10](https://cs.stanford.edu/~acoates/stl10/)\n",
    "with higher resolution and different object classes. So, there is a discrepancy between the previous dataset (CIFAR-100) and the new dataset (STL-10). One solution would be to train the whole network from scratch. However, the number of parameters is too large to be trained properly with such few images. Another way is to use the pre-trained network (on CIFAR-100) and then finetune the network on the new dataset (STL-10) (*e.g.*, use the same architectures in all layers except the output layer, as the number of output classes changes (from 100 to 5)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XR1xyOW_sib8",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "57689c51-a2cc-40c7-c6fb-216010d2f13f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the following codes if necessary\n",
    "# referenced codes: https://cs.stanford.edu/~acoates/stl10/\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os, sys, tarfile, errno\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "try:\n",
    "    from imageio import imsave\n",
    "except:\n",
    "    from scipy.misc import imsave\n",
    "\n",
    "print(sys.version_info) \n",
    "\n",
    "# image shape\n",
    "HEIGHT = 96\n",
    "WIDTH = 96\n",
    "DEPTH = 3\n",
    "\n",
    "# size of a single image in bytes\n",
    "SIZE = HEIGHT * WIDTH * DEPTH\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# url of the binary data\n",
    "DATA_URL = 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz'\n",
    "\n",
    "# path to the binary train file with image data\n",
    "DATA_PATH = './data/stl10_binary/train_X.bin'\n",
    "\n",
    "# path to the binary train file with labels\n",
    "LABEL_PATH = './data/stl10_binary/train_y.bin'\n",
    "\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "\n",
    "\n",
    "def read_single_image(image_file):\n",
    "    \"\"\"\n",
    "    CAREFUL! - this method uses a file as input instead of the path - so the\n",
    "    position of the reader will be remembered outside of context of this method.\n",
    "    :param image_file: the open file containing the images\n",
    "    :return: a single image\n",
    "    \"\"\"\n",
    "    # read a single image, count determines the number of uint8's to read\n",
    "    image = np.fromfile(image_file, dtype=np.uint8, count=SIZE)\n",
    "    # force into image matrix\n",
    "    image = np.reshape(image, (3, 96, 96))\n",
    "    # transpose to standard format\n",
    "    # You might want to comment this line or reverse the shuffle\n",
    "    # if you will use a learning algorithm like CNN, since they like\n",
    "    # their channels separated.\n",
    "    image = np.transpose(image, (2, 1, 0))\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_image(image):\n",
    "    \"\"\"\n",
    "    :param image: the image to be plotted in a 3-D matrix format\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def save_image(image, name):\n",
    "    imsave(\"%s.png\" % name, image, format=\"png\")\n",
    "\n",
    "def download_and_extract():\n",
    "    \"\"\"\n",
    "    Download and extract the STL-10 dataset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
    "        print('Downloaded', filename)\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "\n",
    "def save_images(images, labels):\n",
    "    print(\"Saving images to disk\")\n",
    "    i = 0\n",
    "    for image in images:\n",
    "        label = labels[i]\n",
    "        directory = './img/' + str(label) + '/'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as exc:\n",
    "            if exc.errno == errno.EEXIST:\n",
    "                pass\n",
    "        filename = directory + str(i)\n",
    "        print(filename)\n",
    "        save_image(image, filename)\n",
    "        i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "uU0aikuks2Lp",
    "outputId": "77f31a37-ead0-4df3-858f-529b19607948"
   },
   "outputs": [],
   "source": [
    "# download data if needed\n",
    "download_and_extract()\n",
    "\n",
    "# # test to check if the image is read correctly\n",
    "with open(DATA_PATH) as f:\n",
    "    image = read_single_image(f)\n",
    "    plot_image(image)\n",
    "\n",
    "# # test to check if the whole dataset is read correctly\n",
    "images = read_all_images(DATA_PATH)\n",
    "print(images.shape)\n",
    "\n",
    "# labels = read_labels(LABEL_PATH)\n",
    "print(labels.shape)\n",
    "\n",
    "# # save images to disk\n",
    "save_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW0p0R6d99iA"
   },
   "source": [
    "#### **`Q2.1 Create the STL10_Dataset (5-pts)`**\n",
    "In this Session, download STL-10 and extract 5 classes from STL-10 training dataset. The the labels of images will be defined as: \n",
    "\n",
    "`{1: 'car',2:'deer',3:'horse',4:'monkey',5:'truck'}`\n",
    "\n",
    " Extract mentioned 5 classes of images from STL-10. Complement *`STL10_Dataset`* class and match each class with the label accordingly. Hint: You can use the codes above to help to complement *`STL10_Dataset`* class. (5-pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(images[1],\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPhepFAq99iA"
   },
   "outputs": [],
   "source": [
    "class STL10_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        \n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        if train:\n",
    "            x_path =  'train_X.bin'\n",
    "            y_path =  'train_y.bin'\n",
    "        else:\n",
    "            x_path = 'test_X.bin'\n",
    "            y_path = 'test_y.bin'\n",
    "\n",
    "        # DATA\n",
    "        path_to_data = os.path.join(self.root, x_path)\n",
    "        with open(path_to_data, 'rb') as f:\n",
    "            input = np.fromfile(f, dtype=np.uint8)\n",
    "            images = np.transpose(np.reshape(input, (-1, 3, 96, 96)), (0, 1, 3, 2))\n",
    "        self.data = images\n",
    "\n",
    "        # LABELS\n",
    "        labels = None\n",
    "        path_to_labels = os.path.join(self.root, y_path)\n",
    "        with open(path_to_labels, 'rb') as f:\n",
    "            labels = np.fromfile(f, dtype=np.uint8) - 1\n",
    "        self.labels = labels\n",
    "\n",
    "        #KEEP ONLY REQUIRED LABELS \n",
    "        self.data = self.data[self.labels < 5]\n",
    "        self.labels = self.labels[self.labels < 5]\n",
    "\n",
    "        #CHECK SHAPE\n",
    "        print(self.data.shape, self.labels.shape[0])\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        img = self.data[item]\n",
    "        target = int(self.labels[item])\n",
    "\n",
    "        # Transform (if applicable)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_stl = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((32, 32)), # Scale to 32x32\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                    ])\n",
    "\n",
    "# Use our custom dataset with the dataloader\n",
    "trainset_stl = STL10_Dataset(root='./data/stl10_binary', train=True, transform=transform_stl)\n",
    "trainloader_stl = torch.utils.data.DataLoader(trainset_stl, batch_size=4, shuffle=True, num_workers=2)\n",
    "testset_stl = STL10_Dataset(root='./data/stl10_binary', train=False, transform=transform_stl)\n",
    "testloader_stl = torch.utils.data.DataLoader(testset_stl, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = STL10_Dataset(path1, transform = transform)\n",
    "trainloader_stl = DataLoader(new_train, batch_size=50, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNLyFVEWw3Ge"
   },
   "source": [
    "### 2.2 Fine-tuning ConvNet\n",
    "You should load the pre-trained parameters and modify the output layer of pre-trained ConvNet from 100 to 5. You can either load the pre-trained parameters and then modify the output layer, or change the output layer firstly and then load the matched pre-trained parameters. The examples can be found at [link1](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and [link2](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC3ipaYR99iB"
   },
   "source": [
    "#### **`Q2.2  Finetuning from ConvNet (10-pts)`**\n",
    "1. Load the pre-trained parameters (pretrained on CIFAR-100) and modify the ConvNet. (5-pts)\n",
    "2. Train the model and show the results (settings of hyperparameters, accuracy, learning curve). (5-pts)\n",
    "\n",
    "**Hint**:  Once the network is trained, it is a good practice to understand the feature space by visualization techniques. There are several techniques to visualize the feature space. [**t-sne**](https://lvdmaaten.github.io/tsne/) is a dimensionality reduction method which can help you better understand the feature learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PN_l_WX199iB"
   },
   "outputs": [],
   "source": [
    "class ConvNet_5(nn.Module):\n",
    "    # Complete the code using LeNet-5\n",
    "    # reference: https://ieeexplore.ieee.org/document/726791\n",
    "    def __init__(self):\n",
    "        super(ConvNet_2, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 6, 5)\n",
    "        self.BN2 = nn.BatchNorm2d(6)\n",
    "        self.S3 = nn.AvgPool2d(2,2)\n",
    "        self.C4 = nn.Conv2d(6, 16, 5)\n",
    "        self.S5 = nn.AvgPool2d(2,2)\n",
    "        self.C6 = nn.Conv2d(16, 120, 5)\n",
    "        self.F7 = nn.Linear(120, 84)\n",
    "        self.D8 = nn.Dropout(0.4)\n",
    "        self.out = nn.Linear(84, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.BN2(self.C1(x))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.S3(x)\n",
    "        x = F.leaky_relu(self.C4(x))\n",
    "        x = self.S5(x)\n",
    "        x = F.leaky_relu(self.C6(x))\n",
    "        #flatten before linear layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.F7(x))\n",
    "        x = self.D8(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jkgXVGt99iB"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDUT9jtY99iB"
   },
   "source": [
    "###  **`Bonus (optional)`**\n",
    "Play with the code and try to get a higher accuracy on the test dataset (5 class from STL-10) as high as you can. The only data you can use is from CIFAR-100 and SLT-10. The higher accuracy among all teams can get extra points. Specifically, **1st:** *5-pts*, **2nd and 3rd:** *4-pts*, **4th and 5th:** *3-pts*, **6th and 7th:** *2-pts*, **8th-10th:** *1-pts*. You can adjust the hyperparameters and changing structures. Your strategies should be described and explained in your report.\n",
    "\n",
    "*Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers.*\n",
    "\n",
    "**Hints**:\n",
    "*   Data augmentation\n",
    "*   Grid Search\n",
    "*   Freezing early layers\n",
    "*   Modifying Architecture\n",
    "*   Modifying hyperparameters, *etc*.\n",
    "*   [Other advice](https://cs231n.github.io/transfer-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1YsjfP199iB"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
